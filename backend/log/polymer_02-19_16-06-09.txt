2025-02-19 16:06:09 - WARNING: {'<pad>': 0, '<suc>': 1, 'monomer': 2, 'organic': 3, 'inorganic': 4, 'condition': 5, 'polymer_family': 6, 'syn_method': 7, 'prop_name': 8, 'prop_value': 9, 'ref_exp': 10, 'char_method': 11, 'polymer': 12, 'material_amount': 13, 'composite': 14, 'other_material': 15}
2025-02-19 16:06:09 - INFO: Building Model
2025-02-19 16:06:09 - WARNING: 2025-02-19 16:06:09 - INFO: Building Model
2025-02-19 16:06:16 - WARNING: uploads/_ACL_2025__LLM_Efficiency (2) (1).pdf
2025-02-19 16:06:16 - WARNING: start parsing pdf
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 4
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 4
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 4
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 4
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 4
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 4
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 4
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 4
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 3
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 2
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 1
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: 0
2025-02-19 16:06:18 - WARNING: parsed 1637 paragraphs
2025-02-19 16:06:18 - WARNING: ['SPECTRA: Faster Large Language Model Inference withOptimized Internal and External Speculation', 'Anonymous ACL submission', 'Abstract', 'Inference with modern Large Language Mod-001', 'els (LLMs) is both computationally expensive002', 'and time-consuming. Speculative decoding has003', 'emerged as a promising solution, but existing004', 'approaches face key limitations: training-based005', 'methods require a draft model that is challeng-006', 'ing to obtain and lacks generalizability, while007', 'non-training methods offer limited speedup008', 'gains. In this work, we present SPECTRA, a009', 'novel framework for accelerating LLM infer-010', 'ence without the need for additional training.011', 'SPECTRA introduces two innovative techniques012', 'for efficiently managing internal and external013', 'knowledge, each outperforming corresponding014', 'state-of-the-art (SOTA) methods independently.015', 'When combined, these techniques achieve up016', 'to a 4.08x speedup across various benchmarks017', 'and LLM architectures, significantly surpassing018', 'existing non-training approaches. The imple-019', 'mentation of SPECTRA is publicly available.020', '1Introduction021', 'Generating long sequences with low latency using022', 'Large Language Models (LLMs) is a critical re-023', 'quirement. Current LLMs rely on autoregressive024', 'decoding (Touvron et al., 2023; Bai et al., 2023;025', 'Jiang et al., 2023; OpenAI et al., 2024), which026', 'suffers from inefficiency because it generates text027', 'one token at a time. This results in generation028', 'time scaling linearly with the sequence length and029', 'underutilizes the parallel processing capabilities030', 'of modern GPUs. A widely studied approach to031', 'mitigate this issue is speculative decoding (Chen032', 'et al., 2023; Leviathan et al., 2023), which fol-033', 'lows a guess-and-verify paradigm. In this approach,034', 'a smaller LLM (draft model) (Chen et al., 2023;035', 'Leviathan et al., 2023; Miao et al., 2024; Sun et al.,036', '2023b; Zhou et al., 2024; Cai et al., 2024) or the037', 'original LLM trained in a specialized manner (self-038', 'speculative decoding) (Elhoushi et al., 2024; Liu039', 'et al., 2024a; Yang et al., 2024; Zhang et al., 2024a;040', 'Li et al., 2024b) predicts multiple tokens in ad-041', 'vance. The original LLM then verifies these pre-042', 'dictions in parallel, improving efficiency. However,043', 'these approaches require additional training, which044', 'demands substantial computational resources and045', 'may degrade the original model’s capabilities.046', 'Another line of research focuses on speculat-047', 'ing subsequent tokens without requiring additional048', 'training. This approach eliminates the need for049', 'training new models or modifying the original large050', 'language model (LLM), making it practical for051', 'off-the-shelf deployment. Some methods leverage052', 'specialized mechanisms to generate speculative to-053', 'kens directly from the LLM’s predictions (Fu et al.,054', '2024; Ou et al., 2024), while others rely on ex-055', 'ternal information sources to derive these tokens056', '(Yang et al., 2023; He et al., 2024; Li et al., 2024a).057', 'However, the speedup gain in these approaches re-058', 'mains limited due to the quality of the speculative059', 'guesses.060', 'We introduce SPECTRA (Figure 1a), a specu-061', 'lative decoding method that improves generation062', 'speed without requiring any training or modifica-063', 'tions to the original LLM. SPECTRA consists of064', 'two main components: a core module (SPECTRA-065', 'CORE, Figure 1c), which integrates seamlessly into066', 'LLMs in a plug-and-play manner, and an optional067', 'retrieval module (SPECTRA-RETRIEVAL, Figure068', '1e) that further enhances performance. The core069', 'module SPECTRA-CORE improves speculative de-070', 'coding by leveraging the token distribution pre-071', 'dicted by the LLM to generate high-quality guesses.072', 'Specifically, it employs two multi-level N-gram073', 'dictionaries that enable bi-directional search for074', 'dynamic-length guesses, balancing both quality075', 'and quantity. Additionally, SPECTRA optimizes a076', 'candidate pool to continuously update the N-gram077', 'dictionaries, ensuring broad token coverage. All078', 'updates to these resources, along with guess verifi-079', 'cation, are performed efficiently in a single forward080', 'pass. The retrieval module, SPECTRA-RETRIEVAL,081', '1', 'SPECTRA-RETRIEVAL', 'Module', 'N-gram Store', 'Input', 'Lookahead', 'Candidate', 'LLM', '(b) Lookahead decoding(c) SPECTRA-CORE', 'Guesses', 'Input', 'SPECTRA', 'Candidate', 'LLM', 'Bi-directional', 'Guesses', 'Multi-level', 'N-gram Store', 'InputCorpus', 'Guesses', 'LLM', 'Trie', 'Input', 'Guesses', 'LLM', 'Trie', '(d) REST', 'Corpus Refined', 'by Perplexity', '(e) SPECTRA-RETRIEVAL', '(a) Ours: SPECTRA', 'SPECTRA-CORE', 'Module', 'Input', 'GuessesLLM', 'Figure 1: Overview of Spectra and comparison with other non-training SOTA approaches. (a) Overview of SPECTRA.(b) Overview of Lookahead Decoding (Fu et al., 2024). (c) Overview of the SPECTRA-CORE module, which utilizesthe knowledge inside LLM for obtaining guesses. (d) Overview of REST (He et al., 2024). (e) Overview of theSPECTRA-RETRIEVAL module, which is designed to be integrated efficiently with SPECTRA-CORE to boost thespeedup. The results in the bar chart are measured on HumanEval.', 'can be integrated to further enhance speedup. Ex-082', 'isting approaches that rely on external sources for083', 'generating guesses (He et al., 2024) struggle to in-084', 'tegrate with other speculative decoding methods,085', 'as the search time outweighs the speedup gains.086', 'SPECTRA-RETRIEVAL addresses this issue by re-087', 'ducing the search space, selecting only high-quality088', 'content from the corpus based on perplexity scores089', 'computed by the target LLM. This optimization en-090', 'ables seamless integration with SPECTRA-CORE,091', 'maximizing efficiency.092', 'Empirical results on six tasks—including multi-093', 'turn conversation, code generation, and mathemati-094', 'cal reasoning—across three LLM families (Llama095', '2 (Touvron et al., 2023), Llama 3 (Dubey et al.,096', '2024), and CodeLlama (Rozière et al., 2024)) with097', 'model sizes ranging from 7B to 70B demonstrate098', 'that SPECTRA outperforms other non-training spec-099', 'ulative decoding methods, achieving speedups of100', 'up to 4x. We publicly release the code and data.101', 'The key contributions of this paper are as follows:102', '• We introduce SPECTRA, which improves spec-103', 'ulative decoding by effectively leveraging the104', 'LLM’s predicted token distribution. SPEC-105', 'TRA is a plug-and-play solution that requires106', 'no modifications to the LLM (Section 3.1).107', '• SPECTRA’s retrieval module refines external108', 'corpora using perplexity scores computed by109', 'the target LLM, providing a general frame-110', 'work that enables speculative decoding ap-111', 'proaches relying on external information to112', 'be seamlessly integrated with other specula-113', 'tive decoding techniques (Section 3.2).114', '• Extensive experiments across diverse tasks,115', 'LLM architectures, GPU types, and settings116', 'demonstrate the efficiency of SPECTRA, out-117', 'performing other non-training speculative de-118', 'coding approaches (Section 5).SPECTRA119', 'also integrates with acceleration tools such as120', 'FlashAttention and pipeline parallelism (Sec-121', 'tion 5.2). The code and data are available.122', '2Preliminaries123', '2.1Autoregressive Decoding in LLMs124', 'Given an input sequence x = (x1, x2, . . . , xs)125', 'of length s, and a slice of length m as x1:m =126', '(x1, x2, . . . , xm), the output of an LLM repre-127', 'sents a probability distribution over the next to-128', 'ken. The probability of generating the s-th token,129', 'conditioned on all preceding tokens, is given by130', 'PM(xs | x1:s−1). The next token xs is sampled131', 'from this distribution using methods such as greedy,132', 'top-k, or top-p sampling (see (Kool et al., 2020;133', 'Holtzman et al., 2020)). For greedy sampling, the134', 'next token is selected as xs = arg max PM(xs |135', 'x1:s−1). Consequently, the LLM generates an out-136', 'put sequence (y1, y2, . . . , ym) of length m autore-137', 'gressively, where each token yi is computed as138', '2', 'yi = argmax PM(yi | y1:i−1, x).139', '2.2Speculative Decoding140', 'Speculative decoding follows a guess-and-verify141', 'approach, where multiple candidate future to-142', 'kens are speculated and subsequently verified143', 'in a single decoding step.With tree attention144', '(Miao et al., 2024), multiple drafts can be ver-145', 'ified simultaneously. Let G denote the number146', 'of guesses, and define the set of guesses as ˜Y =147', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence148', 'has length K. The j-th token of the i-th guess is149', 'denoted as ˜y(i) ', 'j .150', 'In the case of speculative decoding with greedy151', 'sampling, given the prompt x, a drafting method152', 'generates the draft sequences ˜Y . Using these drafts,153', 'the LLM computes the true tokens (y′1, y′2, . . . , y′K)154', 'in parallel. These tokens are then verified, and155', 'h is defined as the highest number of correctly156', 'guessed tokens across all guesses. Consequently,157', 'h + 1 tokens are generated in a single forward158', 'step. Algorithm 2 outlines speculative decoding159', 'with greedy sampling, and additional details are160', 'provided in Appendix A.161', '3SPECTRA DECODING162', 'SPECTRA consists of two modules (SPECTRA-163', 'CORE and SPECTRA-RETRIEVAL) that can func-164', 'tion independently or together. The core module165', '(SPECTRA-CORE) improves speedup by leveraging166', 'the LLM’s predicted token distribution to gener-167', 'ate high-quality guesses and integrates into LLMs168', 'in a plug-and-play manner. The retrieval module169', '(SPECTRA-RETRIEVAL) derives guesses from a re-170', 'fined external information source and is designed to171', 'integrate with SPECTRA-CORE to further enhance172', 'performance.173', '3.1SPECTRA-CORE174', 'SPECTRA-CORE maintains an N-gram storage and175', 'a candidate pool. The candidate pool C contains176', 'W sequences, {c(0), c(1), . . . , c(W−1)}, with each177', 'sequence consisting of N tokens. Let c(i) ', 'jrepresent178', 'the j-th token in the i-th sequence. The N-gram179', 'storage includes two dictionaries: the forward dic-180', 'tionary Sfwd and the backward dictionary Sbwd. At181', 'each time step, guesses G are obtained through a182', 'bidirectional search using Sfwd and Sbwd. A sin-183', 'gle forward pass to the LLM retrieves all neces-184', 'sary distributions, which are used to generate new185', 'candidate tokens for C and verify the guesses G.186', 'Algorithm 1 SPECTRA Internal Knowledge', 'Require: Sequence x = (x1, x2, . . . , xn), model PM, maxN-gram size N, candidate pool size W, max guesses G,max number of new tokens m. Refine threshold τ', '1: Initialize N-gram Forward-dictionary Sfwd ←∅2: Initialize N-gram Backward-dictionary Sbwd ←∅3: Random c(i) ', 'j , ∀j ∈[0, N −1], ∀i ∈[0..W −1]', '4: t ←n + 15: while t ≤m do6:{Obtain the guesses}', '7:G ←Sfwd[xt−1]', '8:u = ∅', '9:for j = 0 to N −1 do', '10:for k = N −1 to 1 do', '11:uj ←Sbwd[xt+j−k:t−1 ⊕u0:j−1]', '12:break if found value for uj', '13:end for', '14:end for', '15:G.append(u)', '16:{Foward in LLM}', '17:Obtain necessary distributions of PM in parallel.', '18:{Verification}', '19:{Greedy verify (Alg. 3) or Sampling verify (Alg. 4)}', '20:hits ←VerificationFunction(x, PM, g)', '21:x = x ⊕hits', '22:t ←t + size(hits)', '23:{Predict Candidates}', '24:for i = 0 to W −1 do', '25:r ∼Uniform[0, 1]', '26:Pc(c(i) ', 'N−1) ←PM(c(i) ', 'N−1 | c(i) ', ':N−2, x)', '27:if r > τ then', '28:c(i) ', 'N−1 ←argmax ', 'c/∈SfwdPc(c(i) ', 'N−1)', '29:else', '30:c(i) ', 'N−1 ←argmax Pc(c(i) ', 'N−1)', '31:end if', '32:end for', '33:{Update N-gram dictionaries}', '34:for i = 0 to W −1 do', '35:for j = 0 to N −2 do', '36:Sfwd[c(i) ', 'j ].append(c(i) ', 'j+1:)', '37:Sbwd[c(i) ', '0:j] ←c(i) ', 'j+1', '38:end for', '39:end for', '40:{Update Candidates}', '41:c(i) ', 'j←c(i) ', 'j+1, ∀j ∈[0, N −2], ∀i', '42: end while43: Output: xn+1:n+m = (y1, y2, . . . , ym)', 'The dictionaries Sfwd and Sbwd are updated with N-187', 'grams from the candidate pool. The details of the188', 'SPECTRA-CORE decoding process are described189', 'in Algorithm 1.190', 'Bi-directional Search for GuessesAt each step,191', 'SPECTRA generates G guess sequences G=192', '{˜y(0), ˜y(1), . . . , ˜y(G)}. Unlike previous work (Fu193', 'et al., 2024), which enforces uniform guess lengths,194', 'SPECTRA supports variable-length guesses, im-195', 'proving both flexibility and efficiency. The for-196', 'ward dictionary Sfwd maps a token to a list of197', 'sequences, while the backward dictionary Sbwd198', 'maps a sequence to a single token. At time step199', '3', 'Input', 'Input', 'Large Language Model', 'Token distribution', 'Figure 2: Details of SPECTRA forward step in LLM.The dashed arrow indicates interactions between thetokens, which are realized by the LLM’s attention mask.', 't, the set of guesses is obtained through a bidi-200', 'rectional search (Alg. 1, lines 7–15). This search201', 'operates in two directions: (1) the forward direc-202', 'tion, which prioritizes the quantity of guesses, and203', '(2) the backward direction, which prioritizes the204', 'quality of guesses. In the forward direction, the205', 'last generated token xt−1 is used to search Sfwd206', 'for guess sequences (Alg. 1, line 7). In the back-207', 'ward direction, a high-quality guess is constructed208', 'by iteratively predicting one token at a time using209', 'Sbwd, repeating the process until a desired sequence210', 'length N is reached (Alg. 1, lines 8–14).211', 'Predict & Verify in One Forward PassAll dis-212', 'tributions required for predicting candidates and213', 'verifying guesses are obtained in a single forward214', 'pass to the LLM, leveraging parallel processing215', '(Figure 2). This is achieved using a specially de-216', 'signed attention mask that specifies the allowed217', 'interactions between tokens. For instance, the to-218', 'ken c(1) ', '2attends only to c(1) ', '1 , c(1) ', '0 , and the input.219', 'Predict Tokens for Candidate PoolWe predict220', 'the next candidate tokens c(i) ', 'N−1 for the candidate221', 'pool using the distribution obtained from the for-222', 'ward pass (Alg. 1, lines 24–32). A straightfor-223', 'ward approach is to select tokens with the highest224', 'probability in the token distribution. However, we225', 'observe that when searching for guesses in the for-226', 'ward dictionary Sfwd, it is crucial for the search to-227', 'ken to exist in the dictionary; otherwise, no guesses228', 'can be retrieved. To address this, we introduce a229', 'randomness-based mechanism to increase the cov-230', 'erage of Sfwd. Specifically, we probabilistically231', 'encourage the selection of unseen tokens in Sfwd232', 'using a hyperparameter τ ∈[0, 1]. Let r be a233', 'random draw from [0, 1]. If r > τ, we select to-234', 'kens with the highest probability that are not in235', 'Sfwd; otherwise, we choose tokens with the high-236', 'est probability regardless of their presence in Sfwd.237', 'Although c(i) ', 'N−1 does not immediately affect the238', 'coverage of Sfwd, it contributes to coverage expan-239', 'sion in subsequent time steps through our candidate240', 'updating mechanism. At the end of each time step,241', 'all candidate sequences are shifted left by one to-242', 'ken: c(i) ', 'j←c(i) ', 'j+1, leaving c(i) ', 'N−1 empty and ready243', 'for prediction in the next time step (Alg. 1, line 41).244', 'Update N-gram DictionariesAt the end of each245', 'time step, candidate tokens from the pool C are246', 'used to update the N-gram dictionaries Sfwd and247', 'Sbwd. While previous work (Fu et al., 2024) only248', 'adds the full N-gram (c(i) ', '0 , c(i) ', '1 , . . . , c(i) ', 'N ), we ob-249', 'serve that subsequences within N-grams often ap-250', 'pear later in the generation process. By including251', 'these subsequences in the N-gram storage, we im-252', 'prove both the quality of guesses and the coverage253', 'of the dictionaries. Specifically, we add subse-254', 'quences to Sfwd using the first token as the key,255', 'and update Sbwd by mapping the preceding part of256', 'the sequence to the last token (Alg. 1, lines 33–39).257', '3.2SPECTRA-RETRIEVAL258', 'SPECTRA-RETRIEVALleveragesanexternal259', 'knowledge source to generate guesses. This in-260', 'volves processing a text corpus and indexing it into261', 'a structure that supports fast prefix search, such as a262', 'trie. At each time step, the last generated tokens are263', 'used as input to this structure to retrieve guesses for264', 'speculative decoding. However, we observe that us-265', 'ing random texts from the corpus without selection266', 'can limit the speedup gain. To address this, we pro-267', 'pose a method to identify and select high-quality,268', 'relevant texts from the corpus tailored to the spe-269', 'cific LLM. This improves the speedup gain and270', 'enables seamless integration with other speculative271', 'decoding approaches, including SPECTRA-CORE.272', 'Corpus Refinement by PerplexityGiven a text sequence u = (u0, u1, u2, . . . ), perplexity quan-tifies the average uncertainty of the model when predicting the next token, conditioned on the pre-ceding tokens. It is calculated as:', 'PPL(u) = exp', '�', '−1', 't', 't�', 'i=1log pθ(ui | u<i)', '�', 'A lower perplexity indicates that the model assigns273', 'higher probabilities to the sequence, suggesting274', 'that the sequence is well-aligned with the model’s275', 'predictions and can produce high-quality guesses276', 'for speculative decoding. To optimize the retrieval277', 'process, we select texts with the lowest perplexity278', '4', 'from the corpus to form a smaller, high-quality sub-279', 'set, which is then used to construct the trie structure280', 'for generating guesses.281', 'Integration with SPECTRA-COREOur exper-282', 'iments (Section 5.2, Table 2) demonstrate that283', 'naively integrating guesses from external sources284', '(e.g., REST (He et al., 2024)) into other specula-285', 'tive methods (e.g., Lookahead (Fu et al., 2024))286', 'can lead to a noticeable drop in speedup. This oc-287', 'curs because the forward pass in the LLM can only288', 'handle a limited number of guesses, and exceeding289', 'this limit increases memory usage and slows down290', 'generation. With a limited guess budget, guesses291', 'from external sources can only account for a frac-292', 'tion of the total guesses, causing the search time293', 'in the indexing structure (e.g., a trie) to outweigh294', 'the speedup gain. To address this, it is crucial295', 'to limit the size of the external knowledge while296', 'maintaining the quality of the guesses. By refining297', 'the corpus using perplexity, SPECTRA-RETRIEVAL298', 'seamlessly integrates with SPECTRA-CORE, further299', 'boosting the speedup gain. Specifically, we inte-300', 'grate SPECTRA-RETRIEVAL into SPECTRA-CORE301', 'by including its guesses in the set of guesses during302', 'the guess generation step (Alg. 1, lines 7–15).303', '4Experiments304', 'Models.We evaluate LLaMA-2-Chat 7B, 13B,305', '70B (Touvron et al., 2023), CodeLlama 7B, 13B306', '(Rozière et al., 2024), and LLaMA-3-Instruct 8B,307', '70B (Dubey et al., 2024).308', 'Tasks.Weconductcomprehensiveevalua-309', 'tions on various generation tasks.MT-Bench310', '(Zheng et al., 2023) for multi-turn conversation;311', 'GSM8K(Cobbe et al., 2021) for mathemati-312', 'cal reasoning; HumanEval(Chen et al., 2021),313', 'MBPP(Austin et al., 2021) and ClassEval (Du314', 'et al., 2023) for code generation.315', 'Metrics.SPECTRA does not modify the original316', 'LLM and the acceptance conditions, making it a317', 'lossless acceleration method. Therefore, the gener-318', 'ation quality remains the same as the original LLM.319', 'We only evaluate the acceleration performance us-320', 'ing the following metrics.321', '• Speedup Ratio: The speedup ratio relative to322', 'autoregressive decoding.323', '• Compression ratio: The ratio of the total324', 'number of autoregressive steps to the number325', 'of Spectra decoding steps needed to produce326', 'the same sequence length.327', 'Baselines.We use standard autoregressive decod-328', 'ing as the baseline (speed-up ratio = 1.00x). We fur-329', 'ther compare SPECTRA with leading non-training330', 'speculative decoding approaches, namely Adaptive331', 'N-gram (Ou et al., 2024), REST (He et al., 2024),332', 'and Lookahead (Fu et al., 2024). For details regard-333', 'ing implementation settings of both SPECTRA and334', 'these baselines, please refer to Appendix B.335', '5Results336', '5.1Main Results337', 'Overall Performance.The top portion of Table 1338', 'presents the speedup ratios of all evaluated meth-339', 'ods under a greedy decoding setup. Our approach,340', 'SPECTRA, consistently yields the highest accelera-341', 'tion across the entire range of datasets and LLMs.342', 'In particular, SPECTRA achieves speedups up to343', '4.08× with LLama-3-8B-Instruct on the MBPP344', 'dataset.345', 'For smaller models (7B), SPECTRA often sur-346', 'passes 3× acceleration, underscoring the effective-347', 'ness of multi-token compression. By contrast, for348', '13B models, while the boost remains strong, it is349', 'relatively more moderate, typically falling in the350', '1.6×–3× band. We attribute this trend to the in-351', 'creased overhead of each forward pass in larger net-352', 'works, which can dampen the proportional gains of353', 'fewer decoding iterations per token. Despite this,354', 'SPECTRA continues to outperform baselines across355', 'all parameter settings.356', 'Significant advantages are evident in tasks such357', 'as GSM8K and ClassEval, where outputs often358', 'follow recurring patterns (e.g., repeated variable359', 'names or class definitions). In these scenarios,360', 'SPECTRA combines internal knowledge of par-361', 'tial sequences with external retrieval suggestions,362', 'thereby proposing accurate multi-token guesses.363', 'On the other hand, in domains featuring more var-364', 'ied or unpredictable responses—such as complex365', 'multi-turn conversations in MT-Bench—the accep-366', 'tance rate is somewhat lower, although still com-367', 'petitive.368', 'Compression Ratio.Table 1 also reports each369', 'method’s compression rate, a measure agnostic370', 'to specific hardware configurations. Across ev-371', 'ery dataset and LLM tested, SPECTRA delivers the372', 'highest average compression ratio. Each of SPEC-373', 'TRA’s draft-and-verify iterations typically yields374', '5', 'ClassevalGSM8KHumanevalMBPPMTBenchAVGModelMethodspeedupτspeedupτspeedupτspeedupτspeedupτspeedup', 'Greedy (temperature=0)', 'CL-13BANPD1.942.522.813.722.082.502.713.582.613.412.43 CL-13BLookahead2.253.612.804.242.303.162.914.442.594.042.57 CL-13BREST1.282.140.931.541.582.310.851.400.941.531.12 CL-13BSPECTRA (Ours)2.384.062.914.652.633.953.294.462.654.402.77', 'CL-7BANPD2.302.683.213.752.162.473.163.783.353.832.84 CL-7BLookahead2.593.662.993.832.503.052.903.673.234.272.84 CL-7BREST1.452.220.911.391.702.340.961.451.021.441.21 CL-7BSPECTRA (Ours)2.704.103.334.592.963.903.564.453.704.523.25', 'L2-13BANPD1.361.781.471.721.341.611.121.321.171.371.29 L2-13BLookahead1.812.761.461.871.732.321.381.691.512.041.58 L2-13BREST1.222.010.941.461.251.940.951.441.141.901.10 L2-13BSPECTRA (Ours)2.003.241.832.621.962.911.632.241.752.601.83', 'L2-70BANPD1.821.901.631.611.861.871.171.201.341.301.56 L2-70BLookahead2.652.871.862.022.572.671.491.541.942.002.10 L2-70BSPECTRA (Ours)3.103.402.522.693.223.371.861.932.432.512.62', 'L2-7BANPD1.621.951.521.681.541.671.191.331.301.371.43 L2-7BLookahead2.192.941.661.932.062.421.461.691.732.051.82 L2-7BREST1.362.121.011.471.412.041.011.461.251.901.21 L2-7BSPECTRA (Ours)2.403.432.112.642.403.051.772.162.022.592.14', 'L3-70BANPD1.541.671.501.471.831.881.461.411.231.231.51 L3-70BLookahead2.402.621.541.582.562.701.431.451.761.861.94 L3-70BSPECTRA (Ours)2.672.912.102.142.843.021.941.942.062.132.32', 'L3-8BANPD2.112.493.864.571.832.093.363.581.141.232.46 L3-8BLookahead2.593.443.714.612.492.893.794.651.531.852.82 L3-8BSPECTRA (Ours)2.833.493.894.772.573.024.084.761.692.103.01', 'Sampling (temperature=1.0)', 'CL-13BANPD1.151.461.071.311.051.301.001.242.312.891.31 CL-13BLookahead1.382.001.081.431.291.751.021.342.333.481.42 CL-13BREST1.141.870.821.351.271.960.841.390.931.501.00 CL-13BSPECTRA (Ours)1.682.221.201.751.652.121.151.702.373.801.61', 'CL-7BANPD1.291.501.161.301.101.321.121.272.773.051.49 CL-7BLookahead1.542.031.191.411.431.811.191.432.723.501.61 CL-7BREST1.231.860.881.331.331.980.911.400.971.441.06 CL-7BSPECTRA (Ours)1.812.251.351.731.682.121.331.722.783.941.79', 'L2-13BANPD1.201.521.241.461.171.401.031.221.171.351.16 L2-13BLookahead1.522.221.321.691.482.001.181.481.492.011.40 L2-13BREST1.181.960.931.451.191.880.921.441.121.881.07 L2-13BSPECTRA (Ours)1.702.751.552.231.692.591.341.891.742.571.60', 'L2-7BANPD1.311.511.341.481.281.461.101.221.251.361.26 L2-7BLookahead1.782.301.511.761.722.091.251.491.682.021.59 L2-7BREST1.262.030.991.461.271.930.961.411.211.881.14 L2-7BSPECTRA (Ours)1.972.831.782.282.042.751.471.841.972.541.85', 'L3-8BANPD1.251.371.972.181.431.651.892.071.151.211.54 L3-8BLookahead1.481.782.072.411.792.211.992.401.571.811.78 L3-8BSPECTRA (Ours)1.942.842.272.781.922.512.192.781.702.052.01', 'Table 1: Overall performance of speculative decoding methods across multiple tasks. “CL-xB” denotes CodeLlamawith xB parameters, “L2-xB” denotes LLaMA-2-Chat of size xB, and “L3-xB” denotes LLaMA-3-Instruct of size xB. We report the speedup ratio (vs. autoregressive) and the compression ratio τ.', '2.1–4.8 tokens, substantially outpacing alternative375', 'approaches and nearly doubling the acceptance376', 'length achieved by REST.377', 'Acceleration in Sampling Decoding.The lower378', 'section of Table 1 investigates the performance379', 'of SPECTRA under sampling-based decoding with380', 'a temperature of 1.0. The results highlight how381', '6', 'SPECTRA continues to accelerate generation rel-382', 'ative to baselines, offering roughly 1.15–2.77×383', 'speedups over standard autoregressive decoding.384', 'These gains are more modest than in greedy decod-385', 'ing, reflecting the lower acceptance rate under the386', 'sampling-based verification phase, which is consis-387', 'tent with earlier findings (Fu et al., 2024; Leviathan388', 'et al., 2023).389', '5.2Analysis390', 'Ablationstudy.Weconductedadetailed391', 'component-wise analysis to determine the contri-392', 'bution of each module to the framework’s over-393', 'all performance (Table 2). Specifically, the re-394', 'sults on LLaMA2-7B-chat reveal that removing395', 'different components yields varying impacts on396', 'GSM8K speedups.Under the “CORE Module”397', 'configuration, excluding multi-level n-grams low-398', 'ers the speedup from 2.04× to 1.95× (a 4% de-399', 'crease), whereas turning off forward information400', 'reduces it from 2.04× to 1.50× (a 26% drop). Sim-401', 'ilarly, omitting backward information results in402', 'a speedup of 1.94×, down from 2.04×. In con-403', 'trast, the “RETRIEVAL Module” setting shows that404', 'leaving out perplexity-based filtering decreases the405', 'speedup from 1.18× to 1.16×. Our fully integrated406', 'approach, SPECTRA, achieves a 2.14× speedup407', 'on GSM8K—outperforming both the “CORE Mod-408', 'ule” (2.04×) and “RETRIEVAL Module” (1.18×)409', 'variants. This improvement demonstrates the im-410', 'portance of combining multi-level n-grams, for-411', 'ward/backward drafting, and perplexity-based re-412', 'finement in boosting acceptance rates and enhanc-413', 'ing overall speedups. A similar trend was also414', 'observed in the results of the MTBench dataset.415', 'Additionally,wecomparedourmethod416', 'against a naive combination of Lookahead and417', 'REST—where guess sequences from REST are418', 'added to Lookahead. This combined approach419', 'falls significantly short of our SPECTRA method,420', 'highlighting that a simple merger of two techniques421', 'is insufficient without our carefully optimized422', 'integration strategy and components.423', 'Priority for source of guessesSince verifying424', 'too many candidate tokens at once can strain GPU425', 'resources and reduce speedups (Fu et al., 2024; Li426', 'et al., 2024b), SPECTRA limits how many guesses427', 'proceed to verification in each step (Appendix B).428', 'To understand whether internal or external guesses429', 'are more valuable, we temporarily remove this430', 'cap and measure acceptance rates (Figure 3). We431', 'GSM8KMTBenchMethodspeedupτspeedupτ', 'REST1.011.471.251.90', 'Lookahead1.661.931.732.05', 'Lookahead + REST1.081.471.271.90', 'SPECTRA’s ablation', 'CORE Module2.042.501.922.35 - w/o forward info1.501.681.201.37 - w/o backward info1.942.211.742.12 - w/o Sub-Ngram1.952.341.752.18', 'RETRIEVAL Module1.181.311.241.50 - w/o PPL refine1.161.291.201.45', 'SPECTRA (ours)2.142.642.022.59', 'Table 2: Ablation of SPECTRA’s components (greedydecoding, LLaMA2-7B-Chat).“Sub-Ngram” aug-ments each n-gram with its sub-sequences; “for-ward/backward info” uses internal expansions; and“PPL refine” applies perplexity-based filtering for ex-ternal retrieval. “Lookahead + REST” denotes a naivecombination where guess sequences from REST are di-rectly added to Lookahead', 'observe that sequences generated via internal ex-432', 'pansions—particularly forward and backward pre-433', 'dictions—have a higher acceptance probability434', 'than those retrieved from external sources. Con-435', 'sequently, SPECTRA prioritizes internal guesses436', 'for verification. Interestingly, in code-generation437', 'tasks like HumanEval, external suggestions be-438', 'come more influential, likely due to code’s repeti-439', 'tive structure and the retrieval of similar snippets.440', 'This observation indicates that a strategic blend441', 'of backward internal knowledge and external re-442', 'trieval can be particularly fruitful in these domains,443', 'especially when computational resources limit ex-444', 'tensive forward expansions.445', 'FlashAttention.Figure 3 shows that enabling446', 'FlashAttention consistently boosts the speedup of447', 'all methods, albeit to varying degrees. Notably,448', 'we observe an additional 0.24× speedup gain for449', 'SPECTRA on both GSM8K and MTBench. This450', 'is because FlashAttention better exploits the paral-451', 'lel structure of speculative decoding by reducing452', 'attention overheads, especially when verifying mul-453', 'tiple guessed tokens in parallel. Although smaller454', 'gains are also seen for other methods, SPECTRA455', 'benefits the most, as it presents the longest verifica-456', 'tion branches and thus stands to profit significantly457', 'from more efficient attention implementations.458', '7', '020406080100', 'Total accept tokens (%)', 'ClassEval', 'GSM8K', 'HumanEval', 'MBPP', 'MT-Bench', 'Internal-FwdInternal-BwdExternal', 'Figure 3: Acceptance rates for different guess sources(e.g., SPECTRA-CORE forward dictionary, backwarddictionary, SPECTRA-RETRIEVAL’s guesses). The ac-ceptance rate is the fraction of guessed tokens that passverification and are appended to the final output.', '0', '1', '2', 'Speedup', '1.00x', '1.68x1.55x', '1.03x', '2.08x', '1.07x', '1.86x1.70x', '1.09x', '2.32x', 'GSM8K', 'W/o flashWith flash', 'Autoreg.LookaheadANPDRESTSpectra', '0', '1', '2', 'Speedup', '1.00x', '1.75x', '1.31x1.24x', '2.01x', '1.10x', '1.96x', '1.42x1.38x', '2.25x', 'MTBench', 'W/o flashWith flash', 'Figure 4: Effect of FlashAttention on speculative de-coding speed: Measured speedups on GSM8K andMTBench (LLama2-7B-Chat, greedy decoding). “NoFlash” uses standard attention; “With Flash” usesFlashAttention for faster parallel verification.', '6Related Works459', 'Large language models (LLMs) are increasingly460', 'deployed in a range of applications, motivating on-461', 'going research into more efficient inference (Liu462', 'et al., 2025). Common strategies include quan-463', 'tizing model weights into lower-precision formats464', '(Liu et al., 2024b; Lin et al., 2024; Zhao et al., 2024;465', 'Park et al., 2024), pruning redundant parameters466', '(Ma et al., 2023; Xia et al., 2023; Sun et al., 2023a;467', 'Le et al., 2025), and employing knowledge distilla-468', 'tion (Gu et al., 2024; Friha et al., 2024; Zhang et al.,469', '2024b). These techniques help reduce the compu-470', 'tational load per forward pass, thereby lowering471', 'generation latency. However, they often introduce472', 'some degradation in model performance, forcing473', 'practitioners to balance quality with efficiency.474', 'A growing line of work explores speculative de-475', 'coding as a strategy for accelerating generation476', 'while maintaining the exact output distribution477', '(Chen et al., 2023; Leviathan et al., 2023). Some478', 'speculative decoding approaches train a smaller479', 'LLM (referred to as a draft model) (Chen et al.,480', '2023; Leviathan et al., 2023; Miao et al., 2024; Sun481', 'et al., 2023b; Zhou et al., 2024; Cai et al., 2024),482', 'or train the original LLM itself in a special man-483', 'ner (referred to as self-speculative) (Elhoushi et al.,484', '2024; Liu et al., 2024a; Yang et al., 2024; Zhang485', 'et al., 2024a; Li et al., 2024b) to guess several sub-486', 'sequent tokens and then verify them parallelly us-487', 'ing the original LLM. As these approaches require488', 'training, they pose limitations, such as requiring489', 'heavy computational resources and losing the orig-490', 'inal model capabilities.491', 'To avoid additional training, alternative specula-492', 'tive decoding methods leverage external resources493', 'or structural properties of language generation.494', 'Retrieval-based methods sidestep draft model train-495', 'ing by using a datastore indexed with observed496', 'prefixes to retrieve guess sequences (Yang et al.,497', '2023; He et al., 2024; Li et al., 2024a). Other498', 'approaches, such as Jacobi-like parallel decoding499', '(Santilli et al., 2023) and lookahead decoding (Fu500', 'et al., 2024), mitigate left-to-right dependencies by501', 'generating and validating multiple candidate tokens502', 'in parallel. These training-free techniques achieve503', 'comparable speedups to learned methods without504', 'requiring model optimization, making them ideal505', 'for scenarios with computational or deployment506', 'constraints.507', '7Conclusions508', 'In this work, we introduced SPECTRA, a hybrid509', 'speculative decoding framework that combines510', 'multi-level n-grams (internal knowledge) with511', 'perplexity-based retrieval (external knowledge) to512', 'achieve speedups of up to 4.08× across various513', 'LLMs and benchmarks, without additional train-514', 'ing or compromising exact output fidelity. Our515', 'ablation studies show that each module (multi-516', 'level n-grams, forward/backward expansions, and517', 'perplexity-based datastore curation) substantially518', 'boosts acceptance rates, and their synergy outper-519', 'forms existing non-training methods. By offering a520', 'lossless speedup that efficiently exploits both inter-521', 'nal patterns and external texts, SPECTRA provides522', 'a practical, high-impact solution for accelerating523', 'inference in large language models.524', '8', '8Limitations525', '(1) Cost of Building External Datastores.Al-526', 'though our internal-knowledge strategy only relies527', 'on sequences observed during generation (and thus528', 'requires no extra data), our external-knowledge529', 'approach depends on constructing and indexing a530', 'sizeable datastore from potentially large corpora.531', 'This process can be time-consuming and memory-532', 'intensive, particularly in domains where data up-533', 'dates frequently or storage is constrained. While534', 'this additional investment can yield substantial535', 'speedups by increasing token acceptance rates, it536', 'may not be universally feasible or cost-effective.537', '(2) Limited Evaluation Scope.Our experiments538', 'center primarily on English-language benchmarks539', 'in conversational and coding tasks using LLaMA-540', 'based models. Although SPECTRA can, in princi-541', 'ple, be applied to other models or languages, addi-542', 'tional factors such as domain-specific tokenization543', 'or specialized textual structures may affect the ac-544', 'ceptance rate and overall speedup. Future work is545', 'needed to assess the generality of SPECTRA across546', 'diverse linguistic settings (e.g., low-resource lan-547', 'guages or specialized technical documents) and for548', 'a wider range of model families (beyond LLaMA-549', 'based architectures) to confirm and refine its appli-550', 'cability.551', 'References552', 'Jacob Austin, Augustus Odena, Maxwell Nye, Maarten553', 'Bosma, Henryk Michalewski, David Dohan, Ellen554', 'Jiang, Carrie Cai, Michael Terry, Quoc Le, and others.555', '2021. Program synthesis with large language models.556', 'arXiv preprint arXiv:2108.07732.557', 'Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,558', 'Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei559', 'Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,560', 'Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,561', 'Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,562', 'Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong563', 'Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-564', 'guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,565', 'Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,566', 'Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-567', 'uan Zhang, Yichang Zhang, Zhenru Zhang, Chang568', 'Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang569', 'Zhu. 2023. Qwen Technical Report.570', 'Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu571', 'Peng, Jason D. Lee, Deming Chen, and Tri Dao.572', '2024. MEDUSA: Simple LLM inference acceler-573', 'ation framework with multiple decoding heads. In574', 'Proceedings of the 41st International Conference on575', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-576', 'enna, Austria.577', 'Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,578', 'Jean-Baptiste Lespiau, Laurent Sifre, and John579', 'Jumper. 2023. Accelerating Large Language Model580', 'Decoding with Speculative Sampling.581', 'Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,582', 'Henrique Ponde De Oliveira Pinto, Jared Kaplan,583', 'Harri Edwards, Yuri Burda, Nicholas Joseph, Greg584', 'Brockman, and others. 2021.Evaluating large585', 'language models trained on code. arXiv preprint586', 'arXiv:2107.03374.587', 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,588', 'Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias589', 'Plappert, Jerry Tworek, Jacob Hilton, Reiichiro590', 'Nakano, and others. 2021.Training verifiers591', 'to solve math word problems.arXiv preprint592', 'arXiv:2110.14168.593', 'Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin,594', 'Shengding Hu, Zhiyuan Liu, Maosong Sun, and595', 'Bowen Zhou. 2023. Enhancing Chat Language Mod-596', 'els by Scaling High-quality Instructional Conversa-597', 'tions. In Proceedings of the 2023 Conference on598', 'Empirical Methods in Natural Language Processing,599', 'pages 3029–3051, Singapore. Association for Com-600', 'putational Linguistics.601', 'Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang,602', 'Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng603', 'Sha, Xin Peng, and Yiling Lou. 2023.Classe-604', 'val: A manually-crafted benchmark for evaluating605', 'llms on class-level code generation. arXiv preprint606', 'arXiv:2308.01861.607', 'Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,608', 'Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,609', 'Akhil Mathur, Alan Schelten, Amy Yang, Angela610', 'Fan, et al. 2024. The llama 3 herd of models. arXiv611', 'preprint arXiv:2407.21783.612', 'Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,613', 'Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas614', 'Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed615', 'Roman, Ahmed Aly, Beidi Chen, and Carole-Jean616', 'Wu. 2024. LayerSkip: Enabling Early Exit Infer-617', 'ence and Self-Speculative Decoding. In Proceedings618', 'of the 62nd Annual Meeting of the Association for619', 'Computational Linguistics (Volume 1: Long Papers),620', 'pages 12622–12642, Bangkok, Thailand. Association621', 'for Computational Linguistics.622', 'Othmane Friha, Mohamed Amine Ferrag, Burak623', 'Kantarci, Burak Cakmak, Arda Ozgun, and Nassira624', 'Ghoualmi-Zine. 2024. Llm-based edge intelligence:625', 'A comprehensive survey on architectures, applica-626', 'tions, security and trustworthiness. IEEE Open Jour-627', 'nal of the Communications Society.628', 'Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.629', '2024.Break the sequential dependency of LLM630', 'inference using LOOKAHEAD DECODING. In631', 'Proceedings of the 41st International Conference on632', '9', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-633', 'enna, Austria.634', 'Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024.635', 'Minillm: Knowledge distillation of large language636', 'models. In The Twelfth International Conference on637', 'Learning Representations.638', 'Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and639', 'Di He. 2024. REST: Retrieval-Based Speculative640', 'Decoding. In Proceedings of the 2024 Conference of641', 'the North American Chapter of the Association for642', 'Computational Linguistics: Human Language Tech-643', 'nologies (Volume 1: Long Papers), pages 1582–1595,644', 'Mexico City, Mexico. Association for Computational645', 'Linguistics.646', 'Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and647', 'Yejin Choi. 2020. The Curious Case of Neural Text648', 'Degeneration. In International Conference on Learn-649', 'ing Representations.650', 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-651', 'sch, Chris Bamford, Devendra Singh Chaplot, Diego652', 'de las Casas, Florian Bressand, Gianna Lengyel, Guil-653', 'laume Lample, Lucile Saulnier, Lélio Renard Lavaud,654', 'Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,655', 'Thibaut Lavril, Thomas Wang, Timothée Lacroix,656', 'and William El Sayed. 2023. Mistral 7B.657', 'Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI,658', 'Chenghao Mou, Yacine Jernite, Margaret Mitchell,659', 'Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf,660', 'Dzmitry Bahdanau, Leandro Von Werra, and Harm de661', 'Vries. 2023. The Stack: 3 TB of permissively li-662', 'censed source code. Transactions on Machine Learn-663', 'ing Research.664', 'Wouter Kool, Herke van Hoof, and Max Welling. 2020.665', 'Ancestral Gumbel-Top-k Sampling for Sampling666', 'Without Replacement. Journal of Machine Learning667', 'Research, 21(47):1–36.668', 'Khang Nguyen Le, Ryo Sato, Dai Nakashima, Takeshi669', 'Suzuki, and Minh Le Nguyen. 2025. Optiprune: Ef-670', 'fective pruning approach for every target sparsity. In671', 'Proceedings of the 31st International Conference on672', 'Computational Linguistics, pages 3600–3612.673', 'Yaniv Leviathan, Matan Kalman, and Yossi Matias.674', '2023. Fast inference from transformers via spec-675', 'ulative decoding. In Proceedings of the 40th Interna-676', 'tional Conference on Machine Learning, ICML’23.677', 'JMLR.org. Place: Honolulu, Hawaii, USA.678', 'Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen,679', 'Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024a.680', 'Nearest Neighbor Speculative Decoding for LLM681', 'Generation and Attribution. In The Thirty-eighth An-682', 'nual Conference on Neural Information Processing683', 'Systems.684', 'Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang685', 'Zhang. 2024b. EAGLE-2: Faster Inference of Lan-686', 'guage Models with Dynamic Draft Trees. In Proceed-687', 'ings of the 2024 Conference on Empirical Methods688', 'in Natural Language Processing, pages 7421–7432,689', 'Miami, Florida, USA. Association for Computational690', 'Linguistics.691', 'Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-692', 'Ming Chen, Wei-Chen Wang, Guangxuan Xiao,693', 'Xingyu Dang, Chuang Gan, and Song Han. 2024.694', 'Awq: Activation-aware weight quantization for on-695', 'device llm compression and acceleration. Proceed-696', 'ings of Machine Learning and Systems, 6:87–100.697', 'Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng698', 'Ni, Duyu Tang, Kai Han, and Yunhe Wang. 2024a.699', 'Kangaroo: Lossless Self-Speculative Decoding for700', 'Accelerating LLMs via Double Early Exiting. In The701', 'Thirty-eighth Annual Conference on Neural Informa-702', 'tion Processing Systems.703', 'Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan704', 'Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xi-705', 'aohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu,706', 'Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang,707', 'Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu,708', 'and Bao Ge. 2025. Understanding llms: A compre-709', 'hensive overview from training to inference. Neuro-710', 'computing, 620:129190.711', 'Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie712', 'Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,713', 'Raghuraman Krishnamoorthi, and Vikas Chandra.714', '2024b. LLM-QAT: Data-free quantization aware715', 'training for large language models. In Findings of716', 'the Association for Computational Linguistics: ACL717', '2024, pages 467–484, Bangkok, Thailand. Associa-718', 'tion for Computational Linguistics.719', 'Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.720', 'Llm-pruner: On the structural pruning of large lan-721', 'guage models. Advances in neural information pro-722', 'cessing systems, 36:21702–21720.723', 'Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao724', 'Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee725', 'Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan726', 'Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Ab-727', 'hyankar, and Zhihao Jia. 2024. SpecInfer: Accelerat-728', 'ing Large Language Model Serving with Tree-based729', 'Speculative Inference and Verification. In Proceed-730', 'ings of the 29th ACM International Conference on Ar-731', 'chitectural Support for Programming Languages and732', 'Operating Systems, Volume 3, ASPLOS ’24, pages733', '932–949, New York, NY, USA. Association for Com-734', 'puting Machinery. Event-place: La Jolla, CA, USA.735', 'Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,736', 'Ça˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-737', 'tive text summarization using sequence-to-sequence738', 'RNNs and beyond.In Proceedings of the 20th739', 'SIGNLL Conference on Computational Natural Lan-740', 'guage Learning, pages 280–290, Berlin, Germany.741', 'Association for Computational Linguistics.742', 'Shashi Narayan, Shay B Cohen, and Mirella Lap-743', 'ata. 2018.Don’t give me the details, just the744', '10', 'summary!topic-aware convolutional neural net-745', 'works for extreme summarization. arXiv preprint746', 'arXiv:1808.08745.747', 'OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,748', 'Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-749', 'man, Diogo Almeida, Janko Altenschmidt, Sam Alt-750', 'man, Shyamal Anadkat, Red Avila, Igor Babuschkin,751', 'Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-752', 'ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-753', 'wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,754', 'Christopher Berner, Lenny Bogdonoff, Oleg Boiko,755', 'Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-756', 'man, Tim Brooks, Miles Brundage, Kevin Button,757', 'Trevor Cai, Rosie Campbell, Andrew Cann, Brittany758', 'Carey, Chelsea Carlson, Rory Carmichael, Brooke759', 'Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully760', 'Chen, Ruby Chen, Jason Chen, Mark Chen, Ben761', 'Chess, Chester Cho, Casey Chu, Hyung Won Chung,762', 'Dave Cummings, Jeremiah Currier, Yunxing Dai,763', 'Cory Decareaux, Thomas Degry, Noah Deutsch,764', 'Damien Deville, Arka Dhar, David Dohan, Steve765', 'Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,766', 'Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,767', 'Simón Posada Fishman, Juston Forte, Isabella Ful-768', 'ford, Leo Gao, Elie Georges, Christian Gibson, Vik769', 'Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-770', 'Lopes, Jonathan Gordon, Morgan Grafstein, Scott771', 'Gray, Ryan Greene, Joshua Gross, Shixiang Shane772', 'Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,773', 'Yuchen He, Mike Heaton, Johannes Heidecke, Chris774', 'Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,775', 'Brandon Houghton, Kenny Hsu, Shengli Hu, Xin776', 'Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,777', 'Joanne Jang, Angela Jiang, Roger Jiang, Haozhun778', 'Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-779', 'woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-780', 'mali, Ingmar Kanitscheider, Nitish Shirish Keskar,781', 'Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,782', 'Christina Kim, Yongjik Kim, Jan Hendrik Kirch-783', 'ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,784', 'Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-785', 'stantinidis, Kyle Kosic, Gretchen Krueger, Vishal786', 'Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan787', 'Leike, Jade Leung, Daniel Levy, Chak Ming Li,788', 'Rachel Lim, Molly Lin, Stephanie Lin, Mateusz789', 'Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,790', 'Anna Makanju, Kim Malfacini, Sam Manning, Todor791', 'Markov, Yaniv Markovski, Bianca Martin, Katie792', 'Mayer, Andrew Mayne, Bob McGrew, Scott Mayer793', 'McKinney, Christine McLeavey, Paul McMillan,794', 'Jake McNeil, David Medina, Aalok Mehta, Jacob795', 'Menick, Luke Metz, Andrey Mishchenko, Pamela796', 'Mishkin, Vinnie Monaco, Evan Morikawa, Daniel797', 'Mossing, Tong Mu, Mira Murati, Oleg Murk, David798', 'Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,799', 'Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,800', 'Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex801', 'Paino, Joe Palermo, Ashley Pantuliano, Giambat-802', 'tista Parascandolo, Joel Parish, Emy Parparita, Alex803', 'Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-804', 'man, Filipe de Avila Belbute Peres, Michael Petrov,805', 'Henrique Ponde de Oliveira Pinto, Michael, Poko-806', 'rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-807', 'ell, Alethea Power, Boris Power, Elizabeth Proehl,808', 'Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,809', 'Cameron Raymond, Francis Real, Kendra Rimbach,810', 'Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-811', 'der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,812', 'Girish Sastry, Heather Schmidt, David Schnurr, John813', 'Schulman, Daniel Selsam, Kyla Sheppard, Toki814', 'Sherbakov, Jessica Shieh, Sarah Shoker, Pranav815', 'Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,816', 'Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin817', 'Sokolowsky, Yang Song, Natalie Staudacher, Fe-818', 'lipe Petroski Such, Natalie Summers, Ilya Sutskever,819', 'Jie Tang, Nikolas Tezak, Madeleine B. Thompson,820', 'Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,821', 'Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-822', 'lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,823', 'Chelsea Voss, Carroll Wainwright, Justin Jay Wang,824', 'Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,825', 'C. J. Weinmann, Akila Welihinda, Peter Welin-826', 'der, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave827', 'Willner, Clemens Winter, Samuel Wolrich, Hannah828', 'Wong, Lauren Workman, Sherwin Wu, Jeff Wu,829', 'Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin830', 'Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers,831', 'Chong Zhang, Marvin Zhang, Shengjia Zhao, Tian-832', 'hao Zheng, Juntang Zhuang, William Zhuk, and Bar-833', 'ret Zoph. 2024. GPT-4 Technical Report.834', 'Jie Ou, Yueming Chen, and Prof. Tian. 2024. Lossless835', 'Acceleration of Large Language Model via Adap-836', 'tive N-gram Parallel Decoding. In Proceedings of837', 'the 2024 Conference of the North American Chap-838', 'ter of the Association for Computational Linguistics:839', 'Human Language Technologies (Volume 6: Industry840', 'Track), pages 10–22, Mexico City, Mexico. Associa-841', 'tion for Computational Linguistics.842', 'Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun843', 'Sim, and Jae W. Lee. 2024. Any-precision llm: Low-844', 'cost deployment of multiple, different-sized llms. In845', 'Proceedings of the 41st International Conference on846', 'Machine Learning.847', 'Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten848', 'Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,849', 'Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy850', 'Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna851', 'Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron852', 'Grattafiori, Wenhan Xiong, Alexandre Défossez,853', 'Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-854', 'tin, Nicolas Usunier, Thomas Scialom, and Gabriel855', 'Synnaeve. 2024. Code Llama: Open Foundation856', 'Models for Code. _eprint: 2308.12950.857', 'Andrea Santilli, Silvio Severino, Emilian Postolache,858', 'Valentino Maiorca, Michele Mancusi, Riccardo859', 'Marin, and Emanuele Rodola. 2023. Accelerating860', 'transformer inference for translation via parallel de-861', 'coding. In Proceedings of the 61st Annual Meeting of862', 'the Association for Computational Linguistics (Vol-863', 'ume 1: Long Papers), pages 12336–12355, Toronto,864', 'Canada. Association for Computational Linguistics.865', 'Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.866', '11', '2023a. A simple and effective pruning approach for867', 'large language models. ArXiv, abs/2306.11695.868', 'Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-869', 'mad Beirami, Himanshu Jain, and Felix Yu. 2023b.870', 'SpecTr: fast speculative decoding via optimal trans-871', 'port. In Proceedings of the 37th International Con-872', 'ference on Neural Information Processing Systems,873', 'NIPS ’23, Red Hook, NY, USA. Curran Associates874', 'Inc. Event-place: New Orleans, LA, USA.875', 'Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-876', 'bert, Amjad Almahairi, Yasmine Babaei, Nikolay877', 'Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti878', 'Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton879', 'Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,880', 'Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,881', 'Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-882', 'thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan883', 'Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,884', 'Isabel Kloumann, Artem Korenev, Punit Singh Koura,885', 'Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-886', 'ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-887', 'tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-888', 'bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-889', 'stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,890', 'Ruan Silva, Eric Michael Smith, Ranjan Subrama-891', 'nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-892', 'lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,893', 'Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,894', 'Melanie Kambadur, Sharan Narang, Aurelien Ro-895', 'driguez, Robert Stojnic, Sergey Edunov, and Thomas896', 'Scialom. 2023. Llama 2: Open Foundation and Fine-897', 'Tuned Chat Models.898', 'Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi899', 'Chen. 2023. Sheared llama: Accelerating language900', 'model pre-training via structured pruning. ArXiv,901', 'abs/2310.06694.902', 'Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin903', 'Jiang, Linjun Yang, Rangan Majumder, and Furu904', 'Wei. 2023.Inference with Reference: Lossless905', 'Acceleration of Large Language Models. _eprint:906', '2304.04487.907', 'Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris908', 'Papailiopoulos, and Kangwook Lee. 2024. Predictive909', 'Pipelined Decoding: A Compute-Latency Trade-off910', 'for Exact LLM Decoding. Transactions on Machine911', 'Learning Research.912', 'Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,913', 'Gang Chen, and Sharad Mehrotra. 2024a. Draft&914', 'Verify: Lossless Large Language Model Acceleration915', 'via Self-Speculative Decoding. In Proceedings of the916', '62nd Annual Meeting of the Association for Compu-917', 'tational Linguistics (Volume 1: Long Papers), pages918', '11263–11282, Bangkok, Thailand. Association for919', 'Computational Linguistics.920', 'Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng921', 'Chen, and Jinan Xu. 2024b. Dual-space knowledge922', 'distillation for large language models. In Proceed-923', 'ings of the 2024 Conference on Empirical Methods in924', 'Natural Language Processing, pages 18164–18181,925', 'Miami, Florida, USA. Association for Computational926', 'Linguistics.927', 'Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn928', 'Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,929', 'Tianqi Chen, and Baris Kasikci. 2024. Atom: Low-930', 'bit quantization for efficient and accurate llm serv-931', 'ing. Proceedings of Machine Learning and Systems,932', '6:196–209.933', 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan934', 'Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,935', 'Zhuohan Li, Dacheng Li, Eric Xing, and others. 2023.936', 'Judging llm-as-a-judge with mt-bench and chatbot937', 'arena. Advances in Neural Information Processing938', 'Systems, 36:46595–46623.939', 'Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,940', 'Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv941', 'Kumar, Jean-François Kagy, and Rishabh Agarwal.942', '2024. DistillSpec: Improving Speculative Decoding943', 'via Knowledge Distillation. In The Twelfth Interna-944', 'tional Conference on Learning Representations.945', 'AMore on Speculative Decoding946', 'Autoregressive decoding (Touvron et al., 2023; Bai947', 'et al., 2023; Jiang et al., 2023; OpenAI et al., 2024),948', 'suffers from inefficiency because it generates text949', 'one token at a time (Figure 5, Left).Specula-950', 'tive decoding (Chen et al., 2023; Leviathan et al.,951', '2023) follows a guess-and-verify paradigm (Figure952', '5, Right). In speculative decoding, a smaller LLM953', '(draft model) (Chen et al., 2023; Leviathan et al.,954', '2023; Miao et al., 2024; Sun et al., 2023b; Zhou955', 'et al., 2024; Cai et al., 2024) or the original LLM956', 'trained in a specialized manner (self-speculative957', 'decoding) (Elhoushi et al., 2024; Liu et al., 2024a;958', 'Yang et al., 2024; Zhang et al., 2024a; Li et al.,959', '2024b) predicts multiple tokens in advance. The960', 'original LLM then verifies these predictions in par-961', 'allel, improving efficiency.962', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'Autoregressive Decoding', '...', 'Speculate ', '(make guesses)', 'Speculative Decoding', 'A', 'B', 'B', 'C', 'C', 'D', 'T', 'Target LLM', 'AAcceptReject', 'Figure 5: Examples of Autoregressive decoding (Left)and Speculative Decoding (Right). While autoregres-sive decoding generates one token per forward step,speculative decoding generates three tokens with oneforward step.', '12', 'LLMs process discrete integer sequences as in-963', 'puts, where each integer represents a token. We de-964', 'fine the input sequence as x = (x1, x2, . . . , xs) ∈965', 'Ns of length s, and denote a slice of length m at966', 'step t as x1:m = (x1, x2, . . . , xm). The output of967', 'an LLM represents the probability distribution over968', 'the next token. The probability of generating the969', 's-th token, conditioned on all preceding tokens, is970', 'given by PM(xs | x1:s−1). The next token xs is971', 'then sampled from this distribution using various972', 'methods (e.g., greedy, top-k, and top-p sampling;973', 'see (Kool et al., 2020; Holtzman et al., 2020)). In974', 'the case of greedy sampling, the next token is se-975', 'lected as xs = arg max PM(xs | x1:s−1)976', 'Let x0 be the prompt tokens provided by the977', 'user. The LLM generates an output sequence of978', 'length m, with each generated token yi computed979', 'autoregressively. Assuming greedy sampling, the980', 'decoding process follows:981', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y1 = arg max PM(y1 | x) ', 'y2 = arg max PM(y2 | y1, x)... ', 'ym = arg max PM(ym | y1:m−1, x).', '(1)982', 'A.1Speculative Decoding983', 'Speculative decoding follows a Guess-And-Verify984', 'approach, where multiple candidate future to-985', 'kens are speculated and subsequently verified986', 'in a single decoding step.With tree attention987', '(Miao et al., 2024), multiple drafts can be ver-988', 'ified simultaneously. Let G denote the number989', 'of guesses, and define the set of guesses as ˜Y =990', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence991', 'has length K. The j-th token of the i-th guess is992', 'denoted as ˜y(i) ', 'j .993', 'In the case of speculative decoding with greedy994', 'sampling, given the prompt x, a drafting method995', 'is used to generate the draft sequences ˜Y . Using996', 'these drafts, the LLM then computes the true tokens997', '(y′1, y′2, . . . , y′K) in parallel. For instance, for the998', 'guess sequence ˜y(0), the true tokens are determined999', 'as:1000', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y′1 = arg max PM(y1 | x)', 'y′2 = arg max PM(y2 | ˜y(0) ', '1 , x)...', 'y′K = arg max PM(yK | ˜y(0) ', '1:K−1, x).', '(2)1001', 'These generated tokens are then verified. Let h1002', 'be the highest number of correct guessed tokens1003', 'across all guesses. Consequently, h + 1 tokens are1004', 'generated in one forward step. Algorithm 2 out-1005', 'lines speculative decoding with greedy sampling.1006', 'BImplementation Details1007', 'B.1Frameworks and Libraries1008', 'We implement SPECTRA in Python using PyTorch1009', '2.1.0 and the Hugging Face transformers library.1010', 'For large-scale model loading (e.g., LLaMA-2-1011', '70B, LLaMA-3-70B), we employ 16-bit (FP16)1012', 'precision with a pre-allocated key-value cache.1013', 'B.2Models and Checkpoints1014', 'We run our experiments primarily with:1015', '• LLaMA-2-Chat (Touvron et al., 2023) in1016', 'sizes 7B, 13B, 70B.1017', '• CodeLlama (Rozière et al., 2024) in sizes 7B1018', 'and 13B.1019', '• LLaMA-3-Instruct (Dubey et al., 2024) in1020', 'sizes 8B and 70B.1021', 'All checkpoints are obtained from the official or1022', 'Hugging Face repositories without fine-tuning or1023', 'modification. For each model, we enable half-1024', 'precision inference. We also verify numerically (by1025', 'comparing 32-bit and 16-bit outputs) that specula-1026', 'tive decoding preserves exact or near-exact token1027', 'sequences within typical floating-point tolerances.1028', 'B.3Hardware1029', 'Most experiments are conducted on a single1030', 'NVIDIA A100 GPU with 80GB of memory. We1031', 'also evaluate on other NVIDIA GPUs (RTX 3090,1032', 'RTX 8000, A40, A6000) to study hardware-1033', 'specific scaling. For the largest checkpoints (70B)1034', 'that do not fit on a single GPU under certain con-1035', 'figurations, we optionally distribute them across1036', 'multiple GPUs (2x, 4x, or 8x H100) using standard1037', 'pipeline-parallelism from Hugging Face’s library.1038', 'B.4Hyperparameters1039', 'Lookahead, REST, and ANPD.We replicate1040', 'each baseline using their publicly available GitHub1041', 'code, keeping to the default settings and hyperpa-1042', 'rameters outlined in the original papers.1043', 'Spectra.By default, we use a 5-gram setup for1044', 'our forward/backward dictionaries, storing all sub-1045', 'sequences (i.e., sub-ngrams). We also maintain a1046', 'candidate pool of size W = 15 per key to generate1047', '13', 'new n-gram records; after each forward pass, can-1048', 'didate sequences are shifted by one token and then1049', 're-populated. We introduce a threshold τ ∈[0, 1],1050', 'default set to 0.1, to decide when to force the se-1051', 'lection of a token not yet in the forward dictionary.1052', 'This mechanism balances coverage of unseen pre-1053', 'fixes with reinforcing common contexts. At every1054', 'speculative decoding step, we allow up to G = 601055', 'guess tokens. Internal guesses receive priority, and1056', 'if there is still capacity under the guess limit, we1057', 'add external guesses.1058', 'For external lookups, we implement a trie struc-1059', 'ture for rapid prefix queries, following a design1060', 'similar to REST (He et al., 2024). For conver-1061', 'sation tasks (e.g., MT-Bench), we gather approxi-1062', 'mately 100k examples from the UltraChat dataset1063', '(Ding et al., 2023), focusing on those with minimal1064', 'perplexity under the same LLM we aim to accel-1065', 'erate. For code tasks (e.g., HumanEval, MBPP),1066', 'we draw from TheStack (Kocetkov et al., 2023)1067', 'and again refine it to the 100k snippets with the1068', 'lowest perplexity for memory efficiency. We mea-1069', 'sure perplexity by running a single forward pass (in1070', 'streaming mode) over candidate samples and rank-1071', 'ing them. Despite being relatively small (100k),1072', 'this curated corpus achieves robust guess quality.1073', 'All speedup and throughput metrics are com-1074', 'puted at a batch size of 1. In code generation tasks,1075', 'the maximum generation length is typically 5121076', 'tokens, whereas for conversation tasks (MT-Bench,1077', 'GSM8K), we allow up to 1024 tokens or stop early1078', 'if the model outputs an end-of-sequence token. All1079', 'random seeds are set to 0.1080', 'CDetails Results with Throughputs1081', 'We provide a detailed throughput analysis to com-1082', 'plement the speedup ratios reported in the main1083', 'text. Our goal is to demonstrate how SPECTRA1084', 'scales across various model sizes, datasets, and1085', 'GPU architectures. We measure throughput using1086', 'two key metrics:1087', '• Macro Throughput (Mac-TP). Calculated1088', 'as the average of per-generation token-1089', 'processing rates—i.e., for each generation1090', 'step i, we compute tokeni/timei and then1091', 'average over all steps.1092', '• Micro Throughput (Mic-TP). Calculated as1093', 'the total number of generated tokens divided1094', 'by the total elapsed time1095', 'Table 4 focuses on GSM8K and MTBench per-1096', 'formance across four different GPU models, while1097', 'Table 3 provides more granular results on ad-1098', 'ditional datasets and model configurations.In1099', 'all cases, SPECTRA consistently achieves higher1100', 'throughput than both non-speculative baselines and1101', 'other training-free accelerators, as evidenced by im-1102', 'provements in both Mic-TP and Mac-TP. Notably,1103', 'this performance advantage remains stable even on1104', 'older GPUs (e.g., the RTX 3090 and RTX 8000),1105', 'demonstrating SPECTRA’s robustness to varying1106', 'hardware capabilities.1107', 'DEvaluating SPECTRA in Different GPU1108', 'Types1109', 'Different GPU types.Table 5 reports speedups1110', 'on GSM8K and MTBench across four GPUs with1111', 'varying memory throughput and compute capabili-1112', 'ties. While absolute wall-clock times differ across1113', 'GPUs, the relative accelerations remain consistent.1114', 'SPECTRA consistently outperforms other baselines,1115', 'including Lookahead, achieving higher speedups in1116', 'all cases. On older GPUs (e.g., RTX 3090 or RTX1117', '8000), the gap between Lookahead and SPECTRA1118', 'narrows slightly due to less efficient parallelism,1119', 'but SPECTRA maintains its lead. These results1120', 'demonstrate that SPECTRA is robust to hardware1121', 'variations and effective across both data-center and1122', 'consumer-grade GPUs.1123', 'EEvaluating SPECTRA in Multi-GPU1124', 'Environments1125', 'A critical consideration for practical deployment is1126', 'how SPECTRA scales when models are distributed1127', 'across multiple GPUs—a common requirement for1128', 'large LLMs exceeding single-device memory ca-1129', 'pacity. To evaluate this, we measure SPECTRA’s1130', 'performance under three distributed configurations1131', 'of LLaMA-2-70B: (1) 2xH100 with full precision,1132', '(2) 4xH100 with full precision, and (3) 8xH1001133', 'with full precision. We also include a baseline1134', 'of 1xH100 with 8-bit quantization for memory-1135', 'constrained single-GPU inference. Table 6 reports1136', 'throughput and speedup metrics.1137', 'SPECTRA achieves consistent speedups of 2.00—1138', '2.03× across all multi-GPU configurations while1139', 'maintaining a stable compression ratio (τ) of 2.52.1140', 'This demonstrates robust scalability—partitioning1141', 'model weights introduces minimal overhead, and1142', 'the speculative verification process remains effi-1143', 'cient despite inter-GPU communication. Notably,1144', '14', 'ClassevalGSM8KHumanevalMBPPMTBenchModelMethodMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TP', 'Greedy (temperature=0)', 'CL-13BAutoregressive30.8530.8532.0332.0332.3532.3532.0732.0730.6930.63 CL-13BANPD59.7758.0389.9989.1867.4364.6586.7686.4180.1076.68 CL-13BLookahead69.2868.6289.7389.0074.3373.2393.3892.8079.3878.67 CL-13BREST39.5337.7329.9329.4751.1547.4927.4127.3928.9227.18 CL-13BSPECTRA (Ours)73.4772.9893.3693.2384.9184.41105.44105.3981.3280.68', 'CL-7BAutoregressive41.1741.1741.1741.1741.4141.4141.6041.6038.9138.93 CL-7BANPD94.7693.02132.26131.3089.2687.13131.35130.99130.41126.64 CL-7BLookahead106.51105.95123.04121.90103.45103.51120.75120.23125.58124.77 CL-7BREST59.4956.6137.6137.2170.3865.2240.1140.0939.6436.70 CL-7BSPECTRA (Ours)111.09110.68137.24136.86122.54122.41148.32148.07143.98144.32', 'L2-13BAutoregressive31.8531.5632.4032.4332.2732.2732.1932.1931.9331.78 L2-13BANPD43.3044.4447.5445.2243.2442.2836.2035.8437.4434.84 L2-13BLookahead57.4958.9447.4447.6255.7655.5844.4144.1548.1146.62 L2-13BREST38.8137.7430.3630.2240.4739.7030.7030.6736.3937.02 L2-13BSPECTRA (Ours)63.6464.3159.2158.6363.3963.1852.4352.1956.0453.75', 'L2-70BAutoregressive2.602.602.612.612.612.612.632.632.602.60 L2-70BANPD4.724.804.254.104.854.763.073.073.473.30 L2-70BLookahead6.907.164.875.126.716.733.923.935.055.02 L2-70BSPECTRA (Ours)8.078.356.586.758.418.414.884.886.326.22', 'L2-7BAutoregressive40.3340.3241.0141.0341.1441.1341.0041.0440.4840.50 L2-7BANPD65.5468.1062.4059.3863.2759.9848.9447.6752.4750.06 L2-7BLookahead88.4191.0568.0068.2084.6983.8759.7960.7670.0469.07 L2-7BREST54.7453.9341.4341.3857.9956.4141.2840.7450.5851.79 L2-7BSPECTRA (Ours)96.8898.7586.5185.5098.7798.3872.3973.2281.9379.20', 'L3-70BAutoregressive2.582.572.582.582.592.592.592.592.552.55 L3-70BANPD3.974.193.863.724.724.753.773.593.143.03 L3-70BLookahead6.176.473.993.966.636.753.703.664.494.53 L3-70BSPECTRA (Ours)6.877.185.435.347.337.505.014.885.255.16', 'L3-8BAutoregressive36.5936.5836.7436.7436.2036.2135.2435.2036.5536.69 L3-8BANPD77.2178.76141.89141.3666.3165.57118.47112.9541.7740.20 L3-8BLookahead94.9297.09136.32135.9289.9990.47133.67133.1256.0955.49 L3-8BSPECTRA (Ours)103.61105.88142.89142.7292.8693.16143.80142.7261.6960.22', 'Sampling (temperature=1.0)', 'CL-13BAutoregressive30.9030.6431.3831.3731.2431.3931.4631.4530.7130.67 CL-13BANPD35.4834.8633.5432.3432.6434.3631.5730.9570.9265.68 CL-13BLookahead42.5440.7433.7932.4940.2542.1732.0231.1971.5068.46 CL-13BREST35.1533.2225.6725.2439.5838.4926.4325.8928.4126.69 CL-13BSPECTRA (Ours)51.8650.0437.5735.6751.6052.6436.2935.2772.9069.98', 'CL-7BAutoregressive39.6039.5840.8540.8740.0540.1040.8140.8140.4940.50 CL-7BANPD50.8951.7647.4446.6844.1446.3445.8645.81112.29103.57 CL-7BLookahead60.8760.2948.5447.6457.1261.1448.6448.27110.07105.00 CL-7BREST48.6446.4135.9835.4653.3552.2637.0436.5739.3636.51 CL-7BSPECTRA (Ours)71.7071.7855.2452.8167.2769.2054.4852.91112.43108.49', 'L2-13BAutoregressive31.2331.1731.4431.4731.4131.4232.0232.0631.6731.59 L2-13BANPD37.5337.9439.1137.9936.7936.7532.9732.7136.9134.34 L2-13BLookahead47.5947.3541.6041.7646.3346.5137.8237.8247.3545.48 L2-13BREST36.7836.1729.3329.2537.4636.7129.3829.2835.5036.21 L2-13BSPECTRA (Ours)53.1352.2848.6048.1152.9353.1142.9543.0354.9852.42', 'L2-7BAutoregressive39.8939.8840.5840.5940.0940.1040.5940.6640.6540.70 L2-7BANPD52.1452.7854.2352.9051.4050.9744.7343.7750.9248.24 L2-7BLookahead70.8271.1761.1561.3468.7869.0150.8451.8368.2766.77 L2-7BREST50.3549.9940.1940.0950.8650.0638.9438.1849.1250.54 L2-7BSPECTRA (Ours)78.4678.7472.1371.6881.7181.7659.7760.0980.2177.00', 'L3-8BAutoregressive35.7535.7635.1635.1736.0136.0236.0536.0735.3935.48 L3-8BANPD44.7143.7269.1266.7351.4851.5768.0364.5440.8439.23 L3-8BLookahead53.0550.5772.6869.1164.5963.7971.8868.9055.4653.74 L3-8BSPECTRA (Ours)69.5068.9279.8876.5369.0968.6278.9976.6960.3357.69', 'Table 3: Micro throughput (Mic-TP) and Macro throughput (Mac-TP) across multiple tasks and models.', '15', 'GPUMethodGSM8KMTBenchMac-TPMic-TPMac-TPMic-TP', 'A40Autoregressive32.6632.6632.1431.66 ', 'Lookahead48.5948.7349.1347.96 ', 'SPECTRA62.5661.5259.0056.80', 'A6000Autoregressive39.1539.1738.7838.24 ', 'Lookahead58.1358.3058.8457.40 ', 'SPECTRA75.2074.1671.369.28', 'RTX8000Autoregressive34.0334.2734.2134.02 ', 'Lookahead45.2545.4245.7344.16 ', 'SPECTRA57.9557.0954.1652.32', 'RTX3090Autoregressive40.6740.7641.1741.22 ', 'Lookahead53.6953.7553.5152.09 ', 'SPECTRA74.8773.8871.5869.79', 'Table 4: Throughput results for different GPU types on GSM8K and MTBench.', 'GPUMethodGSM8KMTBenchspeedupτspeedupτ', 'A40Lookahead1.491.931.532.07 ', 'SPECTRA1.922.461.842.36', 'A6000Lookahead1.481.921.522.06 ', 'SPECTRA1.922.461.842.36', 'RTX8000Lookahead1.331.931.342.08 ', 'SPECTRA1.702.461.582.35', 'RTX3090Lookahead1.321.921.302.06 ', 'SPECTRA1.842.461.742.36', 'Table 5: Hardware scalability of SPECTRA decoding onGSM8K and MTBench for various GPU architectures.', 'even in the quantized single-GPU setting, SPEC-1145', 'TRA provides a 2.43× speedup, outperforming1146', 'standard autoregressive decoding. These results1147', 'validate SPECTRA’s practicality for large-scale de-1148', 'ployments where memory constraints necessitate1149', 'distributed inference.1150', 'FVerifying Generation Quality with1151', 'SPECTRA Decoding1152', 'Greedy Decoding Performance.To assess the1153', 'quality of greedy decoding, we compare the in-1154', 'ference results of the LLaMA-2-7B Chat model1155', 'using SPECTRA Decoding against Hugging Face’s1156', 'standard greedy search. Our baseline consists of1157', 'single-precision (FP32) inference on 160 conver-1158', 'sational turns from the MT-Bench dataset. Under1159', 'FP32, SPECTRA Decoding produces identical out-1160', 'puts to the baseline.1161', 'However, when transitioning to half-precision1162', '(FP16), even Hugging Face’s native greedy search1163', 'generates 25 discrepancies (out of 160) compared1164', 'to the FP32 baseline. SPECTRA Decoding exhibits1165', 'a similar discrepancy rate (26), confirming that it1166', 'maintains the output distribution within the numer-1167', 'ical error margins typically observed in standard1168', 'half-precision inference libraries.1169', 'Sampling Decoding Performance.We also as-1170', 'sess generation quality under a stochastic sam-1171', 'pling setting (temperature = 1.0). As detailed in1172', 'Table 7, SPECTRA Decoding produces ROUGE-1173', '1, ROUGE-2, and ROUGE-L scores on both the1174', 'CNN/DailyMail (Nallapati et al., 2016) and XSum1175', '(Narayan et al., 2018) summarization datasets1176', 'that are nearly identical to those of standard1177', 'autoregressive sampling.At the same time,1178', 'SPECTRA achieves notable speedups (1.60× on1179', 'CNN/DailyMail and 1.69× on XSum) with com-1180', 'pression ratios of 2.05 and 2.08, respectively.1181', 'These results confirm that SPECTRA Decoding1182', 'accelerates inference while preserving generation1183', 'quality across diverse tasks.1184', 'These findings reaffirm that SPECTRA Decoding,1185', 'does not degrade generation quality compared to1186', 'conventional greedy or sampling-based methods.1187', 'GToken Acceptance Rate Analysis1188', 'Figure 6 plots the cumulative number of accepted1189', 'tokens versus decoding steps for each dataset (MT-1190', 'Bench, HumanEval, MBPP, and GSM8K). The1191', 'steeper ascent of the SPECTRA curve indicates that1192', 'our method requires substantially fewer decoding1193', 'steps compared to alternatives, for example, almost1194', 'two times shorter than ANPD. This improvement is1195', '16', 'GPU & Model SettingMethodMTBenchMac-TPMic-TPSpeedupτ', '1xH100 - Quantized Int8Autoregressive2.602.601.001.00 ', 'SPECTRA6.326.222.432.51', '2xH100 - FP16Autoregressive14.8114.701.001.00 ', 'SPECTRA29.6228.912.002.52', '4xH100 - FP16Autoregressive14.6014.481.001.00 ', 'SPECTRA29.6728.892.032.52', '8xH100 - FP16Autoregressive14.3914.281.001.00 ', 'SPECTRA29.2728.552.032.52', 'Table 6: Results in multi-GPU Enviroments on GSM8K and MTBench using LLama-2-chat-70B.', 'DatasetMethodROUGE-1ROUGE-2ROUGE-LSpeedupτ', 'CNNAutoregressive9.770.397.201.001.00 ', 'SPECTRA9.740.417.181.602.05', 'XSUMAutoregressive18.124.3612.431.001.00 ', 'SPECTRA18.134.4012.491.692.08', 'Table 7: Evaluation of SPECTRA Decoding on CNN/DailyMail and XSum using a temperature of 1.0. ROUGEscores, speedups over autoregressive decoding, and compression ratio (τ) are reported for LLaMA-2-7B-Chat.', 'attributed to a higher token acceptance rate, which1196', 'in turn reduces the overall number of decoding iter-1197', 'ations and enhances the efficiency of the generation1198', 'process.1199', 'HAlgorithms1200', '17', '0250500750', '0', '10', '20', '30', '40', 'MT-Bench', '0200400', '0', '10', '20', '30', '40', 'HumanEval', '050100150', '0', '5', '10', '15', 'MBPP', '0100200', '0', '10', '20', '30', '40', '50', '60', 'GSM8K', '#Accepted tokens (in thousands)', 'LookaheadRESTANPDSpectra (Ours)', 'Figure 6: Total number of accepted tokens across all samples at each decoding step.', 'Algorithm 2 Speculative Decoding (Multiple guesses and Greedy Sampling)', 'Given guess size K, number of guesses G, and target length T. Given initial prompt sequence x. ', 'while n < T do', 'Obtain multiple drafts ˜Y = {˜y(0), ˜y(1), . . . , ˜y(G)}.', 'In parallel, compute K + 1 verification tokens y′: for i = 1 : K do', 'y′(g) ', 'i= arg max PM(yi | ˜y(g) ', 'i−1, x),∀g ∈{0, . . . , G}', 'end forIdentify the sequence ˜y(g∗) with the highest token matches and the corresponding y′(g).', 'for t = 1 : K do', 'if y′(g) ', 't= ˜y(g∗) ', 'tthen', 'Set yn+t ←˜y(g∗) ', 'tand n ←n + 1.', 'else', 'yn+t ←y′(g) ', 'tand exit for loop.', 'end if', 'end for', 'end while', '18', 'Algorithm 3 Greedy Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function GREEDYVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x and all gi2’s outputs – totally N ', 'distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:P ←D[l][i]', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:if sj = arg max P then▷accepted, update all potential speculations and probabilities', '14:o.append(sj)', '15:is_accept ←1', '16:Vnew, Dnew ←∅, ∅', '17:for k = j to size(V ) do', '18:if sj = V [k] then', '19:Vnew.append(V [k])', '20:Dnew.append(D[k])', '21:end if', '22:end for', '23:V, D ←Vnew, Dnew', '24:break', '25:else▷rejected, go to next speculation', '26:j ←j + 1', '27:end if', '28:end while', '29:if is_accept then', '30:continue', '31:else▷guarantee one step movement', '32:o.append(arg max P)', '33:break', '34:end if', '35:end for', '36:if is_accept then', '37:o.append(arg max D[1]N)', '38:end if', '39:return o', '40: end function', '19', 'Algorithm 4 Sample Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function SAMPLEVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x0 and all gi2’s outputs – totally N ', 'probability distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:Pj ←D[1]i', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:sample r ∼U(0, 1)', '14:if r ≤Pj(sj) then▷accepted, update all potential speculations and probabilities', '15:o.append(sj)', '16:is_accept ←1', '17:Vnew, Dnew ←∅, ∅', '18:for k = j to size(V ) do', '19:if sj = V [k] then', '20:Vnew.append(V [k])', '21:Dnew.append(D[k])', '22:end if', '23:end for', '24:V, D ←Vnew, Dnew', '25:break', '26:else▷rejected, go to next speculation', '27:Pj(sj) ←0', '28:Pj+1 = norm(Pj)', '29:j ←j + 1', '30:end if', '31:end while', '32:if is_accept then', '33:continue', '34:else▷guarantee one step movement', '35:sample xnext ∼Pj', '36:o.append(xnext)', '37:break', '38:end if', '39:end for', '40:if is_accept then', '41:o.append(sample xnext ∼D[1]N)', '42:end if', '43:return o', '44: end function', '20']
2025-02-19 16:06:18 - WARNING: done parsing pdf
2025-02-19 16:06:18 - WARNING: start ner pdf
2025-02-19 16:06:22 - INFO: Loading Data
2025-02-19 16:06:22 - WARNING: 2025-02-19 16:06:22 - INFO: Loading Data
2025-02-19 16:06:25 - WARNING: Predicting NER ...
2025-02-19 16:06:28 - WARNING: Finished predicting.
2025-02-19 16:06:28 - WARNING: Converting to Brat format...
2025-02-19 16:06:28 - WARNING: # of discontinuous mentions:
2025-02-19 16:06:28 - WARNING:  
2025-02-19 16:06:28 - WARNING: 0
2025-02-19 16:06:28 - WARNING: Finished.
2025-02-19 16:06:28 - WARNING: start predict re
2025-02-19 16:06:32 - WARNING: Example:   0%|          | 0/39 [00:00<?, ?it/s]
2025-02-19 16:06:32 - WARNING: Example: 100%|##########| 39/39 [00:00<00:00, 2091.17it/s]
2025-02-19 16:06:32 - WARNING: # of documents 39.
2025-02-19 16:06:32 - WARNING: # of positive examples 0.
2025-02-19 16:06:32 - WARNING: # of negative examples 104.
2025-02-19 16:06:32 - WARNING: dict_keys(['511', '541', '544', '1364', '1478'])
2025-02-19 16:06:32 - WARNING: done predict re
2025-02-19 16:06:32 - WARNING: old num para 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: 1637
2025-02-19 16:06:32 - WARNING: start debug in ner re processing
2025-02-19 16:06:32 - WARNING: 1631
2025-02-19 16:06:32 - WARNING: 1637
2025-02-19 16:06:32 - WARNING: model output text 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 16:06:32 - WARNING: len of model_output_text 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: 51
2025-02-19 16:06:32 - WARNING: original_text 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: and time-consuming. Speculative decoding has003
2025-02-19 16:06:32 - WARNING: mapping dict 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 7, 9: 8, 10: 8, 11: 9, 12: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 17, 21: 18, 22: 19, 23: 20, 24: 21, 25: 22, 26: 23, 27: 24, 28: 25, 29: 26, 30: 27, 31: 28, 32: 29, 33: 30, 34: 31, 35: 32, 36: 33, 37: 34, 38: 35, 39: 36, 40: 37, 41: 38, 42: 39, 43: 40, 44: 41, 45: 42, 46: 43, 47: 44, 48: 45, 49: 46, 50: 46}
2025-02-19 16:06:32 - WARNING: text: 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 16:06:32 - WARNING: origin bbox 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 16:06:32 - WARNING: normalized bbox 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 16:06:32 - WARNING: finalized bbox 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 16:06:32 - WARNING:  final bouding box 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: {'x1': 12.217000007629395, 'y1': 269.133544921875, 'x2': 272.1279296875, 'y2': 281.13848876953125, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 16:06:32 - WARNING: new num para 
2025-02-19 16:06:32 - WARNING:  
2025-02-19 16:06:32 - WARNING: 1566
2025-02-19 16:06:32 - WARNING: done save re and ner, 
2025-02-19 16:06:32 - WARNING: start count num_rel 
2025-02-19 16:06:32 - WARNING: done count num_rel 
2025-02-19 16:06:32 - WARNING: start update document 
2025-02-19 16:06:33 - WARNING: start matching infor
2025-02-19 16:06:33 - WARNING: done matching infor
2025-02-19 16:06:33 - WARNING: start commit
2025-02-19 16:06:34 - WARNING: done update document 
2025-02-19 16:06:36 - WARNING: Failed to send email. Error: {'an': (553, b'5.1.3 The recipient address <an> is not a valid RFC 5321 address. For more\n5.1.3 information, go to\n5.1.3  https://support.google.com/a/answer/3221692 and review RFC 5321\n5.1.3 specifications. d2e1a72fcca58-7325f5d1eb9sm8481247b3a.17 - gsmtp')}
2025-02-19 16:06:36 - INFO: Task dev_tasks.process_pdf_task[a3dd0fea-f232-49bc-8b90-7bd176e74c89] succeeded in 26.72798141464591s: {'id': 89669145, 'filename': '_ACL_2025__LLM_Efficiency (2) (1).pdf', 'upload_time': '2025/02/19, 07:06:09', 'entities': 237, 'relations': 6, 'pages': 20, 'status': 'completed'}
2025-02-19 16:06:36 - WARNING: 2025-02-19 16:06:36 - INFO: Task dev_tasks.process_pdf_task[a3dd0fea-f232-49bc-8b90-7bd176e74c89] succeeded in 26.72798141464591s: {'id': 89669145, 'filename': '_ACL_2025__LLM_Efficiency (2) (1).pdf', 'upload_time': '2025/02/19, 07:06:09', 'entities': 237, 'relations': 6, 'pages': 20, 'status': 'completed'}
