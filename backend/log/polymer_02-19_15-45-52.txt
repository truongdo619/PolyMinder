2025-02-19 15:45:52 - WARNING: {'<pad>': 0, '<suc>': 1, 'monomer': 2, 'organic': 3, 'inorganic': 4, 'condition': 5, 'polymer_family': 6, 'syn_method': 7, 'prop_name': 8, 'prop_value': 9, 'ref_exp': 10, 'char_method': 11, 'polymer': 12, 'material_amount': 13, 'composite': 14, 'other_material': 15}
2025-02-19 15:45:52 - INFO: Building Model
2025-02-19 15:45:52 - WARNING: 2025-02-19 15:45:52 - INFO: Building Model
2025-02-19 15:45:58 - WARNING: uploads/yoonessi2011.pdf
2025-02-19 15:45:58 - WARNING: start parsing pdf
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 5
2025-02-19 15:45:59 - WARNING: 5
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 20
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 7
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 14
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 40
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 6
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 14
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 34
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 45
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 6
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 4
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 49
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 35
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 30
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 17
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 9
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 8
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 14
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 9
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 17
2025-02-19 15:45:59 - WARNING: 17
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 3
2025-02-19 15:45:59 - WARNING: 4
2025-02-19 15:45:59 - WARNING: 4
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 1
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 2
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 20
2025-02-19 15:45:59 - WARNING: 17
2025-02-19 15:45:59 - WARNING: 21
2025-02-19 15:45:59 - WARNING: 20
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: 0
2025-02-19 15:45:59 - WARNING: parsed 174 paragraphs
2025-02-19 15:45:59 - WARNING: ['Morphology of sulfonated polyarylenethioethersulfone random copolymer series as proton exchange fuel cells membranes by small angle neutron scattering', 'Mitra Yoonessi a,b,*, Hendrik Heinz c, Thuy D. Dang d, Zongwu Bai e', 'a Ohio Aerospace Institute, Cleveland, OH 44142, USAb NASA Glenn Research Center, Cleveland, OH 44135, USAc Department of Polymer Engineering, University of Akron, Akron, OH 44325, USAd Air Force Research Laboratory, AFRL/RXBN, Wright-Patterson AFB, OH 45433, USAe University of Dayton Research Institute, 300 College Park Drive, Dayton, OH 45469, USA', 'a r t i c l e i n f o', 'Article history: ', 'Received 27 June 2011 ', 'Received in revised form ', '23 September 2011 ', 'Accepted 28 September 2011 ', 'Available online 4 October 2011', 'Keywords: ', 'Fuel cells membrane Morphology ', 'Neutron scattering', 'a b s t r a c t', 'Sulfonated polyarylenethioethersulfone (SPTES) copolymers with high proton conductivity (100 e215 mS/cm at 65 �C, 85% relative humidity) are promising potential proton exchange membrane (PEM) for fuel cells. Small angle neutron scattering (SANS) of the hydrated SPTES copolymer membranes at 25�C exhibit a nanostructure which can be approximated by correlated polydisperse spherical aggregates containing water molecules with liquid-like ordering (Percus Yevick approximation) and large scale water pockets. The ionic domain radius and the volume packing density of the aggregates present in the hydrated SPTES copolymer membranes at 25 �C increased with increasing degree of sulfonation. SPTES-80 with highest degree of sulfonation (71.6%) showed a Guinier plateau at the very low q range (q < 1 �10�4 1/Å) indicating presence of isolated large scale morphology (Rg ¼ 1.3 �0.18 micron). The radius of spherical ionic aggregates present in the hydrated SPTES-50 and SPTES-60 copolymer membranes increased with increasing temperature to 55 �C, but the large scale morphology changed to a fractal network. Further increase of the sulfonation degree to 63.3% and 71.6% (SPTES-70 and SPTES-80) resulted in a substantial morphology change of the spherical aggregates to an irregular bicontinuous hydrophobic/hydrophilic morphology for the hydrated SPTES-70 and SPTES-80 copolymer membranes at 55 �C. Presence of ionic maxima followed by a power law decay of �4 for SPTES-70 and SPTES-80 copolymer membranes was attributed to the bicontinuous phase morphology at high degree of sulfonation and elevated temperature (55 �C). The disruption of the larger scale fractal morphology was characterized by significant decrease in the intermediate scattering intensity. Hydrophobic and hydrophilic domains were separated distinctly by sulfonic groups at the interface showing as power law decay of �4 for all hydrated SPTES copolymers. ', '�2011 Elsevier Ltd. All rights reserved.', '1. Introduction', 'Ionomers are important class of polymeric materials with ionizable groups on the polymer backbone or in the pendant which can phase separate to hydrophobic and hydrophilic domains [1,2]. Ionomers with ionizable acidic groups have potential application as polyelectrolyte membranes in fuel cells. Hydrogen fuel cell is an electrochemical reactor in which the proton transport from anode to cathode leads to a reaction at the cathode catalyst interface [3e5]. Therefore, transport of protons and hydronium ions through proton', 'exchange membrane (PEM) is the key factor on the performance of a hydrogen fuel cell. High proton conductivity, impermeability to reactant gases, high thermal and mechanical stability both in the dry and hydrated states, water uptake, dimensional stability, and low cost are fundamental characteristics of PEM for hydrogen fuel cells. The structure, dynamics, and transport characteristics of Nafion�as commercially utilized PEM have been studied by small angle neutron scattering (SANS) [6e11], small angle x-ray scattering (SAXS) [12e16], quasi-elastic neutron scattering (QENS) [17], and nuclear magnetic resonance spectroscopy (NMR) [18]. ', 'Transport properties and nanostructure of sulfonated polyimide (SPI) membranes have been studied using pulsed field gradient NMR and NMR quadrupolar relaxation rates determinations [19,20], and small angle scattering methods (SAXS and SANS), respectively [21,22].', '* Corresponding author. Ohio Aerospace Institute, Cleveland, OH 44135, USA. Tel./ fax: þ1 9376265333. ', 'E-mail address: mitra.yoonessi@nasa.gov (M. Yoonessi).', 'Contents lists available at SciVerse ScienceDirect', 'Polymer', 'journal homepage: www.elsevier.com/locate/polymer', '0032-3861/$ e see front matter �2011 Elsevier Ltd. All rights reserved. doi:10.1016/j.polymer.2011.09.047', 'Polymer 52 (2011) 5615e5621', 'The microstructure of sulfonated polyetherether ketone (sPEEK) has been investigated by SAXS [12]. We recently reported a class of ionomers based on aromatic hydrocarbon copolymers with high proton conductivity and excellent thermal mechanical stability both in the dry and hydrated states [22e29]. Sulfonated poly-arylenethioethersulfone(SPTES)copolymershavefollowing chemical structures (Fig. 1). ', 'SPTES copolymers including SPTES-50, SPTES-60, SPTES-70 and SPTES-80, have equivalent weight (EW) and IEC (mequiv./g) values of 610, 515, 459, 417, and 1.64, 1.94, 2.18, and 2.4, respectively [22]. They exhibited proton conductivity of 100, 145, 175, and 215 mS/cm respectively, at 65 �C and 85% relative humidity [22]. Their excellent proton conductivity at high temperatures combined with their high glass transition temperature (w200 �C) and mechanical stability (both in the dry and hydrated states) make them excellent potentials as high temperature PEM materials for fuel cells. SPTES-50 copolymer membrane has successfully been fabricated to membrane electrode assemblies and exhibited polarization curves and durability up to 400 h [29]. Their successful operations at 90e100 �C were limited by boiling point of water (100 �C at 1 atm). Replacing water molecules with heterocycles such as imidazolium where the charge carrier has a very low vapor pressure can result in proton conductivity at higher temperatures. Despite excellent electrochemical properties, excessive water uptake of SPTES-70 and SPTES-80 copolymer membranes provide difficulties to be made as membrane electrode assembly [22,27] The proton transport and performance of SPTES copolymers highly depend on the presence of water molecules. In addition to the number of sulfonic groups, their acidity (pKa) ability to dissociate water molecules to proton, water activity coefficient, and number of water molecules associated with each sulfonic group, the supermolecular structure of the hydro-phobic and hydrophilic phases is also defined by polymer chain characteristics such as chain persistent length, and presence of sulfonic group on the backbone or in the side chains. We reported the presence of ionic nanodomains containing water molecules in the SPTES-70 using in-situ x-ray scattering [27]. The morphology and the nanostructure of SPTES-50 were approximated by corre-lated polydisperse spherical aggregates and a larger scale water domain network and were quantified by modeling of the SANS spectra with polydisperse hard sphere model with Percus Yevick liquid-like ordering [29]. This study reports the nanostructure and morphology of sulfonated polyarylenethioethersulfone (SPTES) copolymer membranes which is directly related to the proton transport through the membrane in terms of their degree of sulfonation and temperature dependency.', '2. Materials and methods', '2.1. Materials', 'Visually observed defect free films of SPTES copolymers were prepared by dissolving the purified copolymer in dimethyl acet-amide (DMAc, Sigma Aldrich) (5e10 wt%) filtering, placing in a flat dish in a vacuum oven with a gradual temperature rise to 100 �C for 24 h and 120 �C for 2 h. The resulting uniform flat films were immersed for 2 h in deionized water and dried under vacuum (24 h, 80 �C) after they were acidified in sulfuric acid (4 M, 24 h) to ensure complete conversion of sulfonic groups to their protonated forms.', '2.2. Characterization', 'SANS experiments were performed at the National Institute of Standards and Technology (NIST), Neutron Center for Research using 30 m NG-7 SANS instrument with a neutron wavelength, l, of 6 Å (Dl/l ¼ 10%) and three sample-to-detector distance of 1.5 m, 10 m (l ¼ 6 Å), and 15 m (l ¼ 8 Å), 0.001 < q < 0.3168 Å�1 at 25, and 55 �C (accuracy of 0.5 �C). Hydrated membranes were placed in demountable 1 mm thick titanium liquid cells filled with D2O after equilibrium in D2O (24 h). Scattered intensities were reduced, corrected for the transmission and background and placed on absolute scale. Then, circularly averaged to produce absolute scale scattering intensity, I(q), as a function of the wave vector, q, where q ¼ (4p/l)sin(q/2) and q is the scattering angle. Calculations were performed using Igor Pro�software [30,31]. USANS experiments covered a q-range of 0.00005 < q (Å�1) < 0.01, corresponding to a real-space length scale of 0.1 microne10 micron.', '3. Results and discussions', '3.1. Hydrated SPTES copolymers at 25 �C', 'SPTES copolymers have high proton conductivity at high relative humidities and elevated temperatures; e.g. 65 �C and 85% RH of 100, 145, 175, and 215 mS/cm for SPTES-50, -60, -70, and -80 copolymers, respectively [27]. The degree of sulfonation increases for SPTES-50, -60, -70 and -80 copolymers in the order of 45.04, 54.9, 63.3, and 71.6%, respectively. SPTES-50 copolymer has the lowest the degree of sulfonation and SPTES-80 copolymer has the highest degree of sulfonation in this class of copolymers [22,27]. The scattering spectra of the fully hydrated (D2O) SPTES-50, -60, -70, and -80 copolymers at 25 �C show a scattering behavior of a two phase structure (Fig. 2a). The contribution of sulfonic groups to the scattering is considered to be negligible due to their small volume compared to the polymer and the water phase volume. It was visually observed that membranes do not dissociate when fully hydrated in the examined temperature. It was also determined by weight measurement that the membranes maintain the weight after deswelling. Therefore, the hydrophobic polymeric structure is the supporting network even at high water content and increased temperatures. Fig. 2a presents enhanced scattering intensity data (vertically shifted) for the hydrated SPTES-60, -70, and -80 copol-ymer membranes in the order of 20X, 1000X, and 2 �105X for the clarity of data presentation. The absolute scale scattering intensities in the medium q range of 0.02 < q (1/Å) < 0.1 are almost in the same intensity range for all SPTES copolymers. The high q scattering spectra for all copolymers exhibit a feature which is attributed to the presence of ionic domains containing water molecules. According to the scattering results, the morphology of all hydrated SPTES-50, -60, -70, and -80 copolymer membranes at 25 �C can be approximated as correlated polydisperse spherical aggregates with liquid-like ordering (P.Y. ordering) and a power law decay [29]. The scattering data for the hydrated SPTES-50, -60, -70, and -80 copolymer membranes at 25 �C were compared and quantified using polydisperse (Schulz polydispersity) hard sphere model with Percus Yevick liquid-like ordering [32e34] and a low q power law decay [29]. The modeling of the SPETS 50 nanostructure and its properties have been reported elsewhere but presented here for', 'Fig. 1. Chemical structure of the highly sulfonated endcapped polyarylenethioethersulfone, SPTES-50, -60, -70, -80 (k ¼ 0.5, 0.6, 0.7, 0.8).', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215616', 'clarity and completeness of the SPTES series analysis [29]. Fig. 2b shows the experimental scattering spectra of the hydrated SPTES-70 copolymer membrane at 25 �C compared with the theoretical calculations of the model (solid line). The results of the comparison of the hydrated SPTES-50, -60, -70 and -80 copolymer membranes scattering data with the model are summarized in Table 1. According to this modeling the domain radius increased with increasing degree of sulfonation in the order of 13.45,14.9,15.4, and 16.9 Å for SPTES-50, -60, -70 and -80 copolymer membranes. The hard sphere packing density increased with increasing degree of sulfonation in the order of 19.8%, 23%, 25%, 29% for SPTES-50, -60, -70, and -80 copolymer membranes. This could indicate that ionic domains are capable of holding more water molecules with the presence of more sulfonic groups on the polymer backbone (increasing the degree of sulfonation). The high-q scattering spectra exhibits onset of a peak formation when the degree of sulfonation was increased to 71.6% for SPTES-80 copolymer. Pres-ence of scattering maxima can be due to the scattering from an ordered structure (periodicity) or the oscillations resulted from the structural factor effects or the excluded volume effect arising from short range liquid-like ordering. The onset of peak formation is attributed to the excluded volume effects related to the liquid-like ordering [33e35]. The low-q power law decay of nearly �3 was observed for all SPTES copolymers in the range of 1 �10�4 < q (1/ Å) < 3 �10�3. The power law decay of �3 is attributed to the presence of interacting three-dimensional fractal morphology. In addition, a Guinier plateau [35] was present for the hydrated SPTES-80 copolymer at the very low q (q (1/Å) < 9 �10�5). This is in addition to the nearly identical scattering intensity in the inter-mediate q-range (0.02 < q (1/Å) < 0.1). It can be concluded that the change in the degree of sulfonation had little or no effect on the intermediate and low angle scattering wave vector. The low-q power law decay was nearly �3 �0.1 for all SPTES copolymers. The presence of sharp interface between hydrophobic and hydro-philic domains was deduced from the power law decay of �4 at large angle wave vectors for all hydrated SPTES copolymer membranes. All scattering data presented in Fig. 2a shows a power slope of �3.85 to �4.15 which is approximated as w �4. The presence of Porod behavior (decay of �4) has been attributed to the two immiscible phase with a sharp boundary [35e37]. This shows that the hydrophobic and hydrophilic domains are separated with a distinct interface containing sulfonic groups. ', 'Presence of a plateau in the scattering spectra is characteristics of isolated scatterers [35]. The radius of gyration (Rg) of the isolated scatterers can be approximated by IðqÞ ¼ I0expðð�q2R2gÞ=3Þ, where I0 is the extrapolated zero scattering intensity (Guinier approxi-mation) [35]. Presence of the Guinier plateau in the q range of 4.3 �10�5 < q (1/Å) < 8.2 �10�5 for SPTES-80 copolymer suggests segregation of the isolated large scale hydrophilic water pockets when the degree of sulfonation increased to 71.6%. Radius of gyration of the isolated larger scale water pockets in fully hydrated', 'Fig. 2. Scattering spectra of fully hydrated (D2O) SPTES copolymer series at 25 �C 2a) Scattering spectra of SPTES-50 (C), SPTES-60 (B), SPTES-70 (6), and SPTES-80 (,); Scattering intensity for hydrated SPTES-60, SPTES-70 and SPTES-80 were shifted vertically for clarity (SPTES-60: 20X, SPTES-70: 1000X, SPTES-80: 2 �105X). 2b) Experimental scattering spectra of SPTES 70 compared with polydisperse hard sphere model with liquid-like ordering and a power law decay of �2.9. 2c) The plot of Ln (I) vs. q2 in the 4.36 �10�5 < q (1/Å) < 8.25 �10�5.', 'Table 1 ', 'Structural characteristics of SPTES copolymer membrane predicted by polydisperse hard sphere with Percus Yevick liquid-like ordering and a low-q decay.', 'MaterialR (Å)Vol. fractionPolydispersityLow q decay', 'Hydrated Membranes at 25 �C ', 'SPTES-5013.45 �0.20.20.43�3 SPTES-6014.9 �0.50.230.36�2.9 SPTES-7015.4 �0.50.250.32�2.9 SPTES-8016.9 �0.50.290.31�3.1', 'Hydrated Membranes at 55 �C ', 'SPTES-5026.4 �0.50.280.35e SPTES-6032 �0.20.330.31e', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215617', 'SPTES-80 copolymer membrane was 1.3 �0.18 micron which was calculated from this approximation (Fig. 2c).', '3.2. Hydrated SPTES copolymers at 55 �C', 'Based on the physical observations, water saturated membranes at 55 �C were swollen films without visual dissociation. Fig. 3 exhibits the scattering spectra of the fully hydrated (D2O) SPTES-50, -60, -70 and -80 copolymer membranes at 55 �C. The inter-mediate scattering intensity decreased with increasing degree of sulfonation in the order of SPTES-50, -60, -70, and -80 copolymers. The scattering spectra of the fully hydrated SPTES-60, -70, and -80 copolymer membranes start to form scattering maxima at high q range when the degree of sulfonation increased. This indicated the increase in the excluded volume due to the increase in the volume of the scatterers at higher degree of sulfonation and increased temperature (55 �C). The presence of scattering maxima is more significant for SPTES-70 copolymer and SPTES-80 copolymer with 63.3 and 71.6% degree of sulfonation. The large angle scattering feature shows scattering maxima at qmax of 0.103 and 0.1108 1/Å for SPTES-70 and SPTES-80 copolymers which correspond to spatial characteristics lengths (l ¼ 2p/qmax) of 60.97 and 56.7 Å due to concentration fluctuations of hydrated domains (hard spheres). The scattering spectra of the hydrated SPTES-50, -60, -70, and -80 copolymer membranes changed significantlywith increasing degree of sulfonation. This indicates a substantial change in the hydrophobic/hydrophilic morphology with increasing degree of sulfonation at 55 �C which is more pronounced for SPTES-70 and SPTES-80 copolymers (degree of sulfonation 63.3 and 71.6%, respectively). Hydrophobic and hydrophilic regions are segregated distinctly by the sulfonic groups at the interface which is repre-sented by the Porod behavior, asymptotic behavior of w �4 for all SPTES copolymers [35e37]. The morphology of these membranes is complex and controlled by interfacial phenomena. The number of sulfonic groups per repeat unit volume and their acidity com-plemented with the chain persistent length and chain mobility are governing factors in the formed morphology. This study approxi-mates the morphology of the fully hydrated SPTES-50 and SPTES-60 copolymer membranes at 55 �C are approximated as spherical nanodomains containing water molecules with liquid-like ordering similar to their morphology at 25 �C. It is also proposed that this morphology changedtofractalmorphologywithincreasing temperature (increasing the intermediate scattering decay). The domain radius and the sphere packing density were increased from 26.4 Å and 28% for SPTES-50 copolymer to 32 Å and 33% for SPTES-60copolymerwhiletheintermediatescatteringintensity decreased. The domain radius and packing density of the hydrated SPTES-50 copolymer membrane increased from 13.45 Å and 19.8% to 26.4 Å and 28% when the temperature increased from 25 �C to 55 �C with this approximation. The same increasing trend of 14.9 Åe32 Å for the average domain radius and 23%e33% of the sphere packing density was observed. The decrease in the inter-mediate scattering of the hydrated SPTES-60 copolymer membrane compared to the hydrated SPTES-50 copolymer membrane is attributed to the disruption of the large scale morphology. The low q power law decay of the hydrated SPTES-60 copolymer membrane was close to the decay of the hydrated SPTES-50 copolymer membrane. ', 'The increase in the degree of sulfonation of SPTES-60, -70, and -80 copolymers from 54.9, to 63.3 and 71.6% resulted in a shift in the position of the high q scattering maxima of the hydrated membranes toward the large angle scattering regime. This peak formation is more prominent for SPTES-70 and SPTES-80 copoly-mers with higher sulfonation degree. The decrease in the inter-mediate scattering intensity in the order of SPTES-50, -60, -70 and', '-80 copolymers is attributed to the loss of the large scale water network, intermediate fractal morphology within the polymer and onset of a morphology change to a two large scale phase morphology. The closed domain morphology of polydisperse spherical aggregates containing water molecules with fractal network were present for fully hydrated SPTES-50 and SPTES-60 copolymer membranes at 55�C (Figs. 3 and 4a). However, substantialchangesinthemorphologyoffullyhydrated membranes occurred when the degree of sulfonation is increased to 63.3 and 71.6% for SPTES-70 and SPTES-80 copolymers at higher temperature of 55 �C. Combination of high temperature, high density of sulfonic groups, and significant amount of water mole-cules within the polymer backbone could have resulted in the coalescence of the small spherical ionic domains and fractal water network into a larger scale bicontinuous network of intermeshed hydrophobic and hydrophilic morphology for fully hydrated SPTES-80 copolymer membrane at 55 �C. The bicontinuous model origi-nally proposed for micro-emulsion of two immiscible phases of water and oil with comparable amount based on Landau theory [38,39]. This model describes two irregular shapes with distinct boundary and has been used for intermesh of hydrophobic and hydrophilic structure when the particle shape is not well-defined [40,41]. This model proposes I(q) ¼ (a2þc1q þ c2q2)�1 for a2 > 0, c1 < 0, and c2 > 0, a single broad scattering maxima, and power law decay of �4 at large scattering angles. Two characteristics lengths of d and x are deduced from this analysis having a2, c1, and c2[39,40], d is the domain periodicity or interdomain distance and x is the correlation length which has been attributed to the dispersion of d. According to this theory, c1 is negative due to the surfactant. The absolute values of c1 and the ratio of x/d should increase with increasing surfactant. The specific internal surface area can be derived from the ratio of S/V ¼ 44(1�4)/x [38,39]. ', 'The scattering spectra of the hydrated SPTES-70 copolymer membrane has a low-q upturn, a lower intermediate scattering intensity compared to the hydrated SPTES-60 and SPTES-50 copolymers, and a large angle maxima followed by a power law decay of �4. This indicates the onset of the coalescence of the ionic aggregates, disruption of the fractal water network, and the formation of the two irregular large scale bicontinuous phase morphology. The high q range (q > 0.037 1/Å) of the scattering', 'Fig. 3. Scattering spectra of fully hydrated (D2O) SPTES copolymers at 55 �C. The high q scattering exhibit a power law decay of �4. High q range maxima position of hydrated SPTES-60, SPTES-70 and SPTES-80 shifts toward higher q with increasing degree of sulfonation.', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215618', 'spectra of the hydrated SPTES-70 copolymer membrane at 55 �C was compared with the T.S. bicontinuous model (Fig. 4b). This simulation resulted in a periodicity, d, of 59.9 Å which is in excellent agreement with the one obtained from the scattering maxima position, 60.97 Å. According to this simulation, the correlation length (x) is 31.14 Å. The medium-q range scattering still exhibits the larger scale morphology which is not completely converted to the bi-continuous phase morphology. However, they are partially collapsed characterized by a lower scattering intensity in the intermediate q range. The high q maxima followed by a power law decay of �4 is approximated as bicontinuous morphology. The low and medium q (q < 0.037 1/Å) scattering spectra of the SPTES 70 is due to the presence of fractal network of waters which have not coalesce yet. ', 'The scattering spectra of the SPTES-80 copolymer are consistent with a bicontinuous two phase structure of irregular shapes with sulfonic groups as interfacial ionic region. The scattering experi-mental data of the hydrated SPTES-80 copolymer membrane was compared with this model (q > 0.015 1/Å), (Fig. 4c and Table 2). The periodicity of the water domains (or hydrophobic domains), d, predicted by this model is w54 Å which is in excellent agreement with the one obtained from the scattering maxima (qmax), 56.7 Å. This value is the distance between water domains. The correlation length (x) obtained from this model is 37.86 Å which is a measure of dispersion. ', 'The distance between the water phases, periodicity, was decreased from 59.9 Å to 54 Å with increasing degree of sulfona-tion. The correlation length, x, was increased from 31.14 Å to 37.86 Å when the degree of sulfonation increased. Increasing the interfacial area results in an increase in the x/d ratio. The presence of the upturn in the low range of the hydrated SPTES 70 and 80 at 55 �C (Fig. 4b and c) is attributed to the fractal morphology of the large scale features.', '3.3. Discussions', 'Scattering data of series of SPTES membranes were obtained in the full hydration state with increasing the sulfonation degree in the order of 45.04, 54.9, 63.3, and 71.6% for SPTES-50, -60, -70, and -80 copolymer membranes. Complete study was performed to provide understanding membranes morphology with increasing the temperature of the hydrated membranes from 25 �C to 55 �C. Proposed model assumes spherical nanodomains containing water molecules forming from clustering of the sulfonation groups. This assumption was performed based on previously reported study which showed spherical nanodomains in the dry SPTES 50 membrane under HR-TEM [29]. The scattering spectra of the membranes with high degree of sulfonation (SPTES 70, and 80) changed significantly when the temperature increased to 55 �C. The morphology is approximated as bi-continuous system where the hydrocarbon is the main support network containing water. This model has been recently proposed by Wnek et al. [40], Gebel [8], and Kreuer [12], Wnek also provided visualization of the hydro-phobic cluster using SYBYL version 6.7 (Tripos) and HINT (Hydro-pathicINTeractions)computationalmodelingsoftware[40]. Despite the support of the transport studies of this model [42],', 'Fig. 4. Experimental SANS data of fully hydrated (D2O) SPTES-60 (lower degree of sulfonation), SPTES-70 and SPTES-80 (highest degree of sulfonation) membranes at 55 �C. High q scattering spectra exhibits a power law decay of �4 indicating a sharp ionic interface for all hydrated polymer membranes. 4a) SANS spectra of hydrated SPTES-60 at 55 �C (B) is comparedwiththe polydispersehardsphere model withliquid-like ordering and a power law decay. 4b) SANS spectra of the SPTES-70 indicates that the domain aggregates and fractal domains start to collapse and form bicontinuous irregular two immiscible phase morphology. Bicontinuous model was compared with SANS spectra in the q range of (q > 0.0371/Å). 4c) Experimental scattering data of fullyhydrated SPTES-80 (B) at 55 �C compared with Tuebner Strey bicontinuous phase model.', 'Table 2 ', 'Structural characteristics of fully hydrated SPTES-70 and SPTES-80 at 55�C described by Tubnet Strey model.', 'Materiald (Å)x (Å)x/dl (Å)', 'Hydrated Membranes at 55 �C ', 'SPTES-7059.9 �0.231.1 �0.40.5261 �0.3 SPTES-80w54 �0.337.8 �0.60.756.7 �0.2', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215619', 'there is no direct visualization method for this proposed approxi-mation. The T. S. model is also supported by high proton conduc-tivity of the hydrated SPTES 70 and 80 membranes at 55 �C where a larger number of hydronium ions in a larger domain can facilitate proton transport. A summary of the structural evolution of the SPTES membranes as a function of temperature and degree of sulfonation is illustrated in Table 3. This study attempted to study a full range of SPTES membrane nanostructure for the first time and provide an understanding of large membrane water uptakes, and their morphology and their relation with high conductivity of the membranes using both liquid like ordering of polydisperse nano-spheres and bi-continuous T.S. model approximation.', '4. Conclusions', 'A series of SPTES copolymers with high proton conductivity of 100e215 mS/cm at 65 �C and 85% relative humidity as potential fuel cells membranes were studied. SANS studies of fully hydrated membranes showed that the nanostructure of the fully hydrated SPTES-50, -60, -70, and -80 copolymer membranes at 25 �C in agreement with ionic aggregates containing water molecules with a large scale morphology network of water pockets morphology. This model predicted that the increase in the degree of sulfonation resulted in an increase in the radius of ionic domains and an increase in the volume packing density of water in the aggregates. It was assumed that the same morphology of polydisperse correlated spherical ionic domains were present in the SPTES-50 and SPTES-60 copolymers when the temperature increased to 55 �C. Increase in the degree of sulfonation for fully hydrated SPTES-70 and SPTES-80 copolymer at 55 �C led to a substantial reorganization of the membrane morphology which was described by a bi-continuous hydrophobic/hydrophilic network.', 'Acknowledgments', 'The authors would like to thank the Air Force Office of Scientific Research and Materials and Manufacturing Directorate, Nano-structured andBiological Materials Branch forfunding this research. Richard A. Vaia, Michael F. Durstock (WPAFB), and Derek Ho (formerly at NIST) are thanked for the technical discussions support. The National Institute of Standards and Technology is thanked for funding (Proposal S18-38) to conduct neutron scat-tering experiments which were supported by National Science Foundation under agreement DMR-9986442. The mention of commercial products does not imply endorsement by NIST, nor does it imply that the materials or equipment identified are necessarily the best available for the purpose.', 'References', '[1] Schlick S. Ionomers: characterization, theory and applications. FL: CRC Press; ', '1996. ', '[2] Tant MR, Mauritz KA, Wilkes GL, editors. Ionomers: synthesis, structure, ', 'properties and applications. London: Chapman and Hall; 1997. ', '[3] Larminie J, Dicks A, editors. Fuel cells systems explained. London: John Wiley ', '& Sons; 2003. ', '[4] Cleghorn SJC, Ren X, Springer TE, Wilson MS, Zawodinski C, Zawodinski TA, ', 'et al. Int J Hydrogen Energy 1997;22:1137e44. ', '[5] Hoogers G. Fuel cell technology handbook. CRC Press LLC; 2003. MA. ', '[6] Eisenberg A, Yeager HL. ACS symposium series 180. Washington, DC: Amer-ican Chemical Society; 1982. ', '[7] Kim MH, Glinka CJ, Grot SA, Grot WG. Macromolecules 2006;39:4775e87. ', '[8] Gebel G, Lambard J. Macromolecules 1997;30:7914e20. ', '[9] Rollet AL, Diat OR, Gebel G. J Phy Chem B 2002;106:3033e6. ', '[10] Young SK, Trevino SF, Beck Tan NC. J Polym Sci. Part B Polym Phy 2002;40: ', '387e400. ', '[11] Rollet AL, Gebel G, Simonin JP, Turq P. J Polym Sci. Part B Polym Phy 2001;39: ', '548e58. ', '[12] Kreuer D. J Mem Sci 2001;185:29e39.', 'Table 3 ', 'Summary of membrane structural changes (SPTES 50, 60, 70, and 80 at 25 �C, and SPTES 50 and 60 at 55 �C) as a function of degree of sulfonation and temperature by polydisperse hard sphere model with liquid-like ordering (P.Y. ', 'ordering). T.S. bi-continuous model was approximated for higher sulfonation degree copolymers, SPTES 70 and 80, at 55 �C.', 'SPTES 50SPTES 60SPTES 70SPTES 80', 'Sulfonation ', 'Level, %45.0454.963.371.6', 'Temperature, ', 'oC2555255525552555', 'MorphologyP.D. hard ', 'sphere, ', 'P.Y. ', 'ordering', 'P.D. hard ', 'sphere, ', 'P.Y. ', 'ordering', 'P.D. hard ', 'sphere, ', 'P.Y. ', 'ordering', 'P.D. hard ', 'sphere, ', 'P.Y. ', 'ordering', 'P.D. hard ', 'sphere, ', 'P.Y. ', 'ordering', 'T.S. bi-continuous ', 'modelP.D. hard ', 'sphere, ', 'P.Y. ', 'ordering', 'T.S. bi-continuous ', 'model', 'Radius, Å13.45 �0.226.4 �0.514.9 �0.532 �0.215.4 �0.516.9 �0.5 ', 'Correlation ', 'length (x), Å', '31.1 �0.437.8 �0.6', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215620', '[13] Elliott JA, Hanna SA, Elliott MS, Cooley GE. Macromolecule 2000;33:4161e71. [14] Schmidt-Rohr K, Chen Q. Nat Mater 2007;7:75e83. ', '[15] Haubold HG, Vad Th, Jungbluth H, Hiller P. Electro Acta 2001;46:1559e63. ', '[16] Elliott JA, Hanna SJ. Appl Crystallogr 1999;32:1069e83. ', '[17] Pivovar AM, Pivovar BS. J Phys Chem B 2005;109:785e93. ', '[18] Nosaka AY, Nosaka YJ. J Power Sources 2008;180:733e7. ', '[19] Rollet AL, Blachot JF, Delville A, Diat O, Guillermo A, Porion P, et al. Eur Phys J ', '2003;12:130e4. ', '[20] Rollet AL, Porion PT, Delville A, Diat O, Gebel G. Mag Res Imag 2005;23:367e8. [21] Blachot JF, Diat O, Putaux JL, Rollet AL, Rubatat L, Vallois C, et al. J Mem Sci ', '2003;214:31e42. ', '[22] Bai Z, Durstock MF, Dang TD. J Mem Sci 2006;281:508e16. ', '[23] Bai Z, Price GE, Yoonessi M, Juhl SB, Durstock MF, Dang TD. J Mem Sci 2007; ', '305:69e76. ', '[24] Yoonessi M, Bai Z, Dang TD, Durstock MF, Vaia RA. Proceeding of American ', 'Institute of chemical Engineers (AIChE), 2005. ', '[25] Dang T, Bai Z, Dalton MJ. Fossum E 27th ACS National Meeting, 2004 Ana-heim, CA. ', '[26] Bai Z, Williams LD, Durstock MF, Dang TD. Polym Preprints (American Chem ', 'Soc Division Polym Chemistry) 2004;45:60e1. ', '[27] Yoonessi M, Bai Z, Dang TD. J Polym Sci. Part B Polym Phys 2007;45:2813e22. [28] Bai Z, Dang TD. Macro Rapid Comm 2006;27:1271e7.', '[29] Yoonessi M, Heinz H, Dang TD, Wheeler R, Bai Z. Polymer 2010;51: ', '1585e92. ', '[30] Kline S. SANS data reduction tutorial. Gaithersburg, MD: NIST Center for ', 'Neutron Research; 2001. ', '[31] Hammouda B, http://www.ncnr.nist.gov/staff/hammouda/the_SANS_toolbox. ', 'pdf, April, 2008. ', '[32] Kinning DJ, Thomas EL. Macromolecules 1984;17:1712e8. ', '[33] Percus JK, Yevick GJ. Phys Rev 1958;110:1e13. ', '[34] Percus JK. Phys Rev Lett 1962;8:462e3. ', '[35] Guinier A, Fournet G. Small-angle scattering of x-rays. NewYork: John Wiley ', 'and Sons; 1955. ', '[36] Porod G. In: Glatter O, Kratky O, editors. Small-angle x-ray scattering. London: ', 'Academic Press; 1982. ', '[37] Higgins JS, Benoit HC. Polymers and neutron scattering. Oxford: Clarendon ', 'Press; 1994. ', '[38] Teubner M, Strey R. J Chem Phys 1987;87:3195e7. ', '[39] Schubert KV, Strey R, Kline SR, Kaler EW. J Chem Phys 1994;101:5343e56. [40] Serpico JM, Ehrenberg SG, Fontanella JJ, Jiao X, Perahia D, McGrady KA, et al. ', 'Macromolecules 2002;35:5916e21. ', '[41] Nieh MP, Guiver MD, Kim DS, Ding J, Norsten T. Macromolecules 2008;41: ', '6176e82. ', '[42] Edmondson CA, Fontanella JJ, Chung SH, Greenbaum SG, Wnek GE. Electro-chim Acta 2001;46:1623e8.', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215621']
2025-02-19 15:45:59 - WARNING: done parsing pdf
2025-02-19 15:45:59 - WARNING: start ner pdf
2025-02-19 15:45:59 - INFO: Loading Data
2025-02-19 15:45:59 - WARNING: 2025-02-19 15:45:59 - INFO: Loading Data
2025-02-19 15:46:02 - WARNING: Predicting NER ...
2025-02-19 15:46:03 - WARNING: Finished predicting.
2025-02-19 15:46:03 - WARNING: Converting to Brat format...
2025-02-19 15:46:03 - WARNING: # of discontinuous mentions:
2025-02-19 15:46:03 - WARNING:  
2025-02-19 15:46:03 - WARNING: 21
2025-02-19 15:46:03 - WARNING: Finished.
2025-02-19 15:46:03 - WARNING: start predict re
2025-02-19 15:46:04 - WARNING: Example:   0%|          | 0/36 [00:00<?, ?it/s]
2025-02-19 15:46:04 - WARNING: Example:  25%|##5       | 9/36 [00:00<00:00, 89.71it/s]
2025-02-19 15:46:04 - WARNING: Example:  58%|#####8    | 21/36 [00:00<00:00, 106.98it/s]
2025-02-19 15:46:04 - WARNING: Example: 100%|##########| 36/36 [00:00<00:00, 142.80it/s]
2025-02-19 15:46:04 - WARNING: # of documents 36.
2025-02-19 15:46:04 - WARNING: # of positive examples 0.
2025-02-19 15:46:04 - WARNING: # of negative examples 6932.
2025-02-19 15:46:05 - WARNING: dict_keys(['14', '18', '19', '27', '28', '31', '36', '39', '40', '52', '53', '54', '56', '58', '60', '62', '63', '70', '72', '95', '96'])
2025-02-19 15:46:05 - WARNING: done predict re
2025-02-19 15:46:05 - WARNING: model output text 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: Received 27 June 2011 
2025-02-19 15:46:05 - WARNING: len of model_output_text 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: 22
2025-02-19 15:46:05 - WARNING: original_text 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: Received 27 June 2011 
2025-02-19 15:46:05 - WARNING: mapping dict 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21}
2025-02-19 15:46:05 - WARNING: text: 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: Received 27 June 2011 
2025-02-19 15:46:05 - WARNING: origin bbox 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: {'x1': 107.19652557373047, 'y1': 331.2534484863281, 'x2': 110.98934173583984, 'y2': 339.08953857421875, 'width': 595.2760009765625, 'height': 793.7009887695312, 'pageNumber': 1}
2025-02-19 15:46:05 - WARNING: normalized bbox 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: {'x1': 107.19652557373047, 'y1': 331.2534484863281, 'x2': 110.98934173583984, 'y2': 339.08953857421875, 'width': 595.2760009765625, 'height': 793.7009887695312, 'pageNumber': 1}
2025-02-19 15:46:05 - WARNING: finalized bbox 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: {'x1': 107.19652557373047, 'y1': 331.2534484863281, 'x2': 110.98934173583984, 'y2': 339.08953857421875, 'width': 595.2760009765625, 'height': 793.7009887695312, 'pageNumber': 1}
2025-02-19 15:46:05 - WARNING:  final bouding box 
2025-02-19 15:46:05 - WARNING:  
2025-02-19 15:46:05 - WARNING: {'x1': 42.51969909667969, 'y1': 331.2534484863281, 'x2': 110.98934173583984, 'y2': 339.08953857421875, 'width': 595.2760009765625, 'height': 793.7009887695312, 'pageNumber': 1}
2025-02-19 15:46:05 - WARNING: done save re and ner, 
2025-02-19 15:46:05 - WARNING: start count num_rel 
2025-02-19 15:46:05 - WARNING: done count num_rel 
2025-02-19 15:46:05 - WARNING: start update document 
2025-02-19 15:46:06 - WARNING: start matching infor
2025-02-19 15:46:06 - WARNING: done matching infor
2025-02-19 15:46:06 - WARNING: start commit
2025-02-19 15:46:06 - WARNING: done update document 
2025-02-19 15:46:09 - WARNING: Failed to send email. Error: {'an': (553, b'5.1.3 The recipient address <an> is not a valid RFC 5321 address. For more\n5.1.3 information, go to\n5.1.3  https://support.google.com/a/answer/3221692 and review RFC 5321\n5.1.3 specifications. 98e67ed59e1d1-2fc13ba819asm11107520a91.41 - gsmtp')}
2025-02-19 15:46:09 - INFO: Task dev_tasks.process_pdf_task[df12fcf4-6dbd-440d-9ca6-280ffb24bb29] succeeded in 17.378043182194233s: {'id': 89697276, 'filename': 'yoonessi2011.pdf', 'upload_time': '2025/02/19, 06:45:51', 'entities': 490, 'relations': 275, 'pages': 7, 'status': 'completed'}
2025-02-19 15:46:09 - WARNING: 2025-02-19 15:46:09 - INFO: Task dev_tasks.process_pdf_task[df12fcf4-6dbd-440d-9ca6-280ffb24bb29] succeeded in 17.378043182194233s: {'id': 89697276, 'filename': 'yoonessi2011.pdf', 'upload_time': '2025/02/19, 06:45:51', 'entities': 490, 'relations': 275, 'pages': 7, 'status': 'completed'}
2025-02-19 15:53:43 - INFO: Task dev_tasks.process_pdf_task[8974ab72-4721-4af5-a62e-486fe3657b0e] received
2025-02-19 15:53:43 - WARNING: 2025-02-19 15:53:43 - INFO: Task dev_tasks.process_pdf_task[8974ab72-4721-4af5-a62e-486fe3657b0e] received
2025-02-19 15:53:43 - WARNING: uploads/_ACL_2025__LLM_Efficiency (2) (1).pdf
2025-02-19 15:53:43 - WARNING: start parsing pdf
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 4
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 4
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 4
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 4
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 4
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 4
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 4
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 4
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 3
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 2
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 1
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: 0
2025-02-19 15:53:44 - WARNING: parsed 1637 paragraphs
2025-02-19 15:53:44 - WARNING: ['SPECTRA: Faster Large Language Model Inference withOptimized Internal and External Speculation', 'Anonymous ACL submission', 'Abstract', 'Inference with modern Large Language Mod-001', 'els (LLMs) is both computationally expensive002', 'and time-consuming. Speculative decoding has003', 'emerged as a promising solution, but existing004', 'approaches face key limitations: training-based005', 'methods require a draft model that is challeng-006', 'ing to obtain and lacks generalizability, while007', 'non-training methods offer limited speedup008', 'gains. In this work, we present SPECTRA, a009', 'novel framework for accelerating LLM infer-010', 'ence without the need for additional training.011', 'SPECTRA introduces two innovative techniques012', 'for efficiently managing internal and external013', 'knowledge, each outperforming corresponding014', 'state-of-the-art (SOTA) methods independently.015', 'When combined, these techniques achieve up016', 'to a 4.08x speedup across various benchmarks017', 'and LLM architectures, significantly surpassing018', 'existing non-training approaches. The imple-019', 'mentation of SPECTRA is publicly available.020', '1Introduction021', 'Generating long sequences with low latency using022', 'Large Language Models (LLMs) is a critical re-023', 'quirement. Current LLMs rely on autoregressive024', 'decoding (Touvron et al., 2023; Bai et al., 2023;025', 'Jiang et al., 2023; OpenAI et al., 2024), which026', 'suffers from inefficiency because it generates text027', 'one token at a time. This results in generation028', 'time scaling linearly with the sequence length and029', 'underutilizes the parallel processing capabilities030', 'of modern GPUs. A widely studied approach to031', 'mitigate this issue is speculative decoding (Chen032', 'et al., 2023; Leviathan et al., 2023), which fol-033', 'lows a guess-and-verify paradigm. In this approach,034', 'a smaller LLM (draft model) (Chen et al., 2023;035', 'Leviathan et al., 2023; Miao et al., 2024; Sun et al.,036', '2023b; Zhou et al., 2024; Cai et al., 2024) or the037', 'original LLM trained in a specialized manner (self-038', 'speculative decoding) (Elhoushi et al., 2024; Liu039', 'et al., 2024a; Yang et al., 2024; Zhang et al., 2024a;040', 'Li et al., 2024b) predicts multiple tokens in ad-041', 'vance. The original LLM then verifies these pre-042', 'dictions in parallel, improving efficiency. However,043', 'these approaches require additional training, which044', 'demands substantial computational resources and045', 'may degrade the original model’s capabilities.046', 'Another line of research focuses on speculat-047', 'ing subsequent tokens without requiring additional048', 'training. This approach eliminates the need for049', 'training new models or modifying the original large050', 'language model (LLM), making it practical for051', 'off-the-shelf deployment. Some methods leverage052', 'specialized mechanisms to generate speculative to-053', 'kens directly from the LLM’s predictions (Fu et al.,054', '2024; Ou et al., 2024), while others rely on ex-055', 'ternal information sources to derive these tokens056', '(Yang et al., 2023; He et al., 2024; Li et al., 2024a).057', 'However, the speedup gain in these approaches re-058', 'mains limited due to the quality of the speculative059', 'guesses.060', 'We introduce SPECTRA (Figure 1a), a specu-061', 'lative decoding method that improves generation062', 'speed without requiring any training or modifica-063', 'tions to the original LLM. SPECTRA consists of064', 'two main components: a core module (SPECTRA-065', 'CORE, Figure 1c), which integrates seamlessly into066', 'LLMs in a plug-and-play manner, and an optional067', 'retrieval module (SPECTRA-RETRIEVAL, Figure068', '1e) that further enhances performance. The core069', 'module SPECTRA-CORE improves speculative de-070', 'coding by leveraging the token distribution pre-071', 'dicted by the LLM to generate high-quality guesses.072', 'Specifically, it employs two multi-level N-gram073', 'dictionaries that enable bi-directional search for074', 'dynamic-length guesses, balancing both quality075', 'and quantity. Additionally, SPECTRA optimizes a076', 'candidate pool to continuously update the N-gram077', 'dictionaries, ensuring broad token coverage. All078', 'updates to these resources, along with guess verifi-079', 'cation, are performed efficiently in a single forward080', 'pass. The retrieval module, SPECTRA-RETRIEVAL,081', '1', 'SPECTRA-RETRIEVAL', 'Module', 'N-gram Store', 'Input', 'Lookahead', 'Candidate', 'LLM', '(b) Lookahead decoding(c) SPECTRA-CORE', 'Guesses', 'Input', 'SPECTRA', 'Candidate', 'LLM', 'Bi-directional', 'Guesses', 'Multi-level', 'N-gram Store', 'InputCorpus', 'Guesses', 'LLM', 'Trie', 'Input', 'Guesses', 'LLM', 'Trie', '(d) REST', 'Corpus Refined', 'by Perplexity', '(e) SPECTRA-RETRIEVAL', '(a) Ours: SPECTRA', 'SPECTRA-CORE', 'Module', 'Input', 'GuessesLLM', 'Figure 1: Overview of Spectra and comparison with other non-training SOTA approaches. (a) Overview of SPECTRA.(b) Overview of Lookahead Decoding (Fu et al., 2024). (c) Overview of the SPECTRA-CORE module, which utilizesthe knowledge inside LLM for obtaining guesses. (d) Overview of REST (He et al., 2024). (e) Overview of theSPECTRA-RETRIEVAL module, which is designed to be integrated efficiently with SPECTRA-CORE to boost thespeedup. The results in the bar chart are measured on HumanEval.', 'can be integrated to further enhance speedup. Ex-082', 'isting approaches that rely on external sources for083', 'generating guesses (He et al., 2024) struggle to in-084', 'tegrate with other speculative decoding methods,085', 'as the search time outweighs the speedup gains.086', 'SPECTRA-RETRIEVAL addresses this issue by re-087', 'ducing the search space, selecting only high-quality088', 'content from the corpus based on perplexity scores089', 'computed by the target LLM. This optimization en-090', 'ables seamless integration with SPECTRA-CORE,091', 'maximizing efficiency.092', 'Empirical results on six tasks—including multi-093', 'turn conversation, code generation, and mathemati-094', 'cal reasoning—across three LLM families (Llama095', '2 (Touvron et al., 2023), Llama 3 (Dubey et al.,096', '2024), and CodeLlama (Rozière et al., 2024)) with097', 'model sizes ranging from 7B to 70B demonstrate098', 'that SPECTRA outperforms other non-training spec-099', 'ulative decoding methods, achieving speedups of100', 'up to 4x. We publicly release the code and data.101', 'The key contributions of this paper are as follows:102', '• We introduce SPECTRA, which improves spec-103', 'ulative decoding by effectively leveraging the104', 'LLM’s predicted token distribution. SPEC-105', 'TRA is a plug-and-play solution that requires106', 'no modifications to the LLM (Section 3.1).107', '• SPECTRA’s retrieval module refines external108', 'corpora using perplexity scores computed by109', 'the target LLM, providing a general frame-110', 'work that enables speculative decoding ap-111', 'proaches relying on external information to112', 'be seamlessly integrated with other specula-113', 'tive decoding techniques (Section 3.2).114', '• Extensive experiments across diverse tasks,115', 'LLM architectures, GPU types, and settings116', 'demonstrate the efficiency of SPECTRA, out-117', 'performing other non-training speculative de-118', 'coding approaches (Section 5).SPECTRA119', 'also integrates with acceleration tools such as120', 'FlashAttention and pipeline parallelism (Sec-121', 'tion 5.2). The code and data are available.122', '2Preliminaries123', '2.1Autoregressive Decoding in LLMs124', 'Given an input sequence x = (x1, x2, . . . , xs)125', 'of length s, and a slice of length m as x1:m =126', '(x1, x2, . . . , xm), the output of an LLM repre-127', 'sents a probability distribution over the next to-128', 'ken. The probability of generating the s-th token,129', 'conditioned on all preceding tokens, is given by130', 'PM(xs | x1:s−1). The next token xs is sampled131', 'from this distribution using methods such as greedy,132', 'top-k, or top-p sampling (see (Kool et al., 2020;133', 'Holtzman et al., 2020)). For greedy sampling, the134', 'next token is selected as xs = arg max PM(xs |135', 'x1:s−1). Consequently, the LLM generates an out-136', 'put sequence (y1, y2, . . . , ym) of length m autore-137', 'gressively, where each token yi is computed as138', '2', 'yi = argmax PM(yi | y1:i−1, x).139', '2.2Speculative Decoding140', 'Speculative decoding follows a guess-and-verify141', 'approach, where multiple candidate future to-142', 'kens are speculated and subsequently verified143', 'in a single decoding step.With tree attention144', '(Miao et al., 2024), multiple drafts can be ver-145', 'ified simultaneously. Let G denote the number146', 'of guesses, and define the set of guesses as ˜Y =147', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence148', 'has length K. The j-th token of the i-th guess is149', 'denoted as ˜y(i) ', 'j .150', 'In the case of speculative decoding with greedy151', 'sampling, given the prompt x, a drafting method152', 'generates the draft sequences ˜Y . Using these drafts,153', 'the LLM computes the true tokens (y′1, y′2, . . . , y′K)154', 'in parallel. These tokens are then verified, and155', 'h is defined as the highest number of correctly156', 'guessed tokens across all guesses. Consequently,157', 'h + 1 tokens are generated in a single forward158', 'step. Algorithm 2 outlines speculative decoding159', 'with greedy sampling, and additional details are160', 'provided in Appendix A.161', '3SPECTRA DECODING162', 'SPECTRA consists of two modules (SPECTRA-163', 'CORE and SPECTRA-RETRIEVAL) that can func-164', 'tion independently or together. The core module165', '(SPECTRA-CORE) improves speedup by leveraging166', 'the LLM’s predicted token distribution to gener-167', 'ate high-quality guesses and integrates into LLMs168', 'in a plug-and-play manner. The retrieval module169', '(SPECTRA-RETRIEVAL) derives guesses from a re-170', 'fined external information source and is designed to171', 'integrate with SPECTRA-CORE to further enhance172', 'performance.173', '3.1SPECTRA-CORE174', 'SPECTRA-CORE maintains an N-gram storage and175', 'a candidate pool. The candidate pool C contains176', 'W sequences, {c(0), c(1), . . . , c(W−1)}, with each177', 'sequence consisting of N tokens. Let c(i) ', 'jrepresent178', 'the j-th token in the i-th sequence. The N-gram179', 'storage includes two dictionaries: the forward dic-180', 'tionary Sfwd and the backward dictionary Sbwd. At181', 'each time step, guesses G are obtained through a182', 'bidirectional search using Sfwd and Sbwd. A sin-183', 'gle forward pass to the LLM retrieves all neces-184', 'sary distributions, which are used to generate new185', 'candidate tokens for C and verify the guesses G.186', 'Algorithm 1 SPECTRA Internal Knowledge', 'Require: Sequence x = (x1, x2, . . . , xn), model PM, maxN-gram size N, candidate pool size W, max guesses G,max number of new tokens m. Refine threshold τ', '1: Initialize N-gram Forward-dictionary Sfwd ←∅2: Initialize N-gram Backward-dictionary Sbwd ←∅3: Random c(i) ', 'j , ∀j ∈[0, N −1], ∀i ∈[0..W −1]', '4: t ←n + 15: while t ≤m do6:{Obtain the guesses}', '7:G ←Sfwd[xt−1]', '8:u = ∅', '9:for j = 0 to N −1 do', '10:for k = N −1 to 1 do', '11:uj ←Sbwd[xt+j−k:t−1 ⊕u0:j−1]', '12:break if found value for uj', '13:end for', '14:end for', '15:G.append(u)', '16:{Foward in LLM}', '17:Obtain necessary distributions of PM in parallel.', '18:{Verification}', '19:{Greedy verify (Alg. 3) or Sampling verify (Alg. 4)}', '20:hits ←VerificationFunction(x, PM, g)', '21:x = x ⊕hits', '22:t ←t + size(hits)', '23:{Predict Candidates}', '24:for i = 0 to W −1 do', '25:r ∼Uniform[0, 1]', '26:Pc(c(i) ', 'N−1) ←PM(c(i) ', 'N−1 | c(i) ', ':N−2, x)', '27:if r > τ then', '28:c(i) ', 'N−1 ←argmax ', 'c/∈SfwdPc(c(i) ', 'N−1)', '29:else', '30:c(i) ', 'N−1 ←argmax Pc(c(i) ', 'N−1)', '31:end if', '32:end for', '33:{Update N-gram dictionaries}', '34:for i = 0 to W −1 do', '35:for j = 0 to N −2 do', '36:Sfwd[c(i) ', 'j ].append(c(i) ', 'j+1:)', '37:Sbwd[c(i) ', '0:j] ←c(i) ', 'j+1', '38:end for', '39:end for', '40:{Update Candidates}', '41:c(i) ', 'j←c(i) ', 'j+1, ∀j ∈[0, N −2], ∀i', '42: end while43: Output: xn+1:n+m = (y1, y2, . . . , ym)', 'The dictionaries Sfwd and Sbwd are updated with N-187', 'grams from the candidate pool. The details of the188', 'SPECTRA-CORE decoding process are described189', 'in Algorithm 1.190', 'Bi-directional Search for GuessesAt each step,191', 'SPECTRA generates G guess sequences G=192', '{˜y(0), ˜y(1), . . . , ˜y(G)}. Unlike previous work (Fu193', 'et al., 2024), which enforces uniform guess lengths,194', 'SPECTRA supports variable-length guesses, im-195', 'proving both flexibility and efficiency. The for-196', 'ward dictionary Sfwd maps a token to a list of197', 'sequences, while the backward dictionary Sbwd198', 'maps a sequence to a single token. At time step199', '3', 'Input', 'Input', 'Large Language Model', 'Token distribution', 'Figure 2: Details of SPECTRA forward step in LLM.The dashed arrow indicates interactions between thetokens, which are realized by the LLM’s attention mask.', 't, the set of guesses is obtained through a bidi-200', 'rectional search (Alg. 1, lines 7–15). This search201', 'operates in two directions: (1) the forward direc-202', 'tion, which prioritizes the quantity of guesses, and203', '(2) the backward direction, which prioritizes the204', 'quality of guesses. In the forward direction, the205', 'last generated token xt−1 is used to search Sfwd206', 'for guess sequences (Alg. 1, line 7). In the back-207', 'ward direction, a high-quality guess is constructed208', 'by iteratively predicting one token at a time using209', 'Sbwd, repeating the process until a desired sequence210', 'length N is reached (Alg. 1, lines 8–14).211', 'Predict & Verify in One Forward PassAll dis-212', 'tributions required for predicting candidates and213', 'verifying guesses are obtained in a single forward214', 'pass to the LLM, leveraging parallel processing215', '(Figure 2). This is achieved using a specially de-216', 'signed attention mask that specifies the allowed217', 'interactions between tokens. For instance, the to-218', 'ken c(1) ', '2attends only to c(1) ', '1 , c(1) ', '0 , and the input.219', 'Predict Tokens for Candidate PoolWe predict220', 'the next candidate tokens c(i) ', 'N−1 for the candidate221', 'pool using the distribution obtained from the for-222', 'ward pass (Alg. 1, lines 24–32). A straightfor-223', 'ward approach is to select tokens with the highest224', 'probability in the token distribution. However, we225', 'observe that when searching for guesses in the for-226', 'ward dictionary Sfwd, it is crucial for the search to-227', 'ken to exist in the dictionary; otherwise, no guesses228', 'can be retrieved. To address this, we introduce a229', 'randomness-based mechanism to increase the cov-230', 'erage of Sfwd. Specifically, we probabilistically231', 'encourage the selection of unseen tokens in Sfwd232', 'using a hyperparameter τ ∈[0, 1]. Let r be a233', 'random draw from [0, 1]. If r > τ, we select to-234', 'kens with the highest probability that are not in235', 'Sfwd; otherwise, we choose tokens with the high-236', 'est probability regardless of their presence in Sfwd.237', 'Although c(i) ', 'N−1 does not immediately affect the238', 'coverage of Sfwd, it contributes to coverage expan-239', 'sion in subsequent time steps through our candidate240', 'updating mechanism. At the end of each time step,241', 'all candidate sequences are shifted left by one to-242', 'ken: c(i) ', 'j←c(i) ', 'j+1, leaving c(i) ', 'N−1 empty and ready243', 'for prediction in the next time step (Alg. 1, line 41).244', 'Update N-gram DictionariesAt the end of each245', 'time step, candidate tokens from the pool C are246', 'used to update the N-gram dictionaries Sfwd and247', 'Sbwd. While previous work (Fu et al., 2024) only248', 'adds the full N-gram (c(i) ', '0 , c(i) ', '1 , . . . , c(i) ', 'N ), we ob-249', 'serve that subsequences within N-grams often ap-250', 'pear later in the generation process. By including251', 'these subsequences in the N-gram storage, we im-252', 'prove both the quality of guesses and the coverage253', 'of the dictionaries. Specifically, we add subse-254', 'quences to Sfwd using the first token as the key,255', 'and update Sbwd by mapping the preceding part of256', 'the sequence to the last token (Alg. 1, lines 33–39).257', '3.2SPECTRA-RETRIEVAL258', 'SPECTRA-RETRIEVALleveragesanexternal259', 'knowledge source to generate guesses. This in-260', 'volves processing a text corpus and indexing it into261', 'a structure that supports fast prefix search, such as a262', 'trie. At each time step, the last generated tokens are263', 'used as input to this structure to retrieve guesses for264', 'speculative decoding. However, we observe that us-265', 'ing random texts from the corpus without selection266', 'can limit the speedup gain. To address this, we pro-267', 'pose a method to identify and select high-quality,268', 'relevant texts from the corpus tailored to the spe-269', 'cific LLM. This improves the speedup gain and270', 'enables seamless integration with other speculative271', 'decoding approaches, including SPECTRA-CORE.272', 'Corpus Refinement by PerplexityGiven a text sequence u = (u0, u1, u2, . . . ), perplexity quan-tifies the average uncertainty of the model when predicting the next token, conditioned on the pre-ceding tokens. It is calculated as:', 'PPL(u) = exp', '�', '−1', 't', 't�', 'i=1log pθ(ui | u<i)', '�', 'A lower perplexity indicates that the model assigns273', 'higher probabilities to the sequence, suggesting274', 'that the sequence is well-aligned with the model’s275', 'predictions and can produce high-quality guesses276', 'for speculative decoding. To optimize the retrieval277', 'process, we select texts with the lowest perplexity278', '4', 'from the corpus to form a smaller, high-quality sub-279', 'set, which is then used to construct the trie structure280', 'for generating guesses.281', 'Integration with SPECTRA-COREOur exper-282', 'iments (Section 5.2, Table 2) demonstrate that283', 'naively integrating guesses from external sources284', '(e.g., REST (He et al., 2024)) into other specula-285', 'tive methods (e.g., Lookahead (Fu et al., 2024))286', 'can lead to a noticeable drop in speedup. This oc-287', 'curs because the forward pass in the LLM can only288', 'handle a limited number of guesses, and exceeding289', 'this limit increases memory usage and slows down290', 'generation. With a limited guess budget, guesses291', 'from external sources can only account for a frac-292', 'tion of the total guesses, causing the search time293', 'in the indexing structure (e.g., a trie) to outweigh294', 'the speedup gain. To address this, it is crucial295', 'to limit the size of the external knowledge while296', 'maintaining the quality of the guesses. By refining297', 'the corpus using perplexity, SPECTRA-RETRIEVAL298', 'seamlessly integrates with SPECTRA-CORE, further299', 'boosting the speedup gain. Specifically, we inte-300', 'grate SPECTRA-RETRIEVAL into SPECTRA-CORE301', 'by including its guesses in the set of guesses during302', 'the guess generation step (Alg. 1, lines 7–15).303', '4Experiments304', 'Models.We evaluate LLaMA-2-Chat 7B, 13B,305', '70B (Touvron et al., 2023), CodeLlama 7B, 13B306', '(Rozière et al., 2024), and LLaMA-3-Instruct 8B,307', '70B (Dubey et al., 2024).308', 'Tasks.Weconductcomprehensiveevalua-309', 'tions on various generation tasks.MT-Bench310', '(Zheng et al., 2023) for multi-turn conversation;311', 'GSM8K(Cobbe et al., 2021) for mathemati-312', 'cal reasoning; HumanEval(Chen et al., 2021),313', 'MBPP(Austin et al., 2021) and ClassEval (Du314', 'et al., 2023) for code generation.315', 'Metrics.SPECTRA does not modify the original316', 'LLM and the acceptance conditions, making it a317', 'lossless acceleration method. Therefore, the gener-318', 'ation quality remains the same as the original LLM.319', 'We only evaluate the acceleration performance us-320', 'ing the following metrics.321', '• Speedup Ratio: The speedup ratio relative to322', 'autoregressive decoding.323', '• Compression ratio: The ratio of the total324', 'number of autoregressive steps to the number325', 'of Spectra decoding steps needed to produce326', 'the same sequence length.327', 'Baselines.We use standard autoregressive decod-328', 'ing as the baseline (speed-up ratio = 1.00x). We fur-329', 'ther compare SPECTRA with leading non-training330', 'speculative decoding approaches, namely Adaptive331', 'N-gram (Ou et al., 2024), REST (He et al., 2024),332', 'and Lookahead (Fu et al., 2024). For details regard-333', 'ing implementation settings of both SPECTRA and334', 'these baselines, please refer to Appendix B.335', '5Results336', '5.1Main Results337', 'Overall Performance.The top portion of Table 1338', 'presents the speedup ratios of all evaluated meth-339', 'ods under a greedy decoding setup. Our approach,340', 'SPECTRA, consistently yields the highest accelera-341', 'tion across the entire range of datasets and LLMs.342', 'In particular, SPECTRA achieves speedups up to343', '4.08× with LLama-3-8B-Instruct on the MBPP344', 'dataset.345', 'For smaller models (7B), SPECTRA often sur-346', 'passes 3× acceleration, underscoring the effective-347', 'ness of multi-token compression. By contrast, for348', '13B models, while the boost remains strong, it is349', 'relatively more moderate, typically falling in the350', '1.6×–3× band. We attribute this trend to the in-351', 'creased overhead of each forward pass in larger net-352', 'works, which can dampen the proportional gains of353', 'fewer decoding iterations per token. Despite this,354', 'SPECTRA continues to outperform baselines across355', 'all parameter settings.356', 'Significant advantages are evident in tasks such357', 'as GSM8K and ClassEval, where outputs often358', 'follow recurring patterns (e.g., repeated variable359', 'names or class definitions). In these scenarios,360', 'SPECTRA combines internal knowledge of par-361', 'tial sequences with external retrieval suggestions,362', 'thereby proposing accurate multi-token guesses.363', 'On the other hand, in domains featuring more var-364', 'ied or unpredictable responses—such as complex365', 'multi-turn conversations in MT-Bench—the accep-366', 'tance rate is somewhat lower, although still com-367', 'petitive.368', 'Compression Ratio.Table 1 also reports each369', 'method’s compression rate, a measure agnostic370', 'to specific hardware configurations. Across ev-371', 'ery dataset and LLM tested, SPECTRA delivers the372', 'highest average compression ratio. Each of SPEC-373', 'TRA’s draft-and-verify iterations typically yields374', '5', 'ClassevalGSM8KHumanevalMBPPMTBenchAVGModelMethodspeedupτspeedupτspeedupτspeedupτspeedupτspeedup', 'Greedy (temperature=0)', 'CL-13BANPD1.942.522.813.722.082.502.713.582.613.412.43 CL-13BLookahead2.253.612.804.242.303.162.914.442.594.042.57 CL-13BREST1.282.140.931.541.582.310.851.400.941.531.12 CL-13BSPECTRA (Ours)2.384.062.914.652.633.953.294.462.654.402.77', 'CL-7BANPD2.302.683.213.752.162.473.163.783.353.832.84 CL-7BLookahead2.593.662.993.832.503.052.903.673.234.272.84 CL-7BREST1.452.220.911.391.702.340.961.451.021.441.21 CL-7BSPECTRA (Ours)2.704.103.334.592.963.903.564.453.704.523.25', 'L2-13BANPD1.361.781.471.721.341.611.121.321.171.371.29 L2-13BLookahead1.812.761.461.871.732.321.381.691.512.041.58 L2-13BREST1.222.010.941.461.251.940.951.441.141.901.10 L2-13BSPECTRA (Ours)2.003.241.832.621.962.911.632.241.752.601.83', 'L2-70BANPD1.821.901.631.611.861.871.171.201.341.301.56 L2-70BLookahead2.652.871.862.022.572.671.491.541.942.002.10 L2-70BSPECTRA (Ours)3.103.402.522.693.223.371.861.932.432.512.62', 'L2-7BANPD1.621.951.521.681.541.671.191.331.301.371.43 L2-7BLookahead2.192.941.661.932.062.421.461.691.732.051.82 L2-7BREST1.362.121.011.471.412.041.011.461.251.901.21 L2-7BSPECTRA (Ours)2.403.432.112.642.403.051.772.162.022.592.14', 'L3-70BANPD1.541.671.501.471.831.881.461.411.231.231.51 L3-70BLookahead2.402.621.541.582.562.701.431.451.761.861.94 L3-70BSPECTRA (Ours)2.672.912.102.142.843.021.941.942.062.132.32', 'L3-8BANPD2.112.493.864.571.832.093.363.581.141.232.46 L3-8BLookahead2.593.443.714.612.492.893.794.651.531.852.82 L3-8BSPECTRA (Ours)2.833.493.894.772.573.024.084.761.692.103.01', 'Sampling (temperature=1.0)', 'CL-13BANPD1.151.461.071.311.051.301.001.242.312.891.31 CL-13BLookahead1.382.001.081.431.291.751.021.342.333.481.42 CL-13BREST1.141.870.821.351.271.960.841.390.931.501.00 CL-13BSPECTRA (Ours)1.682.221.201.751.652.121.151.702.373.801.61', 'CL-7BANPD1.291.501.161.301.101.321.121.272.773.051.49 CL-7BLookahead1.542.031.191.411.431.811.191.432.723.501.61 CL-7BREST1.231.860.881.331.331.980.911.400.971.441.06 CL-7BSPECTRA (Ours)1.812.251.351.731.682.121.331.722.783.941.79', 'L2-13BANPD1.201.521.241.461.171.401.031.221.171.351.16 L2-13BLookahead1.522.221.321.691.482.001.181.481.492.011.40 L2-13BREST1.181.960.931.451.191.880.921.441.121.881.07 L2-13BSPECTRA (Ours)1.702.751.552.231.692.591.341.891.742.571.60', 'L2-7BANPD1.311.511.341.481.281.461.101.221.251.361.26 L2-7BLookahead1.782.301.511.761.722.091.251.491.682.021.59 L2-7BREST1.262.030.991.461.271.930.961.411.211.881.14 L2-7BSPECTRA (Ours)1.972.831.782.282.042.751.471.841.972.541.85', 'L3-8BANPD1.251.371.972.181.431.651.892.071.151.211.54 L3-8BLookahead1.481.782.072.411.792.211.992.401.571.811.78 L3-8BSPECTRA (Ours)1.942.842.272.781.922.512.192.781.702.052.01', 'Table 1: Overall performance of speculative decoding methods across multiple tasks. “CL-xB” denotes CodeLlamawith xB parameters, “L2-xB” denotes LLaMA-2-Chat of size xB, and “L3-xB” denotes LLaMA-3-Instruct of size xB. We report the speedup ratio (vs. autoregressive) and the compression ratio τ.', '2.1–4.8 tokens, substantially outpacing alternative375', 'approaches and nearly doubling the acceptance376', 'length achieved by REST.377', 'Acceleration in Sampling Decoding.The lower378', 'section of Table 1 investigates the performance379', 'of SPECTRA under sampling-based decoding with380', 'a temperature of 1.0. The results highlight how381', '6', 'SPECTRA continues to accelerate generation rel-382', 'ative to baselines, offering roughly 1.15–2.77×383', 'speedups over standard autoregressive decoding.384', 'These gains are more modest than in greedy decod-385', 'ing, reflecting the lower acceptance rate under the386', 'sampling-based verification phase, which is consis-387', 'tent with earlier findings (Fu et al., 2024; Leviathan388', 'et al., 2023).389', '5.2Analysis390', 'Ablationstudy.Weconductedadetailed391', 'component-wise analysis to determine the contri-392', 'bution of each module to the framework’s over-393', 'all performance (Table 2). Specifically, the re-394', 'sults on LLaMA2-7B-chat reveal that removing395', 'different components yields varying impacts on396', 'GSM8K speedups.Under the “CORE Module”397', 'configuration, excluding multi-level n-grams low-398', 'ers the speedup from 2.04× to 1.95× (a 4% de-399', 'crease), whereas turning off forward information400', 'reduces it from 2.04× to 1.50× (a 26% drop). Sim-401', 'ilarly, omitting backward information results in402', 'a speedup of 1.94×, down from 2.04×. In con-403', 'trast, the “RETRIEVAL Module” setting shows that404', 'leaving out perplexity-based filtering decreases the405', 'speedup from 1.18× to 1.16×. Our fully integrated406', 'approach, SPECTRA, achieves a 2.14× speedup407', 'on GSM8K—outperforming both the “CORE Mod-408', 'ule” (2.04×) and “RETRIEVAL Module” (1.18×)409', 'variants. This improvement demonstrates the im-410', 'portance of combining multi-level n-grams, for-411', 'ward/backward drafting, and perplexity-based re-412', 'finement in boosting acceptance rates and enhanc-413', 'ing overall speedups. A similar trend was also414', 'observed in the results of the MTBench dataset.415', 'Additionally,wecomparedourmethod416', 'against a naive combination of Lookahead and417', 'REST—where guess sequences from REST are418', 'added to Lookahead. This combined approach419', 'falls significantly short of our SPECTRA method,420', 'highlighting that a simple merger of two techniques421', 'is insufficient without our carefully optimized422', 'integration strategy and components.423', 'Priority for source of guessesSince verifying424', 'too many candidate tokens at once can strain GPU425', 'resources and reduce speedups (Fu et al., 2024; Li426', 'et al., 2024b), SPECTRA limits how many guesses427', 'proceed to verification in each step (Appendix B).428', 'To understand whether internal or external guesses429', 'are more valuable, we temporarily remove this430', 'cap and measure acceptance rates (Figure 3). We431', 'GSM8KMTBenchMethodspeedupτspeedupτ', 'REST1.011.471.251.90', 'Lookahead1.661.931.732.05', 'Lookahead + REST1.081.471.271.90', 'SPECTRA’s ablation', 'CORE Module2.042.501.922.35 - w/o forward info1.501.681.201.37 - w/o backward info1.942.211.742.12 - w/o Sub-Ngram1.952.341.752.18', 'RETRIEVAL Module1.181.311.241.50 - w/o PPL refine1.161.291.201.45', 'SPECTRA (ours)2.142.642.022.59', 'Table 2: Ablation of SPECTRA’s components (greedydecoding, LLaMA2-7B-Chat).“Sub-Ngram” aug-ments each n-gram with its sub-sequences; “for-ward/backward info” uses internal expansions; and“PPL refine” applies perplexity-based filtering for ex-ternal retrieval. “Lookahead + REST” denotes a naivecombination where guess sequences from REST are di-rectly added to Lookahead', 'observe that sequences generated via internal ex-432', 'pansions—particularly forward and backward pre-433', 'dictions—have a higher acceptance probability434', 'than those retrieved from external sources. Con-435', 'sequently, SPECTRA prioritizes internal guesses436', 'for verification. Interestingly, in code-generation437', 'tasks like HumanEval, external suggestions be-438', 'come more influential, likely due to code’s repeti-439', 'tive structure and the retrieval of similar snippets.440', 'This observation indicates that a strategic blend441', 'of backward internal knowledge and external re-442', 'trieval can be particularly fruitful in these domains,443', 'especially when computational resources limit ex-444', 'tensive forward expansions.445', 'FlashAttention.Figure 3 shows that enabling446', 'FlashAttention consistently boosts the speedup of447', 'all methods, albeit to varying degrees. Notably,448', 'we observe an additional 0.24× speedup gain for449', 'SPECTRA on both GSM8K and MTBench. This450', 'is because FlashAttention better exploits the paral-451', 'lel structure of speculative decoding by reducing452', 'attention overheads, especially when verifying mul-453', 'tiple guessed tokens in parallel. Although smaller454', 'gains are also seen for other methods, SPECTRA455', 'benefits the most, as it presents the longest verifica-456', 'tion branches and thus stands to profit significantly457', 'from more efficient attention implementations.458', '7', '020406080100', 'Total accept tokens (%)', 'ClassEval', 'GSM8K', 'HumanEval', 'MBPP', 'MT-Bench', 'Internal-FwdInternal-BwdExternal', 'Figure 3: Acceptance rates for different guess sources(e.g., SPECTRA-CORE forward dictionary, backwarddictionary, SPECTRA-RETRIEVAL’s guesses). The ac-ceptance rate is the fraction of guessed tokens that passverification and are appended to the final output.', '0', '1', '2', 'Speedup', '1.00x', '1.68x1.55x', '1.03x', '2.08x', '1.07x', '1.86x1.70x', '1.09x', '2.32x', 'GSM8K', 'W/o flashWith flash', 'Autoreg.LookaheadANPDRESTSpectra', '0', '1', '2', 'Speedup', '1.00x', '1.75x', '1.31x1.24x', '2.01x', '1.10x', '1.96x', '1.42x1.38x', '2.25x', 'MTBench', 'W/o flashWith flash', 'Figure 4: Effect of FlashAttention on speculative de-coding speed: Measured speedups on GSM8K andMTBench (LLama2-7B-Chat, greedy decoding). “NoFlash” uses standard attention; “With Flash” usesFlashAttention for faster parallel verification.', '6Related Works459', 'Large language models (LLMs) are increasingly460', 'deployed in a range of applications, motivating on-461', 'going research into more efficient inference (Liu462', 'et al., 2025). Common strategies include quan-463', 'tizing model weights into lower-precision formats464', '(Liu et al., 2024b; Lin et al., 2024; Zhao et al., 2024;465', 'Park et al., 2024), pruning redundant parameters466', '(Ma et al., 2023; Xia et al., 2023; Sun et al., 2023a;467', 'Le et al., 2025), and employing knowledge distilla-468', 'tion (Gu et al., 2024; Friha et al., 2024; Zhang et al.,469', '2024b). These techniques help reduce the compu-470', 'tational load per forward pass, thereby lowering471', 'generation latency. However, they often introduce472', 'some degradation in model performance, forcing473', 'practitioners to balance quality with efficiency.474', 'A growing line of work explores speculative de-475', 'coding as a strategy for accelerating generation476', 'while maintaining the exact output distribution477', '(Chen et al., 2023; Leviathan et al., 2023). Some478', 'speculative decoding approaches train a smaller479', 'LLM (referred to as a draft model) (Chen et al.,480', '2023; Leviathan et al., 2023; Miao et al., 2024; Sun481', 'et al., 2023b; Zhou et al., 2024; Cai et al., 2024),482', 'or train the original LLM itself in a special man-483', 'ner (referred to as self-speculative) (Elhoushi et al.,484', '2024; Liu et al., 2024a; Yang et al., 2024; Zhang485', 'et al., 2024a; Li et al., 2024b) to guess several sub-486', 'sequent tokens and then verify them parallelly us-487', 'ing the original LLM. As these approaches require488', 'training, they pose limitations, such as requiring489', 'heavy computational resources and losing the orig-490', 'inal model capabilities.491', 'To avoid additional training, alternative specula-492', 'tive decoding methods leverage external resources493', 'or structural properties of language generation.494', 'Retrieval-based methods sidestep draft model train-495', 'ing by using a datastore indexed with observed496', 'prefixes to retrieve guess sequences (Yang et al.,497', '2023; He et al., 2024; Li et al., 2024a). Other498', 'approaches, such as Jacobi-like parallel decoding499', '(Santilli et al., 2023) and lookahead decoding (Fu500', 'et al., 2024), mitigate left-to-right dependencies by501', 'generating and validating multiple candidate tokens502', 'in parallel. These training-free techniques achieve503', 'comparable speedups to learned methods without504', 'requiring model optimization, making them ideal505', 'for scenarios with computational or deployment506', 'constraints.507', '7Conclusions508', 'In this work, we introduced SPECTRA, a hybrid509', 'speculative decoding framework that combines510', 'multi-level n-grams (internal knowledge) with511', 'perplexity-based retrieval (external knowledge) to512', 'achieve speedups of up to 4.08× across various513', 'LLMs and benchmarks, without additional train-514', 'ing or compromising exact output fidelity. Our515', 'ablation studies show that each module (multi-516', 'level n-grams, forward/backward expansions, and517', 'perplexity-based datastore curation) substantially518', 'boosts acceptance rates, and their synergy outper-519', 'forms existing non-training methods. By offering a520', 'lossless speedup that efficiently exploits both inter-521', 'nal patterns and external texts, SPECTRA provides522', 'a practical, high-impact solution for accelerating523', 'inference in large language models.524', '8', '8Limitations525', '(1) Cost of Building External Datastores.Al-526', 'though our internal-knowledge strategy only relies527', 'on sequences observed during generation (and thus528', 'requires no extra data), our external-knowledge529', 'approach depends on constructing and indexing a530', 'sizeable datastore from potentially large corpora.531', 'This process can be time-consuming and memory-532', 'intensive, particularly in domains where data up-533', 'dates frequently or storage is constrained. While534', 'this additional investment can yield substantial535', 'speedups by increasing token acceptance rates, it536', 'may not be universally feasible or cost-effective.537', '(2) Limited Evaluation Scope.Our experiments538', 'center primarily on English-language benchmarks539', 'in conversational and coding tasks using LLaMA-540', 'based models. Although SPECTRA can, in princi-541', 'ple, be applied to other models or languages, addi-542', 'tional factors such as domain-specific tokenization543', 'or specialized textual structures may affect the ac-544', 'ceptance rate and overall speedup. Future work is545', 'needed to assess the generality of SPECTRA across546', 'diverse linguistic settings (e.g., low-resource lan-547', 'guages or specialized technical documents) and for548', 'a wider range of model families (beyond LLaMA-549', 'based architectures) to confirm and refine its appli-550', 'cability.551', 'References552', 'Jacob Austin, Augustus Odena, Maxwell Nye, Maarten553', 'Bosma, Henryk Michalewski, David Dohan, Ellen554', 'Jiang, Carrie Cai, Michael Terry, Quoc Le, and others.555', '2021. Program synthesis with large language models.556', 'arXiv preprint arXiv:2108.07732.557', 'Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,558', 'Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei559', 'Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,560', 'Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,561', 'Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,562', 'Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong563', 'Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-564', 'guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,565', 'Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,566', 'Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-567', 'uan Zhang, Yichang Zhang, Zhenru Zhang, Chang568', 'Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang569', 'Zhu. 2023. Qwen Technical Report.570', 'Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu571', 'Peng, Jason D. Lee, Deming Chen, and Tri Dao.572', '2024. MEDUSA: Simple LLM inference acceler-573', 'ation framework with multiple decoding heads. In574', 'Proceedings of the 41st International Conference on575', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-576', 'enna, Austria.577', 'Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,578', 'Jean-Baptiste Lespiau, Laurent Sifre, and John579', 'Jumper. 2023. Accelerating Large Language Model580', 'Decoding with Speculative Sampling.581', 'Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,582', 'Henrique Ponde De Oliveira Pinto, Jared Kaplan,583', 'Harri Edwards, Yuri Burda, Nicholas Joseph, Greg584', 'Brockman, and others. 2021.Evaluating large585', 'language models trained on code. arXiv preprint586', 'arXiv:2107.03374.587', 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,588', 'Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias589', 'Plappert, Jerry Tworek, Jacob Hilton, Reiichiro590', 'Nakano, and others. 2021.Training verifiers591', 'to solve math word problems.arXiv preprint592', 'arXiv:2110.14168.593', 'Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin,594', 'Shengding Hu, Zhiyuan Liu, Maosong Sun, and595', 'Bowen Zhou. 2023. Enhancing Chat Language Mod-596', 'els by Scaling High-quality Instructional Conversa-597', 'tions. In Proceedings of the 2023 Conference on598', 'Empirical Methods in Natural Language Processing,599', 'pages 3029–3051, Singapore. Association for Com-600', 'putational Linguistics.601', 'Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang,602', 'Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng603', 'Sha, Xin Peng, and Yiling Lou. 2023.Classe-604', 'val: A manually-crafted benchmark for evaluating605', 'llms on class-level code generation. arXiv preprint606', 'arXiv:2308.01861.607', 'Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,608', 'Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,609', 'Akhil Mathur, Alan Schelten, Amy Yang, Angela610', 'Fan, et al. 2024. The llama 3 herd of models. arXiv611', 'preprint arXiv:2407.21783.612', 'Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,613', 'Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas614', 'Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed615', 'Roman, Ahmed Aly, Beidi Chen, and Carole-Jean616', 'Wu. 2024. LayerSkip: Enabling Early Exit Infer-617', 'ence and Self-Speculative Decoding. In Proceedings618', 'of the 62nd Annual Meeting of the Association for619', 'Computational Linguistics (Volume 1: Long Papers),620', 'pages 12622–12642, Bangkok, Thailand. Association621', 'for Computational Linguistics.622', 'Othmane Friha, Mohamed Amine Ferrag, Burak623', 'Kantarci, Burak Cakmak, Arda Ozgun, and Nassira624', 'Ghoualmi-Zine. 2024. Llm-based edge intelligence:625', 'A comprehensive survey on architectures, applica-626', 'tions, security and trustworthiness. IEEE Open Jour-627', 'nal of the Communications Society.628', 'Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.629', '2024.Break the sequential dependency of LLM630', 'inference using LOOKAHEAD DECODING. In631', 'Proceedings of the 41st International Conference on632', '9', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-633', 'enna, Austria.634', 'Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024.635', 'Minillm: Knowledge distillation of large language636', 'models. In The Twelfth International Conference on637', 'Learning Representations.638', 'Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and639', 'Di He. 2024. REST: Retrieval-Based Speculative640', 'Decoding. In Proceedings of the 2024 Conference of641', 'the North American Chapter of the Association for642', 'Computational Linguistics: Human Language Tech-643', 'nologies (Volume 1: Long Papers), pages 1582–1595,644', 'Mexico City, Mexico. Association for Computational645', 'Linguistics.646', 'Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and647', 'Yejin Choi. 2020. The Curious Case of Neural Text648', 'Degeneration. In International Conference on Learn-649', 'ing Representations.650', 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-651', 'sch, Chris Bamford, Devendra Singh Chaplot, Diego652', 'de las Casas, Florian Bressand, Gianna Lengyel, Guil-653', 'laume Lample, Lucile Saulnier, Lélio Renard Lavaud,654', 'Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,655', 'Thibaut Lavril, Thomas Wang, Timothée Lacroix,656', 'and William El Sayed. 2023. Mistral 7B.657', 'Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI,658', 'Chenghao Mou, Yacine Jernite, Margaret Mitchell,659', 'Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf,660', 'Dzmitry Bahdanau, Leandro Von Werra, and Harm de661', 'Vries. 2023. The Stack: 3 TB of permissively li-662', 'censed source code. Transactions on Machine Learn-663', 'ing Research.664', 'Wouter Kool, Herke van Hoof, and Max Welling. 2020.665', 'Ancestral Gumbel-Top-k Sampling for Sampling666', 'Without Replacement. Journal of Machine Learning667', 'Research, 21(47):1–36.668', 'Khang Nguyen Le, Ryo Sato, Dai Nakashima, Takeshi669', 'Suzuki, and Minh Le Nguyen. 2025. Optiprune: Ef-670', 'fective pruning approach for every target sparsity. In671', 'Proceedings of the 31st International Conference on672', 'Computational Linguistics, pages 3600–3612.673', 'Yaniv Leviathan, Matan Kalman, and Yossi Matias.674', '2023. Fast inference from transformers via spec-675', 'ulative decoding. In Proceedings of the 40th Interna-676', 'tional Conference on Machine Learning, ICML’23.677', 'JMLR.org. Place: Honolulu, Hawaii, USA.678', 'Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen,679', 'Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024a.680', 'Nearest Neighbor Speculative Decoding for LLM681', 'Generation and Attribution. In The Thirty-eighth An-682', 'nual Conference on Neural Information Processing683', 'Systems.684', 'Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang685', 'Zhang. 2024b. EAGLE-2: Faster Inference of Lan-686', 'guage Models with Dynamic Draft Trees. In Proceed-687', 'ings of the 2024 Conference on Empirical Methods688', 'in Natural Language Processing, pages 7421–7432,689', 'Miami, Florida, USA. Association for Computational690', 'Linguistics.691', 'Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-692', 'Ming Chen, Wei-Chen Wang, Guangxuan Xiao,693', 'Xingyu Dang, Chuang Gan, and Song Han. 2024.694', 'Awq: Activation-aware weight quantization for on-695', 'device llm compression and acceleration. Proceed-696', 'ings of Machine Learning and Systems, 6:87–100.697', 'Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng698', 'Ni, Duyu Tang, Kai Han, and Yunhe Wang. 2024a.699', 'Kangaroo: Lossless Self-Speculative Decoding for700', 'Accelerating LLMs via Double Early Exiting. In The701', 'Thirty-eighth Annual Conference on Neural Informa-702', 'tion Processing Systems.703', 'Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan704', 'Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xi-705', 'aohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu,706', 'Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang,707', 'Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu,708', 'and Bao Ge. 2025. Understanding llms: A compre-709', 'hensive overview from training to inference. Neuro-710', 'computing, 620:129190.711', 'Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie712', 'Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,713', 'Raghuraman Krishnamoorthi, and Vikas Chandra.714', '2024b. LLM-QAT: Data-free quantization aware715', 'training for large language models. In Findings of716', 'the Association for Computational Linguistics: ACL717', '2024, pages 467–484, Bangkok, Thailand. Associa-718', 'tion for Computational Linguistics.719', 'Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.720', 'Llm-pruner: On the structural pruning of large lan-721', 'guage models. Advances in neural information pro-722', 'cessing systems, 36:21702–21720.723', 'Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao724', 'Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee725', 'Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan726', 'Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Ab-727', 'hyankar, and Zhihao Jia. 2024. SpecInfer: Accelerat-728', 'ing Large Language Model Serving with Tree-based729', 'Speculative Inference and Verification. In Proceed-730', 'ings of the 29th ACM International Conference on Ar-731', 'chitectural Support for Programming Languages and732', 'Operating Systems, Volume 3, ASPLOS ’24, pages733', '932–949, New York, NY, USA. Association for Com-734', 'puting Machinery. Event-place: La Jolla, CA, USA.735', 'Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,736', 'Ça˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-737', 'tive text summarization using sequence-to-sequence738', 'RNNs and beyond.In Proceedings of the 20th739', 'SIGNLL Conference on Computational Natural Lan-740', 'guage Learning, pages 280–290, Berlin, Germany.741', 'Association for Computational Linguistics.742', 'Shashi Narayan, Shay B Cohen, and Mirella Lap-743', 'ata. 2018.Don’t give me the details, just the744', '10', 'summary!topic-aware convolutional neural net-745', 'works for extreme summarization. arXiv preprint746', 'arXiv:1808.08745.747', 'OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,748', 'Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-749', 'man, Diogo Almeida, Janko Altenschmidt, Sam Alt-750', 'man, Shyamal Anadkat, Red Avila, Igor Babuschkin,751', 'Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-752', 'ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-753', 'wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,754', 'Christopher Berner, Lenny Bogdonoff, Oleg Boiko,755', 'Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-756', 'man, Tim Brooks, Miles Brundage, Kevin Button,757', 'Trevor Cai, Rosie Campbell, Andrew Cann, Brittany758', 'Carey, Chelsea Carlson, Rory Carmichael, Brooke759', 'Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully760', 'Chen, Ruby Chen, Jason Chen, Mark Chen, Ben761', 'Chess, Chester Cho, Casey Chu, Hyung Won Chung,762', 'Dave Cummings, Jeremiah Currier, Yunxing Dai,763', 'Cory Decareaux, Thomas Degry, Noah Deutsch,764', 'Damien Deville, Arka Dhar, David Dohan, Steve765', 'Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,766', 'Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,767', 'Simón Posada Fishman, Juston Forte, Isabella Ful-768', 'ford, Leo Gao, Elie Georges, Christian Gibson, Vik769', 'Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-770', 'Lopes, Jonathan Gordon, Morgan Grafstein, Scott771', 'Gray, Ryan Greene, Joshua Gross, Shixiang Shane772', 'Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,773', 'Yuchen He, Mike Heaton, Johannes Heidecke, Chris774', 'Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,775', 'Brandon Houghton, Kenny Hsu, Shengli Hu, Xin776', 'Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,777', 'Joanne Jang, Angela Jiang, Roger Jiang, Haozhun778', 'Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-779', 'woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-780', 'mali, Ingmar Kanitscheider, Nitish Shirish Keskar,781', 'Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,782', 'Christina Kim, Yongjik Kim, Jan Hendrik Kirch-783', 'ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,784', 'Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-785', 'stantinidis, Kyle Kosic, Gretchen Krueger, Vishal786', 'Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan787', 'Leike, Jade Leung, Daniel Levy, Chak Ming Li,788', 'Rachel Lim, Molly Lin, Stephanie Lin, Mateusz789', 'Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,790', 'Anna Makanju, Kim Malfacini, Sam Manning, Todor791', 'Markov, Yaniv Markovski, Bianca Martin, Katie792', 'Mayer, Andrew Mayne, Bob McGrew, Scott Mayer793', 'McKinney, Christine McLeavey, Paul McMillan,794', 'Jake McNeil, David Medina, Aalok Mehta, Jacob795', 'Menick, Luke Metz, Andrey Mishchenko, Pamela796', 'Mishkin, Vinnie Monaco, Evan Morikawa, Daniel797', 'Mossing, Tong Mu, Mira Murati, Oleg Murk, David798', 'Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,799', 'Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,800', 'Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex801', 'Paino, Joe Palermo, Ashley Pantuliano, Giambat-802', 'tista Parascandolo, Joel Parish, Emy Parparita, Alex803', 'Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-804', 'man, Filipe de Avila Belbute Peres, Michael Petrov,805', 'Henrique Ponde de Oliveira Pinto, Michael, Poko-806', 'rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-807', 'ell, Alethea Power, Boris Power, Elizabeth Proehl,808', 'Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,809', 'Cameron Raymond, Francis Real, Kendra Rimbach,810', 'Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-811', 'der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,812', 'Girish Sastry, Heather Schmidt, David Schnurr, John813', 'Schulman, Daniel Selsam, Kyla Sheppard, Toki814', 'Sherbakov, Jessica Shieh, Sarah Shoker, Pranav815', 'Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,816', 'Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin817', 'Sokolowsky, Yang Song, Natalie Staudacher, Fe-818', 'lipe Petroski Such, Natalie Summers, Ilya Sutskever,819', 'Jie Tang, Nikolas Tezak, Madeleine B. Thompson,820', 'Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,821', 'Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-822', 'lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,823', 'Chelsea Voss, Carroll Wainwright, Justin Jay Wang,824', 'Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,825', 'C. J. Weinmann, Akila Welihinda, Peter Welin-826', 'der, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave827', 'Willner, Clemens Winter, Samuel Wolrich, Hannah828', 'Wong, Lauren Workman, Sherwin Wu, Jeff Wu,829', 'Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin830', 'Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers,831', 'Chong Zhang, Marvin Zhang, Shengjia Zhao, Tian-832', 'hao Zheng, Juntang Zhuang, William Zhuk, and Bar-833', 'ret Zoph. 2024. GPT-4 Technical Report.834', 'Jie Ou, Yueming Chen, and Prof. Tian. 2024. Lossless835', 'Acceleration of Large Language Model via Adap-836', 'tive N-gram Parallel Decoding. In Proceedings of837', 'the 2024 Conference of the North American Chap-838', 'ter of the Association for Computational Linguistics:839', 'Human Language Technologies (Volume 6: Industry840', 'Track), pages 10–22, Mexico City, Mexico. Associa-841', 'tion for Computational Linguistics.842', 'Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun843', 'Sim, and Jae W. Lee. 2024. Any-precision llm: Low-844', 'cost deployment of multiple, different-sized llms. In845', 'Proceedings of the 41st International Conference on846', 'Machine Learning.847', 'Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten848', 'Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,849', 'Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy850', 'Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna851', 'Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron852', 'Grattafiori, Wenhan Xiong, Alexandre Défossez,853', 'Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-854', 'tin, Nicolas Usunier, Thomas Scialom, and Gabriel855', 'Synnaeve. 2024. Code Llama: Open Foundation856', 'Models for Code. _eprint: 2308.12950.857', 'Andrea Santilli, Silvio Severino, Emilian Postolache,858', 'Valentino Maiorca, Michele Mancusi, Riccardo859', 'Marin, and Emanuele Rodola. 2023. Accelerating860', 'transformer inference for translation via parallel de-861', 'coding. In Proceedings of the 61st Annual Meeting of862', 'the Association for Computational Linguistics (Vol-863', 'ume 1: Long Papers), pages 12336–12355, Toronto,864', 'Canada. Association for Computational Linguistics.865', 'Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.866', '11', '2023a. A simple and effective pruning approach for867', 'large language models. ArXiv, abs/2306.11695.868', 'Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-869', 'mad Beirami, Himanshu Jain, and Felix Yu. 2023b.870', 'SpecTr: fast speculative decoding via optimal trans-871', 'port. In Proceedings of the 37th International Con-872', 'ference on Neural Information Processing Systems,873', 'NIPS ’23, Red Hook, NY, USA. Curran Associates874', 'Inc. Event-place: New Orleans, LA, USA.875', 'Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-876', 'bert, Amjad Almahairi, Yasmine Babaei, Nikolay877', 'Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti878', 'Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton879', 'Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,880', 'Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,881', 'Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-882', 'thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan883', 'Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,884', 'Isabel Kloumann, Artem Korenev, Punit Singh Koura,885', 'Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-886', 'ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-887', 'tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-888', 'bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-889', 'stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,890', 'Ruan Silva, Eric Michael Smith, Ranjan Subrama-891', 'nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-892', 'lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,893', 'Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,894', 'Melanie Kambadur, Sharan Narang, Aurelien Ro-895', 'driguez, Robert Stojnic, Sergey Edunov, and Thomas896', 'Scialom. 2023. Llama 2: Open Foundation and Fine-897', 'Tuned Chat Models.898', 'Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi899', 'Chen. 2023. Sheared llama: Accelerating language900', 'model pre-training via structured pruning. ArXiv,901', 'abs/2310.06694.902', 'Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin903', 'Jiang, Linjun Yang, Rangan Majumder, and Furu904', 'Wei. 2023.Inference with Reference: Lossless905', 'Acceleration of Large Language Models. _eprint:906', '2304.04487.907', 'Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris908', 'Papailiopoulos, and Kangwook Lee. 2024. Predictive909', 'Pipelined Decoding: A Compute-Latency Trade-off910', 'for Exact LLM Decoding. Transactions on Machine911', 'Learning Research.912', 'Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,913', 'Gang Chen, and Sharad Mehrotra. 2024a. Draft&914', 'Verify: Lossless Large Language Model Acceleration915', 'via Self-Speculative Decoding. In Proceedings of the916', '62nd Annual Meeting of the Association for Compu-917', 'tational Linguistics (Volume 1: Long Papers), pages918', '11263–11282, Bangkok, Thailand. Association for919', 'Computational Linguistics.920', 'Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng921', 'Chen, and Jinan Xu. 2024b. Dual-space knowledge922', 'distillation for large language models. In Proceed-923', 'ings of the 2024 Conference on Empirical Methods in924', 'Natural Language Processing, pages 18164–18181,925', 'Miami, Florida, USA. Association for Computational926', 'Linguistics.927', 'Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn928', 'Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,929', 'Tianqi Chen, and Baris Kasikci. 2024. Atom: Low-930', 'bit quantization for efficient and accurate llm serv-931', 'ing. Proceedings of Machine Learning and Systems,932', '6:196–209.933', 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan934', 'Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,935', 'Zhuohan Li, Dacheng Li, Eric Xing, and others. 2023.936', 'Judging llm-as-a-judge with mt-bench and chatbot937', 'arena. Advances in Neural Information Processing938', 'Systems, 36:46595–46623.939', 'Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,940', 'Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv941', 'Kumar, Jean-François Kagy, and Rishabh Agarwal.942', '2024. DistillSpec: Improving Speculative Decoding943', 'via Knowledge Distillation. In The Twelfth Interna-944', 'tional Conference on Learning Representations.945', 'AMore on Speculative Decoding946', 'Autoregressive decoding (Touvron et al., 2023; Bai947', 'et al., 2023; Jiang et al., 2023; OpenAI et al., 2024),948', 'suffers from inefficiency because it generates text949', 'one token at a time (Figure 5, Left).Specula-950', 'tive decoding (Chen et al., 2023; Leviathan et al.,951', '2023) follows a guess-and-verify paradigm (Figure952', '5, Right). In speculative decoding, a smaller LLM953', '(draft model) (Chen et al., 2023; Leviathan et al.,954', '2023; Miao et al., 2024; Sun et al., 2023b; Zhou955', 'et al., 2024; Cai et al., 2024) or the original LLM956', 'trained in a specialized manner (self-speculative957', 'decoding) (Elhoushi et al., 2024; Liu et al., 2024a;958', 'Yang et al., 2024; Zhang et al., 2024a; Li et al.,959', '2024b) predicts multiple tokens in advance. The960', 'original LLM then verifies these predictions in par-961', 'allel, improving efficiency.962', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'Autoregressive Decoding', '...', 'Speculate ', '(make guesses)', 'Speculative Decoding', 'A', 'B', 'B', 'C', 'C', 'D', 'T', 'Target LLM', 'AAcceptReject', 'Figure 5: Examples of Autoregressive decoding (Left)and Speculative Decoding (Right). While autoregres-sive decoding generates one token per forward step,speculative decoding generates three tokens with oneforward step.', '12', 'LLMs process discrete integer sequences as in-963', 'puts, where each integer represents a token. We de-964', 'fine the input sequence as x = (x1, x2, . . . , xs) ∈965', 'Ns of length s, and denote a slice of length m at966', 'step t as x1:m = (x1, x2, . . . , xm). The output of967', 'an LLM represents the probability distribution over968', 'the next token. The probability of generating the969', 's-th token, conditioned on all preceding tokens, is970', 'given by PM(xs | x1:s−1). The next token xs is971', 'then sampled from this distribution using various972', 'methods (e.g., greedy, top-k, and top-p sampling;973', 'see (Kool et al., 2020; Holtzman et al., 2020)). In974', 'the case of greedy sampling, the next token is se-975', 'lected as xs = arg max PM(xs | x1:s−1)976', 'Let x0 be the prompt tokens provided by the977', 'user. The LLM generates an output sequence of978', 'length m, with each generated token yi computed979', 'autoregressively. Assuming greedy sampling, the980', 'decoding process follows:981', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y1 = arg max PM(y1 | x) ', 'y2 = arg max PM(y2 | y1, x)... ', 'ym = arg max PM(ym | y1:m−1, x).', '(1)982', 'A.1Speculative Decoding983', 'Speculative decoding follows a Guess-And-Verify984', 'approach, where multiple candidate future to-985', 'kens are speculated and subsequently verified986', 'in a single decoding step.With tree attention987', '(Miao et al., 2024), multiple drafts can be ver-988', 'ified simultaneously. Let G denote the number989', 'of guesses, and define the set of guesses as ˜Y =990', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence991', 'has length K. The j-th token of the i-th guess is992', 'denoted as ˜y(i) ', 'j .993', 'In the case of speculative decoding with greedy994', 'sampling, given the prompt x, a drafting method995', 'is used to generate the draft sequences ˜Y . Using996', 'these drafts, the LLM then computes the true tokens997', '(y′1, y′2, . . . , y′K) in parallel. For instance, for the998', 'guess sequence ˜y(0), the true tokens are determined999', 'as:1000', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y′1 = arg max PM(y1 | x)', 'y′2 = arg max PM(y2 | ˜y(0) ', '1 , x)...', 'y′K = arg max PM(yK | ˜y(0) ', '1:K−1, x).', '(2)1001', 'These generated tokens are then verified. Let h1002', 'be the highest number of correct guessed tokens1003', 'across all guesses. Consequently, h + 1 tokens are1004', 'generated in one forward step. Algorithm 2 out-1005', 'lines speculative decoding with greedy sampling.1006', 'BImplementation Details1007', 'B.1Frameworks and Libraries1008', 'We implement SPECTRA in Python using PyTorch1009', '2.1.0 and the Hugging Face transformers library.1010', 'For large-scale model loading (e.g., LLaMA-2-1011', '70B, LLaMA-3-70B), we employ 16-bit (FP16)1012', 'precision with a pre-allocated key-value cache.1013', 'B.2Models and Checkpoints1014', 'We run our experiments primarily with:1015', '• LLaMA-2-Chat (Touvron et al., 2023) in1016', 'sizes 7B, 13B, 70B.1017', '• CodeLlama (Rozière et al., 2024) in sizes 7B1018', 'and 13B.1019', '• LLaMA-3-Instruct (Dubey et al., 2024) in1020', 'sizes 8B and 70B.1021', 'All checkpoints are obtained from the official or1022', 'Hugging Face repositories without fine-tuning or1023', 'modification. For each model, we enable half-1024', 'precision inference. We also verify numerically (by1025', 'comparing 32-bit and 16-bit outputs) that specula-1026', 'tive decoding preserves exact or near-exact token1027', 'sequences within typical floating-point tolerances.1028', 'B.3Hardware1029', 'Most experiments are conducted on a single1030', 'NVIDIA A100 GPU with 80GB of memory. We1031', 'also evaluate on other NVIDIA GPUs (RTX 3090,1032', 'RTX 8000, A40, A6000) to study hardware-1033', 'specific scaling. For the largest checkpoints (70B)1034', 'that do not fit on a single GPU under certain con-1035', 'figurations, we optionally distribute them across1036', 'multiple GPUs (2x, 4x, or 8x H100) using standard1037', 'pipeline-parallelism from Hugging Face’s library.1038', 'B.4Hyperparameters1039', 'Lookahead, REST, and ANPD.We replicate1040', 'each baseline using their publicly available GitHub1041', 'code, keeping to the default settings and hyperpa-1042', 'rameters outlined in the original papers.1043', 'Spectra.By default, we use a 5-gram setup for1044', 'our forward/backward dictionaries, storing all sub-1045', 'sequences (i.e., sub-ngrams). We also maintain a1046', 'candidate pool of size W = 15 per key to generate1047', '13', 'new n-gram records; after each forward pass, can-1048', 'didate sequences are shifted by one token and then1049', 're-populated. We introduce a threshold τ ∈[0, 1],1050', 'default set to 0.1, to decide when to force the se-1051', 'lection of a token not yet in the forward dictionary.1052', 'This mechanism balances coverage of unseen pre-1053', 'fixes with reinforcing common contexts. At every1054', 'speculative decoding step, we allow up to G = 601055', 'guess tokens. Internal guesses receive priority, and1056', 'if there is still capacity under the guess limit, we1057', 'add external guesses.1058', 'For external lookups, we implement a trie struc-1059', 'ture for rapid prefix queries, following a design1060', 'similar to REST (He et al., 2024). For conver-1061', 'sation tasks (e.g., MT-Bench), we gather approxi-1062', 'mately 100k examples from the UltraChat dataset1063', '(Ding et al., 2023), focusing on those with minimal1064', 'perplexity under the same LLM we aim to accel-1065', 'erate. For code tasks (e.g., HumanEval, MBPP),1066', 'we draw from TheStack (Kocetkov et al., 2023)1067', 'and again refine it to the 100k snippets with the1068', 'lowest perplexity for memory efficiency. We mea-1069', 'sure perplexity by running a single forward pass (in1070', 'streaming mode) over candidate samples and rank-1071', 'ing them. Despite being relatively small (100k),1072', 'this curated corpus achieves robust guess quality.1073', 'All speedup and throughput metrics are com-1074', 'puted at a batch size of 1. In code generation tasks,1075', 'the maximum generation length is typically 5121076', 'tokens, whereas for conversation tasks (MT-Bench,1077', 'GSM8K), we allow up to 1024 tokens or stop early1078', 'if the model outputs an end-of-sequence token. All1079', 'random seeds are set to 0.1080', 'CDetails Results with Throughputs1081', 'We provide a detailed throughput analysis to com-1082', 'plement the speedup ratios reported in the main1083', 'text. Our goal is to demonstrate how SPECTRA1084', 'scales across various model sizes, datasets, and1085', 'GPU architectures. We measure throughput using1086', 'two key metrics:1087', '• Macro Throughput (Mac-TP). Calculated1088', 'as the average of per-generation token-1089', 'processing rates—i.e., for each generation1090', 'step i, we compute tokeni/timei and then1091', 'average over all steps.1092', '• Micro Throughput (Mic-TP). Calculated as1093', 'the total number of generated tokens divided1094', 'by the total elapsed time1095', 'Table 4 focuses on GSM8K and MTBench per-1096', 'formance across four different GPU models, while1097', 'Table 3 provides more granular results on ad-1098', 'ditional datasets and model configurations.In1099', 'all cases, SPECTRA consistently achieves higher1100', 'throughput than both non-speculative baselines and1101', 'other training-free accelerators, as evidenced by im-1102', 'provements in both Mic-TP and Mac-TP. Notably,1103', 'this performance advantage remains stable even on1104', 'older GPUs (e.g., the RTX 3090 and RTX 8000),1105', 'demonstrating SPECTRA’s robustness to varying1106', 'hardware capabilities.1107', 'DEvaluating SPECTRA in Different GPU1108', 'Types1109', 'Different GPU types.Table 5 reports speedups1110', 'on GSM8K and MTBench across four GPUs with1111', 'varying memory throughput and compute capabili-1112', 'ties. While absolute wall-clock times differ across1113', 'GPUs, the relative accelerations remain consistent.1114', 'SPECTRA consistently outperforms other baselines,1115', 'including Lookahead, achieving higher speedups in1116', 'all cases. On older GPUs (e.g., RTX 3090 or RTX1117', '8000), the gap between Lookahead and SPECTRA1118', 'narrows slightly due to less efficient parallelism,1119', 'but SPECTRA maintains its lead. These results1120', 'demonstrate that SPECTRA is robust to hardware1121', 'variations and effective across both data-center and1122', 'consumer-grade GPUs.1123', 'EEvaluating SPECTRA in Multi-GPU1124', 'Environments1125', 'A critical consideration for practical deployment is1126', 'how SPECTRA scales when models are distributed1127', 'across multiple GPUs—a common requirement for1128', 'large LLMs exceeding single-device memory ca-1129', 'pacity. To evaluate this, we measure SPECTRA’s1130', 'performance under three distributed configurations1131', 'of LLaMA-2-70B: (1) 2xH100 with full precision,1132', '(2) 4xH100 with full precision, and (3) 8xH1001133', 'with full precision. We also include a baseline1134', 'of 1xH100 with 8-bit quantization for memory-1135', 'constrained single-GPU inference. Table 6 reports1136', 'throughput and speedup metrics.1137', 'SPECTRA achieves consistent speedups of 2.00—1138', '2.03× across all multi-GPU configurations while1139', 'maintaining a stable compression ratio (τ) of 2.52.1140', 'This demonstrates robust scalability—partitioning1141', 'model weights introduces minimal overhead, and1142', 'the speculative verification process remains effi-1143', 'cient despite inter-GPU communication. Notably,1144', '14', 'ClassevalGSM8KHumanevalMBPPMTBenchModelMethodMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TP', 'Greedy (temperature=0)', 'CL-13BAutoregressive30.8530.8532.0332.0332.3532.3532.0732.0730.6930.63 CL-13BANPD59.7758.0389.9989.1867.4364.6586.7686.4180.1076.68 CL-13BLookahead69.2868.6289.7389.0074.3373.2393.3892.8079.3878.67 CL-13BREST39.5337.7329.9329.4751.1547.4927.4127.3928.9227.18 CL-13BSPECTRA (Ours)73.4772.9893.3693.2384.9184.41105.44105.3981.3280.68', 'CL-7BAutoregressive41.1741.1741.1741.1741.4141.4141.6041.6038.9138.93 CL-7BANPD94.7693.02132.26131.3089.2687.13131.35130.99130.41126.64 CL-7BLookahead106.51105.95123.04121.90103.45103.51120.75120.23125.58124.77 CL-7BREST59.4956.6137.6137.2170.3865.2240.1140.0939.6436.70 CL-7BSPECTRA (Ours)111.09110.68137.24136.86122.54122.41148.32148.07143.98144.32', 'L2-13BAutoregressive31.8531.5632.4032.4332.2732.2732.1932.1931.9331.78 L2-13BANPD43.3044.4447.5445.2243.2442.2836.2035.8437.4434.84 L2-13BLookahead57.4958.9447.4447.6255.7655.5844.4144.1548.1146.62 L2-13BREST38.8137.7430.3630.2240.4739.7030.7030.6736.3937.02 L2-13BSPECTRA (Ours)63.6464.3159.2158.6363.3963.1852.4352.1956.0453.75', 'L2-70BAutoregressive2.602.602.612.612.612.612.632.632.602.60 L2-70BANPD4.724.804.254.104.854.763.073.073.473.30 L2-70BLookahead6.907.164.875.126.716.733.923.935.055.02 L2-70BSPECTRA (Ours)8.078.356.586.758.418.414.884.886.326.22', 'L2-7BAutoregressive40.3340.3241.0141.0341.1441.1341.0041.0440.4840.50 L2-7BANPD65.5468.1062.4059.3863.2759.9848.9447.6752.4750.06 L2-7BLookahead88.4191.0568.0068.2084.6983.8759.7960.7670.0469.07 L2-7BREST54.7453.9341.4341.3857.9956.4141.2840.7450.5851.79 L2-7BSPECTRA (Ours)96.8898.7586.5185.5098.7798.3872.3973.2281.9379.20', 'L3-70BAutoregressive2.582.572.582.582.592.592.592.592.552.55 L3-70BANPD3.974.193.863.724.724.753.773.593.143.03 L3-70BLookahead6.176.473.993.966.636.753.703.664.494.53 L3-70BSPECTRA (Ours)6.877.185.435.347.337.505.014.885.255.16', 'L3-8BAutoregressive36.5936.5836.7436.7436.2036.2135.2435.2036.5536.69 L3-8BANPD77.2178.76141.89141.3666.3165.57118.47112.9541.7740.20 L3-8BLookahead94.9297.09136.32135.9289.9990.47133.67133.1256.0955.49 L3-8BSPECTRA (Ours)103.61105.88142.89142.7292.8693.16143.80142.7261.6960.22', 'Sampling (temperature=1.0)', 'CL-13BAutoregressive30.9030.6431.3831.3731.2431.3931.4631.4530.7130.67 CL-13BANPD35.4834.8633.5432.3432.6434.3631.5730.9570.9265.68 CL-13BLookahead42.5440.7433.7932.4940.2542.1732.0231.1971.5068.46 CL-13BREST35.1533.2225.6725.2439.5838.4926.4325.8928.4126.69 CL-13BSPECTRA (Ours)51.8650.0437.5735.6751.6052.6436.2935.2772.9069.98', 'CL-7BAutoregressive39.6039.5840.8540.8740.0540.1040.8140.8140.4940.50 CL-7BANPD50.8951.7647.4446.6844.1446.3445.8645.81112.29103.57 CL-7BLookahead60.8760.2948.5447.6457.1261.1448.6448.27110.07105.00 CL-7BREST48.6446.4135.9835.4653.3552.2637.0436.5739.3636.51 CL-7BSPECTRA (Ours)71.7071.7855.2452.8167.2769.2054.4852.91112.43108.49', 'L2-13BAutoregressive31.2331.1731.4431.4731.4131.4232.0232.0631.6731.59 L2-13BANPD37.5337.9439.1137.9936.7936.7532.9732.7136.9134.34 L2-13BLookahead47.5947.3541.6041.7646.3346.5137.8237.8247.3545.48 L2-13BREST36.7836.1729.3329.2537.4636.7129.3829.2835.5036.21 L2-13BSPECTRA (Ours)53.1352.2848.6048.1152.9353.1142.9543.0354.9852.42', 'L2-7BAutoregressive39.8939.8840.5840.5940.0940.1040.5940.6640.6540.70 L2-7BANPD52.1452.7854.2352.9051.4050.9744.7343.7750.9248.24 L2-7BLookahead70.8271.1761.1561.3468.7869.0150.8451.8368.2766.77 L2-7BREST50.3549.9940.1940.0950.8650.0638.9438.1849.1250.54 L2-7BSPECTRA (Ours)78.4678.7472.1371.6881.7181.7659.7760.0980.2177.00', 'L3-8BAutoregressive35.7535.7635.1635.1736.0136.0236.0536.0735.3935.48 L3-8BANPD44.7143.7269.1266.7351.4851.5768.0364.5440.8439.23 L3-8BLookahead53.0550.5772.6869.1164.5963.7971.8868.9055.4653.74 L3-8BSPECTRA (Ours)69.5068.9279.8876.5369.0968.6278.9976.6960.3357.69', 'Table 3: Micro throughput (Mic-TP) and Macro throughput (Mac-TP) across multiple tasks and models.', '15', 'GPUMethodGSM8KMTBenchMac-TPMic-TPMac-TPMic-TP', 'A40Autoregressive32.6632.6632.1431.66 ', 'Lookahead48.5948.7349.1347.96 ', 'SPECTRA62.5661.5259.0056.80', 'A6000Autoregressive39.1539.1738.7838.24 ', 'Lookahead58.1358.3058.8457.40 ', 'SPECTRA75.2074.1671.369.28', 'RTX8000Autoregressive34.0334.2734.2134.02 ', 'Lookahead45.2545.4245.7344.16 ', 'SPECTRA57.9557.0954.1652.32', 'RTX3090Autoregressive40.6740.7641.1741.22 ', 'Lookahead53.6953.7553.5152.09 ', 'SPECTRA74.8773.8871.5869.79', 'Table 4: Throughput results for different GPU types on GSM8K and MTBench.', 'GPUMethodGSM8KMTBenchspeedupτspeedupτ', 'A40Lookahead1.491.931.532.07 ', 'SPECTRA1.922.461.842.36', 'A6000Lookahead1.481.921.522.06 ', 'SPECTRA1.922.461.842.36', 'RTX8000Lookahead1.331.931.342.08 ', 'SPECTRA1.702.461.582.35', 'RTX3090Lookahead1.321.921.302.06 ', 'SPECTRA1.842.461.742.36', 'Table 5: Hardware scalability of SPECTRA decoding onGSM8K and MTBench for various GPU architectures.', 'even in the quantized single-GPU setting, SPEC-1145', 'TRA provides a 2.43× speedup, outperforming1146', 'standard autoregressive decoding. These results1147', 'validate SPECTRA’s practicality for large-scale de-1148', 'ployments where memory constraints necessitate1149', 'distributed inference.1150', 'FVerifying Generation Quality with1151', 'SPECTRA Decoding1152', 'Greedy Decoding Performance.To assess the1153', 'quality of greedy decoding, we compare the in-1154', 'ference results of the LLaMA-2-7B Chat model1155', 'using SPECTRA Decoding against Hugging Face’s1156', 'standard greedy search. Our baseline consists of1157', 'single-precision (FP32) inference on 160 conver-1158', 'sational turns from the MT-Bench dataset. Under1159', 'FP32, SPECTRA Decoding produces identical out-1160', 'puts to the baseline.1161', 'However, when transitioning to half-precision1162', '(FP16), even Hugging Face’s native greedy search1163', 'generates 25 discrepancies (out of 160) compared1164', 'to the FP32 baseline. SPECTRA Decoding exhibits1165', 'a similar discrepancy rate (26), confirming that it1166', 'maintains the output distribution within the numer-1167', 'ical error margins typically observed in standard1168', 'half-precision inference libraries.1169', 'Sampling Decoding Performance.We also as-1170', 'sess generation quality under a stochastic sam-1171', 'pling setting (temperature = 1.0). As detailed in1172', 'Table 7, SPECTRA Decoding produces ROUGE-1173', '1, ROUGE-2, and ROUGE-L scores on both the1174', 'CNN/DailyMail (Nallapati et al., 2016) and XSum1175', '(Narayan et al., 2018) summarization datasets1176', 'that are nearly identical to those of standard1177', 'autoregressive sampling.At the same time,1178', 'SPECTRA achieves notable speedups (1.60× on1179', 'CNN/DailyMail and 1.69× on XSum) with com-1180', 'pression ratios of 2.05 and 2.08, respectively.1181', 'These results confirm that SPECTRA Decoding1182', 'accelerates inference while preserving generation1183', 'quality across diverse tasks.1184', 'These findings reaffirm that SPECTRA Decoding,1185', 'does not degrade generation quality compared to1186', 'conventional greedy or sampling-based methods.1187', 'GToken Acceptance Rate Analysis1188', 'Figure 6 plots the cumulative number of accepted1189', 'tokens versus decoding steps for each dataset (MT-1190', 'Bench, HumanEval, MBPP, and GSM8K). The1191', 'steeper ascent of the SPECTRA curve indicates that1192', 'our method requires substantially fewer decoding1193', 'steps compared to alternatives, for example, almost1194', 'two times shorter than ANPD. This improvement is1195', '16', 'GPU & Model SettingMethodMTBenchMac-TPMic-TPSpeedupτ', '1xH100 - Quantized Int8Autoregressive2.602.601.001.00 ', 'SPECTRA6.326.222.432.51', '2xH100 - FP16Autoregressive14.8114.701.001.00 ', 'SPECTRA29.6228.912.002.52', '4xH100 - FP16Autoregressive14.6014.481.001.00 ', 'SPECTRA29.6728.892.032.52', '8xH100 - FP16Autoregressive14.3914.281.001.00 ', 'SPECTRA29.2728.552.032.52', 'Table 6: Results in multi-GPU Enviroments on GSM8K and MTBench using LLama-2-chat-70B.', 'DatasetMethodROUGE-1ROUGE-2ROUGE-LSpeedupτ', 'CNNAutoregressive9.770.397.201.001.00 ', 'SPECTRA9.740.417.181.602.05', 'XSUMAutoregressive18.124.3612.431.001.00 ', 'SPECTRA18.134.4012.491.692.08', 'Table 7: Evaluation of SPECTRA Decoding on CNN/DailyMail and XSum using a temperature of 1.0. ROUGEscores, speedups over autoregressive decoding, and compression ratio (τ) are reported for LLaMA-2-7B-Chat.', 'attributed to a higher token acceptance rate, which1196', 'in turn reduces the overall number of decoding iter-1197', 'ations and enhances the efficiency of the generation1198', 'process.1199', 'HAlgorithms1200', '17', '0250500750', '0', '10', '20', '30', '40', 'MT-Bench', '0200400', '0', '10', '20', '30', '40', 'HumanEval', '050100150', '0', '5', '10', '15', 'MBPP', '0100200', '0', '10', '20', '30', '40', '50', '60', 'GSM8K', '#Accepted tokens (in thousands)', 'LookaheadRESTANPDSpectra (Ours)', 'Figure 6: Total number of accepted tokens across all samples at each decoding step.', 'Algorithm 2 Speculative Decoding (Multiple guesses and Greedy Sampling)', 'Given guess size K, number of guesses G, and target length T. Given initial prompt sequence x. ', 'while n < T do', 'Obtain multiple drafts ˜Y = {˜y(0), ˜y(1), . . . , ˜y(G)}.', 'In parallel, compute K + 1 verification tokens y′: for i = 1 : K do', 'y′(g) ', 'i= arg max PM(yi | ˜y(g) ', 'i−1, x),∀g ∈{0, . . . , G}', 'end forIdentify the sequence ˜y(g∗) with the highest token matches and the corresponding y′(g).', 'for t = 1 : K do', 'if y′(g) ', 't= ˜y(g∗) ', 'tthen', 'Set yn+t ←˜y(g∗) ', 'tand n ←n + 1.', 'else', 'yn+t ←y′(g) ', 'tand exit for loop.', 'end if', 'end for', 'end while', '18', 'Algorithm 3 Greedy Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function GREEDYVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x and all gi2’s outputs – totally N ', 'distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:P ←D[l][i]', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:if sj = arg max P then▷accepted, update all potential speculations and probabilities', '14:o.append(sj)', '15:is_accept ←1', '16:Vnew, Dnew ←∅, ∅', '17:for k = j to size(V ) do', '18:if sj = V [k] then', '19:Vnew.append(V [k])', '20:Dnew.append(D[k])', '21:end if', '22:end for', '23:V, D ←Vnew, Dnew', '24:break', '25:else▷rejected, go to next speculation', '26:j ←j + 1', '27:end if', '28:end while', '29:if is_accept then', '30:continue', '31:else▷guarantee one step movement', '32:o.append(arg max P)', '33:break', '34:end if', '35:end for', '36:if is_accept then', '37:o.append(arg max D[1]N)', '38:end if', '39:return o', '40: end function', '19', 'Algorithm 4 Sample Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function SAMPLEVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x0 and all gi2’s outputs – totally N ', 'probability distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:Pj ←D[1]i', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:sample r ∼U(0, 1)', '14:if r ≤Pj(sj) then▷accepted, update all potential speculations and probabilities', '15:o.append(sj)', '16:is_accept ←1', '17:Vnew, Dnew ←∅, ∅', '18:for k = j to size(V ) do', '19:if sj = V [k] then', '20:Vnew.append(V [k])', '21:Dnew.append(D[k])', '22:end if', '23:end for', '24:V, D ←Vnew, Dnew', '25:break', '26:else▷rejected, go to next speculation', '27:Pj(sj) ←0', '28:Pj+1 = norm(Pj)', '29:j ←j + 1', '30:end if', '31:end while', '32:if is_accept then', '33:continue', '34:else▷guarantee one step movement', '35:sample xnext ∼Pj', '36:o.append(xnext)', '37:break', '38:end if', '39:end for', '40:if is_accept then', '41:o.append(sample xnext ∼D[1]N)', '42:end if', '43:return o', '44: end function', '20']
2025-02-19 15:53:44 - WARNING: done parsing pdf
2025-02-19 15:53:44 - WARNING: start ner pdf
2025-02-19 15:53:48 - INFO: Loading Data
2025-02-19 15:53:48 - WARNING: 2025-02-19 15:53:48 - INFO: Loading Data
2025-02-19 15:53:51 - WARNING: Predicting NER ...
2025-02-19 15:53:54 - WARNING: Finished predicting.
2025-02-19 15:53:54 - WARNING: Converting to Brat format...
2025-02-19 15:53:54 - WARNING: # of discontinuous mentions:
2025-02-19 15:53:54 - WARNING:  
2025-02-19 15:53:54 - WARNING: 0
2025-02-19 15:53:54 - WARNING: Finished.
2025-02-19 15:53:54 - WARNING: start predict re
2025-02-19 15:53:58 - WARNING: Example:   0%|          | 0/39 [00:00<?, ?it/s]
2025-02-19 15:53:58 - WARNING: Example: 100%|##########| 39/39 [00:00<00:00, 1415.07it/s]
2025-02-19 15:53:58 - WARNING: # of documents 39.
2025-02-19 15:53:58 - WARNING: # of positive examples 0.
2025-02-19 15:53:58 - WARNING: # of negative examples 104.
2025-02-19 15:53:59 - WARNING: dict_keys(['511', '541', '544', '1364', '1478'])
2025-02-19 15:53:59 - WARNING: done predict re
2025-02-19 15:53:59 - WARNING: model output text 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 15:53:59 - WARNING: len of model_output_text 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: 51
2025-02-19 15:53:59 - WARNING: original_text 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: and time-consuming. Speculative decoding has003
2025-02-19 15:53:59 - WARNING: mapping dict 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 7, 9: 8, 10: 8, 11: 9, 12: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 17, 21: 18, 22: 19, 23: 20, 24: 21, 25: 22, 26: 23, 27: 24, 28: 25, 29: 26, 30: 27, 31: 28, 32: 29, 33: 30, 34: 31, 35: 32, 36: 33, 37: 34, 38: 35, 39: 36, 40: 37, 41: 38, 42: 39, 43: 40, 44: 41, 45: 42, 46: 43, 47: 44, 48: 45, 49: 46, 50: 46}
2025-02-19 15:53:59 - WARNING: text: 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 15:53:59 - WARNING: origin bbox 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 15:53:59 - WARNING: normalized bbox 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 15:53:59 - WARNING: finalized bbox 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 15:53:59 - WARNING:  final bouding box 
2025-02-19 15:53:59 - WARNING:  
2025-02-19 15:53:59 - WARNING: {'x1': 12.217000007629395, 'y1': 269.133544921875, 'x2': 272.1279296875, 'y2': 281.13848876953125, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 15:53:59 - WARNING: done save re and ner, 
2025-02-19 15:53:59 - WARNING: start count num_rel 
2025-02-19 15:53:59 - WARNING: done count num_rel 
2025-02-19 15:53:59 - WARNING: start update document 
2025-02-19 15:54:00 - WARNING: start matching infor
2025-02-19 15:54:00 - WARNING: done matching infor
2025-02-19 15:54:00 - WARNING: start commit
2025-02-19 15:54:00 - WARNING: done update document 
2025-02-19 15:54:03 - WARNING: Failed to send email. Error: {'an': (553, b'5.1.3 The recipient address <an> is not a valid RFC 5321 address. For more\n5.1.3 information, go to\n5.1.3  https://support.google.com/a/answer/3221692 and review RFC 5321\n5.1.3 specifications. 41be03b00d2f7-add540f75f7sm7638184a12.46 - gsmtp')}
2025-02-19 15:54:03 - INFO: Task dev_tasks.process_pdf_task[8974ab72-4721-4af5-a62e-486fe3657b0e] succeeded in 20.67271214351058s: {'id': 31434211, 'filename': '_ACL_2025__LLM_Efficiency (2) (1).pdf', 'upload_time': '2025/02/19, 06:53:43', 'entities': 237, 'relations': 6, 'pages': 20, 'status': 'completed'}
2025-02-19 15:54:03 - WARNING: 2025-02-19 15:54:03 - INFO: Task dev_tasks.process_pdf_task[8974ab72-4721-4af5-a62e-486fe3657b0e] succeeded in 20.67271214351058s: {'id': 31434211, 'filename': '_ACL_2025__LLM_Efficiency (2) (1).pdf', 'upload_time': '2025/02/19, 06:53:43', 'entities': 237, 'relations': 6, 'pages': 20, 'status': 'completed'}
2025-02-19 16:02:52 - INFO: Task dev_tasks.process_pdf_task[42dcf943-459e-4a8b-9209-374cd243bb82] received
2025-02-19 16:02:52 - WARNING: 2025-02-19 16:02:52 - INFO: Task dev_tasks.process_pdf_task[42dcf943-459e-4a8b-9209-374cd243bb82] received
2025-02-19 16:02:52 - WARNING: uploads/_ACL_2025__LLM_Efficiency (2) (1).pdf
2025-02-19 16:02:52 - WARNING: start parsing pdf
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 4
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 4
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 4
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 4
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 4
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 4
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 4
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 4
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 3
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 2
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 1
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:53 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 2
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 2
2025-02-19 16:02:54 - WARNING: 2
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 2
2025-02-19 16:02:54 - WARNING: 2
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 1
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: 0
2025-02-19 16:02:54 - WARNING: parsed 1637 paragraphs
2025-02-19 16:02:54 - WARNING: ['SPECTRA: Faster Large Language Model Inference withOptimized Internal and External Speculation', 'Anonymous ACL submission', 'Abstract', 'Inference with modern Large Language Mod-001', 'els (LLMs) is both computationally expensive002', 'and time-consuming. Speculative decoding has003', 'emerged as a promising solution, but existing004', 'approaches face key limitations: training-based005', 'methods require a draft model that is challeng-006', 'ing to obtain and lacks generalizability, while007', 'non-training methods offer limited speedup008', 'gains. In this work, we present SPECTRA, a009', 'novel framework for accelerating LLM infer-010', 'ence without the need for additional training.011', 'SPECTRA introduces two innovative techniques012', 'for efficiently managing internal and external013', 'knowledge, each outperforming corresponding014', 'state-of-the-art (SOTA) methods independently.015', 'When combined, these techniques achieve up016', 'to a 4.08x speedup across various benchmarks017', 'and LLM architectures, significantly surpassing018', 'existing non-training approaches. The imple-019', 'mentation of SPECTRA is publicly available.020', '1Introduction021', 'Generating long sequences with low latency using022', 'Large Language Models (LLMs) is a critical re-023', 'quirement. Current LLMs rely on autoregressive024', 'decoding (Touvron et al., 2023; Bai et al., 2023;025', 'Jiang et al., 2023; OpenAI et al., 2024), which026', 'suffers from inefficiency because it generates text027', 'one token at a time. This results in generation028', 'time scaling linearly with the sequence length and029', 'underutilizes the parallel processing capabilities030', 'of modern GPUs. A widely studied approach to031', 'mitigate this issue is speculative decoding (Chen032', 'et al., 2023; Leviathan et al., 2023), which fol-033', 'lows a guess-and-verify paradigm. In this approach,034', 'a smaller LLM (draft model) (Chen et al., 2023;035', 'Leviathan et al., 2023; Miao et al., 2024; Sun et al.,036', '2023b; Zhou et al., 2024; Cai et al., 2024) or the037', 'original LLM trained in a specialized manner (self-038', 'speculative decoding) (Elhoushi et al., 2024; Liu039', 'et al., 2024a; Yang et al., 2024; Zhang et al., 2024a;040', 'Li et al., 2024b) predicts multiple tokens in ad-041', 'vance. The original LLM then verifies these pre-042', 'dictions in parallel, improving efficiency. However,043', 'these approaches require additional training, which044', 'demands substantial computational resources and045', 'may degrade the original model’s capabilities.046', 'Another line of research focuses on speculat-047', 'ing subsequent tokens without requiring additional048', 'training. This approach eliminates the need for049', 'training new models or modifying the original large050', 'language model (LLM), making it practical for051', 'off-the-shelf deployment. Some methods leverage052', 'specialized mechanisms to generate speculative to-053', 'kens directly from the LLM’s predictions (Fu et al.,054', '2024; Ou et al., 2024), while others rely on ex-055', 'ternal information sources to derive these tokens056', '(Yang et al., 2023; He et al., 2024; Li et al., 2024a).057', 'However, the speedup gain in these approaches re-058', 'mains limited due to the quality of the speculative059', 'guesses.060', 'We introduce SPECTRA (Figure 1a), a specu-061', 'lative decoding method that improves generation062', 'speed without requiring any training or modifica-063', 'tions to the original LLM. SPECTRA consists of064', 'two main components: a core module (SPECTRA-065', 'CORE, Figure 1c), which integrates seamlessly into066', 'LLMs in a plug-and-play manner, and an optional067', 'retrieval module (SPECTRA-RETRIEVAL, Figure068', '1e) that further enhances performance. The core069', 'module SPECTRA-CORE improves speculative de-070', 'coding by leveraging the token distribution pre-071', 'dicted by the LLM to generate high-quality guesses.072', 'Specifically, it employs two multi-level N-gram073', 'dictionaries that enable bi-directional search for074', 'dynamic-length guesses, balancing both quality075', 'and quantity. Additionally, SPECTRA optimizes a076', 'candidate pool to continuously update the N-gram077', 'dictionaries, ensuring broad token coverage. All078', 'updates to these resources, along with guess verifi-079', 'cation, are performed efficiently in a single forward080', 'pass. The retrieval module, SPECTRA-RETRIEVAL,081', '1', 'SPECTRA-RETRIEVAL', 'Module', 'N-gram Store', 'Input', 'Lookahead', 'Candidate', 'LLM', '(b) Lookahead decoding(c) SPECTRA-CORE', 'Guesses', 'Input', 'SPECTRA', 'Candidate', 'LLM', 'Bi-directional', 'Guesses', 'Multi-level', 'N-gram Store', 'InputCorpus', 'Guesses', 'LLM', 'Trie', 'Input', 'Guesses', 'LLM', 'Trie', '(d) REST', 'Corpus Refined', 'by Perplexity', '(e) SPECTRA-RETRIEVAL', '(a) Ours: SPECTRA', 'SPECTRA-CORE', 'Module', 'Input', 'GuessesLLM', 'Figure 1: Overview of Spectra and comparison with other non-training SOTA approaches. (a) Overview of SPECTRA.(b) Overview of Lookahead Decoding (Fu et al., 2024). (c) Overview of the SPECTRA-CORE module, which utilizesthe knowledge inside LLM for obtaining guesses. (d) Overview of REST (He et al., 2024). (e) Overview of theSPECTRA-RETRIEVAL module, which is designed to be integrated efficiently with SPECTRA-CORE to boost thespeedup. The results in the bar chart are measured on HumanEval.', 'can be integrated to further enhance speedup. Ex-082', 'isting approaches that rely on external sources for083', 'generating guesses (He et al., 2024) struggle to in-084', 'tegrate with other speculative decoding methods,085', 'as the search time outweighs the speedup gains.086', 'SPECTRA-RETRIEVAL addresses this issue by re-087', 'ducing the search space, selecting only high-quality088', 'content from the corpus based on perplexity scores089', 'computed by the target LLM. This optimization en-090', 'ables seamless integration with SPECTRA-CORE,091', 'maximizing efficiency.092', 'Empirical results on six tasks—including multi-093', 'turn conversation, code generation, and mathemati-094', 'cal reasoning—across three LLM families (Llama095', '2 (Touvron et al., 2023), Llama 3 (Dubey et al.,096', '2024), and CodeLlama (Rozière et al., 2024)) with097', 'model sizes ranging from 7B to 70B demonstrate098', 'that SPECTRA outperforms other non-training spec-099', 'ulative decoding methods, achieving speedups of100', 'up to 4x. We publicly release the code and data.101', 'The key contributions of this paper are as follows:102', '• We introduce SPECTRA, which improves spec-103', 'ulative decoding by effectively leveraging the104', 'LLM’s predicted token distribution. SPEC-105', 'TRA is a plug-and-play solution that requires106', 'no modifications to the LLM (Section 3.1).107', '• SPECTRA’s retrieval module refines external108', 'corpora using perplexity scores computed by109', 'the target LLM, providing a general frame-110', 'work that enables speculative decoding ap-111', 'proaches relying on external information to112', 'be seamlessly integrated with other specula-113', 'tive decoding techniques (Section 3.2).114', '• Extensive experiments across diverse tasks,115', 'LLM architectures, GPU types, and settings116', 'demonstrate the efficiency of SPECTRA, out-117', 'performing other non-training speculative de-118', 'coding approaches (Section 5).SPECTRA119', 'also integrates with acceleration tools such as120', 'FlashAttention and pipeline parallelism (Sec-121', 'tion 5.2). The code and data are available.122', '2Preliminaries123', '2.1Autoregressive Decoding in LLMs124', 'Given an input sequence x = (x1, x2, . . . , xs)125', 'of length s, and a slice of length m as x1:m =126', '(x1, x2, . . . , xm), the output of an LLM repre-127', 'sents a probability distribution over the next to-128', 'ken. The probability of generating the s-th token,129', 'conditioned on all preceding tokens, is given by130', 'PM(xs | x1:s−1). The next token xs is sampled131', 'from this distribution using methods such as greedy,132', 'top-k, or top-p sampling (see (Kool et al., 2020;133', 'Holtzman et al., 2020)). For greedy sampling, the134', 'next token is selected as xs = arg max PM(xs |135', 'x1:s−1). Consequently, the LLM generates an out-136', 'put sequence (y1, y2, . . . , ym) of length m autore-137', 'gressively, where each token yi is computed as138', '2', 'yi = argmax PM(yi | y1:i−1, x).139', '2.2Speculative Decoding140', 'Speculative decoding follows a guess-and-verify141', 'approach, where multiple candidate future to-142', 'kens are speculated and subsequently verified143', 'in a single decoding step.With tree attention144', '(Miao et al., 2024), multiple drafts can be ver-145', 'ified simultaneously. Let G denote the number146', 'of guesses, and define the set of guesses as ˜Y =147', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence148', 'has length K. The j-th token of the i-th guess is149', 'denoted as ˜y(i) ', 'j .150', 'In the case of speculative decoding with greedy151', 'sampling, given the prompt x, a drafting method152', 'generates the draft sequences ˜Y . Using these drafts,153', 'the LLM computes the true tokens (y′1, y′2, . . . , y′K)154', 'in parallel. These tokens are then verified, and155', 'h is defined as the highest number of correctly156', 'guessed tokens across all guesses. Consequently,157', 'h + 1 tokens are generated in a single forward158', 'step. Algorithm 2 outlines speculative decoding159', 'with greedy sampling, and additional details are160', 'provided in Appendix A.161', '3SPECTRA DECODING162', 'SPECTRA consists of two modules (SPECTRA-163', 'CORE and SPECTRA-RETRIEVAL) that can func-164', 'tion independently or together. The core module165', '(SPECTRA-CORE) improves speedup by leveraging166', 'the LLM’s predicted token distribution to gener-167', 'ate high-quality guesses and integrates into LLMs168', 'in a plug-and-play manner. The retrieval module169', '(SPECTRA-RETRIEVAL) derives guesses from a re-170', 'fined external information source and is designed to171', 'integrate with SPECTRA-CORE to further enhance172', 'performance.173', '3.1SPECTRA-CORE174', 'SPECTRA-CORE maintains an N-gram storage and175', 'a candidate pool. The candidate pool C contains176', 'W sequences, {c(0), c(1), . . . , c(W−1)}, with each177', 'sequence consisting of N tokens. Let c(i) ', 'jrepresent178', 'the j-th token in the i-th sequence. The N-gram179', 'storage includes two dictionaries: the forward dic-180', 'tionary Sfwd and the backward dictionary Sbwd. At181', 'each time step, guesses G are obtained through a182', 'bidirectional search using Sfwd and Sbwd. A sin-183', 'gle forward pass to the LLM retrieves all neces-184', 'sary distributions, which are used to generate new185', 'candidate tokens for C and verify the guesses G.186', 'Algorithm 1 SPECTRA Internal Knowledge', 'Require: Sequence x = (x1, x2, . . . , xn), model PM, maxN-gram size N, candidate pool size W, max guesses G,max number of new tokens m. Refine threshold τ', '1: Initialize N-gram Forward-dictionary Sfwd ←∅2: Initialize N-gram Backward-dictionary Sbwd ←∅3: Random c(i) ', 'j , ∀j ∈[0, N −1], ∀i ∈[0..W −1]', '4: t ←n + 15: while t ≤m do6:{Obtain the guesses}', '7:G ←Sfwd[xt−1]', '8:u = ∅', '9:for j = 0 to N −1 do', '10:for k = N −1 to 1 do', '11:uj ←Sbwd[xt+j−k:t−1 ⊕u0:j−1]', '12:break if found value for uj', '13:end for', '14:end for', '15:G.append(u)', '16:{Foward in LLM}', '17:Obtain necessary distributions of PM in parallel.', '18:{Verification}', '19:{Greedy verify (Alg. 3) or Sampling verify (Alg. 4)}', '20:hits ←VerificationFunction(x, PM, g)', '21:x = x ⊕hits', '22:t ←t + size(hits)', '23:{Predict Candidates}', '24:for i = 0 to W −1 do', '25:r ∼Uniform[0, 1]', '26:Pc(c(i) ', 'N−1) ←PM(c(i) ', 'N−1 | c(i) ', ':N−2, x)', '27:if r > τ then', '28:c(i) ', 'N−1 ←argmax ', 'c/∈SfwdPc(c(i) ', 'N−1)', '29:else', '30:c(i) ', 'N−1 ←argmax Pc(c(i) ', 'N−1)', '31:end if', '32:end for', '33:{Update N-gram dictionaries}', '34:for i = 0 to W −1 do', '35:for j = 0 to N −2 do', '36:Sfwd[c(i) ', 'j ].append(c(i) ', 'j+1:)', '37:Sbwd[c(i) ', '0:j] ←c(i) ', 'j+1', '38:end for', '39:end for', '40:{Update Candidates}', '41:c(i) ', 'j←c(i) ', 'j+1, ∀j ∈[0, N −2], ∀i', '42: end while43: Output: xn+1:n+m = (y1, y2, . . . , ym)', 'The dictionaries Sfwd and Sbwd are updated with N-187', 'grams from the candidate pool. The details of the188', 'SPECTRA-CORE decoding process are described189', 'in Algorithm 1.190', 'Bi-directional Search for GuessesAt each step,191', 'SPECTRA generates G guess sequences G=192', '{˜y(0), ˜y(1), . . . , ˜y(G)}. Unlike previous work (Fu193', 'et al., 2024), which enforces uniform guess lengths,194', 'SPECTRA supports variable-length guesses, im-195', 'proving both flexibility and efficiency. The for-196', 'ward dictionary Sfwd maps a token to a list of197', 'sequences, while the backward dictionary Sbwd198', 'maps a sequence to a single token. At time step199', '3', 'Input', 'Input', 'Large Language Model', 'Token distribution', 'Figure 2: Details of SPECTRA forward step in LLM.The dashed arrow indicates interactions between thetokens, which are realized by the LLM’s attention mask.', 't, the set of guesses is obtained through a bidi-200', 'rectional search (Alg. 1, lines 7–15). This search201', 'operates in two directions: (1) the forward direc-202', 'tion, which prioritizes the quantity of guesses, and203', '(2) the backward direction, which prioritizes the204', 'quality of guesses. In the forward direction, the205', 'last generated token xt−1 is used to search Sfwd206', 'for guess sequences (Alg. 1, line 7). In the back-207', 'ward direction, a high-quality guess is constructed208', 'by iteratively predicting one token at a time using209', 'Sbwd, repeating the process until a desired sequence210', 'length N is reached (Alg. 1, lines 8–14).211', 'Predict & Verify in One Forward PassAll dis-212', 'tributions required for predicting candidates and213', 'verifying guesses are obtained in a single forward214', 'pass to the LLM, leveraging parallel processing215', '(Figure 2). This is achieved using a specially de-216', 'signed attention mask that specifies the allowed217', 'interactions between tokens. For instance, the to-218', 'ken c(1) ', '2attends only to c(1) ', '1 , c(1) ', '0 , and the input.219', 'Predict Tokens for Candidate PoolWe predict220', 'the next candidate tokens c(i) ', 'N−1 for the candidate221', 'pool using the distribution obtained from the for-222', 'ward pass (Alg. 1, lines 24–32). A straightfor-223', 'ward approach is to select tokens with the highest224', 'probability in the token distribution. However, we225', 'observe that when searching for guesses in the for-226', 'ward dictionary Sfwd, it is crucial for the search to-227', 'ken to exist in the dictionary; otherwise, no guesses228', 'can be retrieved. To address this, we introduce a229', 'randomness-based mechanism to increase the cov-230', 'erage of Sfwd. Specifically, we probabilistically231', 'encourage the selection of unseen tokens in Sfwd232', 'using a hyperparameter τ ∈[0, 1]. Let r be a233', 'random draw from [0, 1]. If r > τ, we select to-234', 'kens with the highest probability that are not in235', 'Sfwd; otherwise, we choose tokens with the high-236', 'est probability regardless of their presence in Sfwd.237', 'Although c(i) ', 'N−1 does not immediately affect the238', 'coverage of Sfwd, it contributes to coverage expan-239', 'sion in subsequent time steps through our candidate240', 'updating mechanism. At the end of each time step,241', 'all candidate sequences are shifted left by one to-242', 'ken: c(i) ', 'j←c(i) ', 'j+1, leaving c(i) ', 'N−1 empty and ready243', 'for prediction in the next time step (Alg. 1, line 41).244', 'Update N-gram DictionariesAt the end of each245', 'time step, candidate tokens from the pool C are246', 'used to update the N-gram dictionaries Sfwd and247', 'Sbwd. While previous work (Fu et al., 2024) only248', 'adds the full N-gram (c(i) ', '0 , c(i) ', '1 , . . . , c(i) ', 'N ), we ob-249', 'serve that subsequences within N-grams often ap-250', 'pear later in the generation process. By including251', 'these subsequences in the N-gram storage, we im-252', 'prove both the quality of guesses and the coverage253', 'of the dictionaries. Specifically, we add subse-254', 'quences to Sfwd using the first token as the key,255', 'and update Sbwd by mapping the preceding part of256', 'the sequence to the last token (Alg. 1, lines 33–39).257', '3.2SPECTRA-RETRIEVAL258', 'SPECTRA-RETRIEVALleveragesanexternal259', 'knowledge source to generate guesses. This in-260', 'volves processing a text corpus and indexing it into261', 'a structure that supports fast prefix search, such as a262', 'trie. At each time step, the last generated tokens are263', 'used as input to this structure to retrieve guesses for264', 'speculative decoding. However, we observe that us-265', 'ing random texts from the corpus without selection266', 'can limit the speedup gain. To address this, we pro-267', 'pose a method to identify and select high-quality,268', 'relevant texts from the corpus tailored to the spe-269', 'cific LLM. This improves the speedup gain and270', 'enables seamless integration with other speculative271', 'decoding approaches, including SPECTRA-CORE.272', 'Corpus Refinement by PerplexityGiven a text sequence u = (u0, u1, u2, . . . ), perplexity quan-tifies the average uncertainty of the model when predicting the next token, conditioned on the pre-ceding tokens. It is calculated as:', 'PPL(u) = exp', '�', '−1', 't', 't�', 'i=1log pθ(ui | u<i)', '�', 'A lower perplexity indicates that the model assigns273', 'higher probabilities to the sequence, suggesting274', 'that the sequence is well-aligned with the model’s275', 'predictions and can produce high-quality guesses276', 'for speculative decoding. To optimize the retrieval277', 'process, we select texts with the lowest perplexity278', '4', 'from the corpus to form a smaller, high-quality sub-279', 'set, which is then used to construct the trie structure280', 'for generating guesses.281', 'Integration with SPECTRA-COREOur exper-282', 'iments (Section 5.2, Table 2) demonstrate that283', 'naively integrating guesses from external sources284', '(e.g., REST (He et al., 2024)) into other specula-285', 'tive methods (e.g., Lookahead (Fu et al., 2024))286', 'can lead to a noticeable drop in speedup. This oc-287', 'curs because the forward pass in the LLM can only288', 'handle a limited number of guesses, and exceeding289', 'this limit increases memory usage and slows down290', 'generation. With a limited guess budget, guesses291', 'from external sources can only account for a frac-292', 'tion of the total guesses, causing the search time293', 'in the indexing structure (e.g., a trie) to outweigh294', 'the speedup gain. To address this, it is crucial295', 'to limit the size of the external knowledge while296', 'maintaining the quality of the guesses. By refining297', 'the corpus using perplexity, SPECTRA-RETRIEVAL298', 'seamlessly integrates with SPECTRA-CORE, further299', 'boosting the speedup gain. Specifically, we inte-300', 'grate SPECTRA-RETRIEVAL into SPECTRA-CORE301', 'by including its guesses in the set of guesses during302', 'the guess generation step (Alg. 1, lines 7–15).303', '4Experiments304', 'Models.We evaluate LLaMA-2-Chat 7B, 13B,305', '70B (Touvron et al., 2023), CodeLlama 7B, 13B306', '(Rozière et al., 2024), and LLaMA-3-Instruct 8B,307', '70B (Dubey et al., 2024).308', 'Tasks.Weconductcomprehensiveevalua-309', 'tions on various generation tasks.MT-Bench310', '(Zheng et al., 2023) for multi-turn conversation;311', 'GSM8K(Cobbe et al., 2021) for mathemati-312', 'cal reasoning; HumanEval(Chen et al., 2021),313', 'MBPP(Austin et al., 2021) and ClassEval (Du314', 'et al., 2023) for code generation.315', 'Metrics.SPECTRA does not modify the original316', 'LLM and the acceptance conditions, making it a317', 'lossless acceleration method. Therefore, the gener-318', 'ation quality remains the same as the original LLM.319', 'We only evaluate the acceleration performance us-320', 'ing the following metrics.321', '• Speedup Ratio: The speedup ratio relative to322', 'autoregressive decoding.323', '• Compression ratio: The ratio of the total324', 'number of autoregressive steps to the number325', 'of Spectra decoding steps needed to produce326', 'the same sequence length.327', 'Baselines.We use standard autoregressive decod-328', 'ing as the baseline (speed-up ratio = 1.00x). We fur-329', 'ther compare SPECTRA with leading non-training330', 'speculative decoding approaches, namely Adaptive331', 'N-gram (Ou et al., 2024), REST (He et al., 2024),332', 'and Lookahead (Fu et al., 2024). For details regard-333', 'ing implementation settings of both SPECTRA and334', 'these baselines, please refer to Appendix B.335', '5Results336', '5.1Main Results337', 'Overall Performance.The top portion of Table 1338', 'presents the speedup ratios of all evaluated meth-339', 'ods under a greedy decoding setup. Our approach,340', 'SPECTRA, consistently yields the highest accelera-341', 'tion across the entire range of datasets and LLMs.342', 'In particular, SPECTRA achieves speedups up to343', '4.08× with LLama-3-8B-Instruct on the MBPP344', 'dataset.345', 'For smaller models (7B), SPECTRA often sur-346', 'passes 3× acceleration, underscoring the effective-347', 'ness of multi-token compression. By contrast, for348', '13B models, while the boost remains strong, it is349', 'relatively more moderate, typically falling in the350', '1.6×–3× band. We attribute this trend to the in-351', 'creased overhead of each forward pass in larger net-352', 'works, which can dampen the proportional gains of353', 'fewer decoding iterations per token. Despite this,354', 'SPECTRA continues to outperform baselines across355', 'all parameter settings.356', 'Significant advantages are evident in tasks such357', 'as GSM8K and ClassEval, where outputs often358', 'follow recurring patterns (e.g., repeated variable359', 'names or class definitions). In these scenarios,360', 'SPECTRA combines internal knowledge of par-361', 'tial sequences with external retrieval suggestions,362', 'thereby proposing accurate multi-token guesses.363', 'On the other hand, in domains featuring more var-364', 'ied or unpredictable responses—such as complex365', 'multi-turn conversations in MT-Bench—the accep-366', 'tance rate is somewhat lower, although still com-367', 'petitive.368', 'Compression Ratio.Table 1 also reports each369', 'method’s compression rate, a measure agnostic370', 'to specific hardware configurations. Across ev-371', 'ery dataset and LLM tested, SPECTRA delivers the372', 'highest average compression ratio. Each of SPEC-373', 'TRA’s draft-and-verify iterations typically yields374', '5', 'ClassevalGSM8KHumanevalMBPPMTBenchAVGModelMethodspeedupτspeedupτspeedupτspeedupτspeedupτspeedup', 'Greedy (temperature=0)', 'CL-13BANPD1.942.522.813.722.082.502.713.582.613.412.43 CL-13BLookahead2.253.612.804.242.303.162.914.442.594.042.57 CL-13BREST1.282.140.931.541.582.310.851.400.941.531.12 CL-13BSPECTRA (Ours)2.384.062.914.652.633.953.294.462.654.402.77', 'CL-7BANPD2.302.683.213.752.162.473.163.783.353.832.84 CL-7BLookahead2.593.662.993.832.503.052.903.673.234.272.84 CL-7BREST1.452.220.911.391.702.340.961.451.021.441.21 CL-7BSPECTRA (Ours)2.704.103.334.592.963.903.564.453.704.523.25', 'L2-13BANPD1.361.781.471.721.341.611.121.321.171.371.29 L2-13BLookahead1.812.761.461.871.732.321.381.691.512.041.58 L2-13BREST1.222.010.941.461.251.940.951.441.141.901.10 L2-13BSPECTRA (Ours)2.003.241.832.621.962.911.632.241.752.601.83', 'L2-70BANPD1.821.901.631.611.861.871.171.201.341.301.56 L2-70BLookahead2.652.871.862.022.572.671.491.541.942.002.10 L2-70BSPECTRA (Ours)3.103.402.522.693.223.371.861.932.432.512.62', 'L2-7BANPD1.621.951.521.681.541.671.191.331.301.371.43 L2-7BLookahead2.192.941.661.932.062.421.461.691.732.051.82 L2-7BREST1.362.121.011.471.412.041.011.461.251.901.21 L2-7BSPECTRA (Ours)2.403.432.112.642.403.051.772.162.022.592.14', 'L3-70BANPD1.541.671.501.471.831.881.461.411.231.231.51 L3-70BLookahead2.402.621.541.582.562.701.431.451.761.861.94 L3-70BSPECTRA (Ours)2.672.912.102.142.843.021.941.942.062.132.32', 'L3-8BANPD2.112.493.864.571.832.093.363.581.141.232.46 L3-8BLookahead2.593.443.714.612.492.893.794.651.531.852.82 L3-8BSPECTRA (Ours)2.833.493.894.772.573.024.084.761.692.103.01', 'Sampling (temperature=1.0)', 'CL-13BANPD1.151.461.071.311.051.301.001.242.312.891.31 CL-13BLookahead1.382.001.081.431.291.751.021.342.333.481.42 CL-13BREST1.141.870.821.351.271.960.841.390.931.501.00 CL-13BSPECTRA (Ours)1.682.221.201.751.652.121.151.702.373.801.61', 'CL-7BANPD1.291.501.161.301.101.321.121.272.773.051.49 CL-7BLookahead1.542.031.191.411.431.811.191.432.723.501.61 CL-7BREST1.231.860.881.331.331.980.911.400.971.441.06 CL-7BSPECTRA (Ours)1.812.251.351.731.682.121.331.722.783.941.79', 'L2-13BANPD1.201.521.241.461.171.401.031.221.171.351.16 L2-13BLookahead1.522.221.321.691.482.001.181.481.492.011.40 L2-13BREST1.181.960.931.451.191.880.921.441.121.881.07 L2-13BSPECTRA (Ours)1.702.751.552.231.692.591.341.891.742.571.60', 'L2-7BANPD1.311.511.341.481.281.461.101.221.251.361.26 L2-7BLookahead1.782.301.511.761.722.091.251.491.682.021.59 L2-7BREST1.262.030.991.461.271.930.961.411.211.881.14 L2-7BSPECTRA (Ours)1.972.831.782.282.042.751.471.841.972.541.85', 'L3-8BANPD1.251.371.972.181.431.651.892.071.151.211.54 L3-8BLookahead1.481.782.072.411.792.211.992.401.571.811.78 L3-8BSPECTRA (Ours)1.942.842.272.781.922.512.192.781.702.052.01', 'Table 1: Overall performance of speculative decoding methods across multiple tasks. “CL-xB” denotes CodeLlamawith xB parameters, “L2-xB” denotes LLaMA-2-Chat of size xB, and “L3-xB” denotes LLaMA-3-Instruct of size xB. We report the speedup ratio (vs. autoregressive) and the compression ratio τ.', '2.1–4.8 tokens, substantially outpacing alternative375', 'approaches and nearly doubling the acceptance376', 'length achieved by REST.377', 'Acceleration in Sampling Decoding.The lower378', 'section of Table 1 investigates the performance379', 'of SPECTRA under sampling-based decoding with380', 'a temperature of 1.0. The results highlight how381', '6', 'SPECTRA continues to accelerate generation rel-382', 'ative to baselines, offering roughly 1.15–2.77×383', 'speedups over standard autoregressive decoding.384', 'These gains are more modest than in greedy decod-385', 'ing, reflecting the lower acceptance rate under the386', 'sampling-based verification phase, which is consis-387', 'tent with earlier findings (Fu et al., 2024; Leviathan388', 'et al., 2023).389', '5.2Analysis390', 'Ablationstudy.Weconductedadetailed391', 'component-wise analysis to determine the contri-392', 'bution of each module to the framework’s over-393', 'all performance (Table 2). Specifically, the re-394', 'sults on LLaMA2-7B-chat reveal that removing395', 'different components yields varying impacts on396', 'GSM8K speedups.Under the “CORE Module”397', 'configuration, excluding multi-level n-grams low-398', 'ers the speedup from 2.04× to 1.95× (a 4% de-399', 'crease), whereas turning off forward information400', 'reduces it from 2.04× to 1.50× (a 26% drop). Sim-401', 'ilarly, omitting backward information results in402', 'a speedup of 1.94×, down from 2.04×. In con-403', 'trast, the “RETRIEVAL Module” setting shows that404', 'leaving out perplexity-based filtering decreases the405', 'speedup from 1.18× to 1.16×. Our fully integrated406', 'approach, SPECTRA, achieves a 2.14× speedup407', 'on GSM8K—outperforming both the “CORE Mod-408', 'ule” (2.04×) and “RETRIEVAL Module” (1.18×)409', 'variants. This improvement demonstrates the im-410', 'portance of combining multi-level n-grams, for-411', 'ward/backward drafting, and perplexity-based re-412', 'finement in boosting acceptance rates and enhanc-413', 'ing overall speedups. A similar trend was also414', 'observed in the results of the MTBench dataset.415', 'Additionally,wecomparedourmethod416', 'against a naive combination of Lookahead and417', 'REST—where guess sequences from REST are418', 'added to Lookahead. This combined approach419', 'falls significantly short of our SPECTRA method,420', 'highlighting that a simple merger of two techniques421', 'is insufficient without our carefully optimized422', 'integration strategy and components.423', 'Priority for source of guessesSince verifying424', 'too many candidate tokens at once can strain GPU425', 'resources and reduce speedups (Fu et al., 2024; Li426', 'et al., 2024b), SPECTRA limits how many guesses427', 'proceed to verification in each step (Appendix B).428', 'To understand whether internal or external guesses429', 'are more valuable, we temporarily remove this430', 'cap and measure acceptance rates (Figure 3). We431', 'GSM8KMTBenchMethodspeedupτspeedupτ', 'REST1.011.471.251.90', 'Lookahead1.661.931.732.05', 'Lookahead + REST1.081.471.271.90', 'SPECTRA’s ablation', 'CORE Module2.042.501.922.35 - w/o forward info1.501.681.201.37 - w/o backward info1.942.211.742.12 - w/o Sub-Ngram1.952.341.752.18', 'RETRIEVAL Module1.181.311.241.50 - w/o PPL refine1.161.291.201.45', 'SPECTRA (ours)2.142.642.022.59', 'Table 2: Ablation of SPECTRA’s components (greedydecoding, LLaMA2-7B-Chat).“Sub-Ngram” aug-ments each n-gram with its sub-sequences; “for-ward/backward info” uses internal expansions; and“PPL refine” applies perplexity-based filtering for ex-ternal retrieval. “Lookahead + REST” denotes a naivecombination where guess sequences from REST are di-rectly added to Lookahead', 'observe that sequences generated via internal ex-432', 'pansions—particularly forward and backward pre-433', 'dictions—have a higher acceptance probability434', 'than those retrieved from external sources. Con-435', 'sequently, SPECTRA prioritizes internal guesses436', 'for verification. Interestingly, in code-generation437', 'tasks like HumanEval, external suggestions be-438', 'come more influential, likely due to code’s repeti-439', 'tive structure and the retrieval of similar snippets.440', 'This observation indicates that a strategic blend441', 'of backward internal knowledge and external re-442', 'trieval can be particularly fruitful in these domains,443', 'especially when computational resources limit ex-444', 'tensive forward expansions.445', 'FlashAttention.Figure 3 shows that enabling446', 'FlashAttention consistently boosts the speedup of447', 'all methods, albeit to varying degrees. Notably,448', 'we observe an additional 0.24× speedup gain for449', 'SPECTRA on both GSM8K and MTBench. This450', 'is because FlashAttention better exploits the paral-451', 'lel structure of speculative decoding by reducing452', 'attention overheads, especially when verifying mul-453', 'tiple guessed tokens in parallel. Although smaller454', 'gains are also seen for other methods, SPECTRA455', 'benefits the most, as it presents the longest verifica-456', 'tion branches and thus stands to profit significantly457', 'from more efficient attention implementations.458', '7', '020406080100', 'Total accept tokens (%)', 'ClassEval', 'GSM8K', 'HumanEval', 'MBPP', 'MT-Bench', 'Internal-FwdInternal-BwdExternal', 'Figure 3: Acceptance rates for different guess sources(e.g., SPECTRA-CORE forward dictionary, backwarddictionary, SPECTRA-RETRIEVAL’s guesses). The ac-ceptance rate is the fraction of guessed tokens that passverification and are appended to the final output.', '0', '1', '2', 'Speedup', '1.00x', '1.68x1.55x', '1.03x', '2.08x', '1.07x', '1.86x1.70x', '1.09x', '2.32x', 'GSM8K', 'W/o flashWith flash', 'Autoreg.LookaheadANPDRESTSpectra', '0', '1', '2', 'Speedup', '1.00x', '1.75x', '1.31x1.24x', '2.01x', '1.10x', '1.96x', '1.42x1.38x', '2.25x', 'MTBench', 'W/o flashWith flash', 'Figure 4: Effect of FlashAttention on speculative de-coding speed: Measured speedups on GSM8K andMTBench (LLama2-7B-Chat, greedy decoding). “NoFlash” uses standard attention; “With Flash” usesFlashAttention for faster parallel verification.', '6Related Works459', 'Large language models (LLMs) are increasingly460', 'deployed in a range of applications, motivating on-461', 'going research into more efficient inference (Liu462', 'et al., 2025). Common strategies include quan-463', 'tizing model weights into lower-precision formats464', '(Liu et al., 2024b; Lin et al., 2024; Zhao et al., 2024;465', 'Park et al., 2024), pruning redundant parameters466', '(Ma et al., 2023; Xia et al., 2023; Sun et al., 2023a;467', 'Le et al., 2025), and employing knowledge distilla-468', 'tion (Gu et al., 2024; Friha et al., 2024; Zhang et al.,469', '2024b). These techniques help reduce the compu-470', 'tational load per forward pass, thereby lowering471', 'generation latency. However, they often introduce472', 'some degradation in model performance, forcing473', 'practitioners to balance quality with efficiency.474', 'A growing line of work explores speculative de-475', 'coding as a strategy for accelerating generation476', 'while maintaining the exact output distribution477', '(Chen et al., 2023; Leviathan et al., 2023). Some478', 'speculative decoding approaches train a smaller479', 'LLM (referred to as a draft model) (Chen et al.,480', '2023; Leviathan et al., 2023; Miao et al., 2024; Sun481', 'et al., 2023b; Zhou et al., 2024; Cai et al., 2024),482', 'or train the original LLM itself in a special man-483', 'ner (referred to as self-speculative) (Elhoushi et al.,484', '2024; Liu et al., 2024a; Yang et al., 2024; Zhang485', 'et al., 2024a; Li et al., 2024b) to guess several sub-486', 'sequent tokens and then verify them parallelly us-487', 'ing the original LLM. As these approaches require488', 'training, they pose limitations, such as requiring489', 'heavy computational resources and losing the orig-490', 'inal model capabilities.491', 'To avoid additional training, alternative specula-492', 'tive decoding methods leverage external resources493', 'or structural properties of language generation.494', 'Retrieval-based methods sidestep draft model train-495', 'ing by using a datastore indexed with observed496', 'prefixes to retrieve guess sequences (Yang et al.,497', '2023; He et al., 2024; Li et al., 2024a). Other498', 'approaches, such as Jacobi-like parallel decoding499', '(Santilli et al., 2023) and lookahead decoding (Fu500', 'et al., 2024), mitigate left-to-right dependencies by501', 'generating and validating multiple candidate tokens502', 'in parallel. These training-free techniques achieve503', 'comparable speedups to learned methods without504', 'requiring model optimization, making them ideal505', 'for scenarios with computational or deployment506', 'constraints.507', '7Conclusions508', 'In this work, we introduced SPECTRA, a hybrid509', 'speculative decoding framework that combines510', 'multi-level n-grams (internal knowledge) with511', 'perplexity-based retrieval (external knowledge) to512', 'achieve speedups of up to 4.08× across various513', 'LLMs and benchmarks, without additional train-514', 'ing or compromising exact output fidelity. Our515', 'ablation studies show that each module (multi-516', 'level n-grams, forward/backward expansions, and517', 'perplexity-based datastore curation) substantially518', 'boosts acceptance rates, and their synergy outper-519', 'forms existing non-training methods. By offering a520', 'lossless speedup that efficiently exploits both inter-521', 'nal patterns and external texts, SPECTRA provides522', 'a practical, high-impact solution for accelerating523', 'inference in large language models.524', '8', '8Limitations525', '(1) Cost of Building External Datastores.Al-526', 'though our internal-knowledge strategy only relies527', 'on sequences observed during generation (and thus528', 'requires no extra data), our external-knowledge529', 'approach depends on constructing and indexing a530', 'sizeable datastore from potentially large corpora.531', 'This process can be time-consuming and memory-532', 'intensive, particularly in domains where data up-533', 'dates frequently or storage is constrained. While534', 'this additional investment can yield substantial535', 'speedups by increasing token acceptance rates, it536', 'may not be universally feasible or cost-effective.537', '(2) Limited Evaluation Scope.Our experiments538', 'center primarily on English-language benchmarks539', 'in conversational and coding tasks using LLaMA-540', 'based models. Although SPECTRA can, in princi-541', 'ple, be applied to other models or languages, addi-542', 'tional factors such as domain-specific tokenization543', 'or specialized textual structures may affect the ac-544', 'ceptance rate and overall speedup. Future work is545', 'needed to assess the generality of SPECTRA across546', 'diverse linguistic settings (e.g., low-resource lan-547', 'guages or specialized technical documents) and for548', 'a wider range of model families (beyond LLaMA-549', 'based architectures) to confirm and refine its appli-550', 'cability.551', 'References552', 'Jacob Austin, Augustus Odena, Maxwell Nye, Maarten553', 'Bosma, Henryk Michalewski, David Dohan, Ellen554', 'Jiang, Carrie Cai, Michael Terry, Quoc Le, and others.555', '2021. Program synthesis with large language models.556', 'arXiv preprint arXiv:2108.07732.557', 'Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,558', 'Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei559', 'Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,560', 'Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,561', 'Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,562', 'Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong563', 'Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-564', 'guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,565', 'Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,566', 'Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-567', 'uan Zhang, Yichang Zhang, Zhenru Zhang, Chang568', 'Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang569', 'Zhu. 2023. Qwen Technical Report.570', 'Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu571', 'Peng, Jason D. Lee, Deming Chen, and Tri Dao.572', '2024. MEDUSA: Simple LLM inference acceler-573', 'ation framework with multiple decoding heads. In574', 'Proceedings of the 41st International Conference on575', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-576', 'enna, Austria.577', 'Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,578', 'Jean-Baptiste Lespiau, Laurent Sifre, and John579', 'Jumper. 2023. Accelerating Large Language Model580', 'Decoding with Speculative Sampling.581', 'Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,582', 'Henrique Ponde De Oliveira Pinto, Jared Kaplan,583', 'Harri Edwards, Yuri Burda, Nicholas Joseph, Greg584', 'Brockman, and others. 2021.Evaluating large585', 'language models trained on code. arXiv preprint586', 'arXiv:2107.03374.587', 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,588', 'Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias589', 'Plappert, Jerry Tworek, Jacob Hilton, Reiichiro590', 'Nakano, and others. 2021.Training verifiers591', 'to solve math word problems.arXiv preprint592', 'arXiv:2110.14168.593', 'Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin,594', 'Shengding Hu, Zhiyuan Liu, Maosong Sun, and595', 'Bowen Zhou. 2023. Enhancing Chat Language Mod-596', 'els by Scaling High-quality Instructional Conversa-597', 'tions. In Proceedings of the 2023 Conference on598', 'Empirical Methods in Natural Language Processing,599', 'pages 3029–3051, Singapore. Association for Com-600', 'putational Linguistics.601', 'Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang,602', 'Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng603', 'Sha, Xin Peng, and Yiling Lou. 2023.Classe-604', 'val: A manually-crafted benchmark for evaluating605', 'llms on class-level code generation. arXiv preprint606', 'arXiv:2308.01861.607', 'Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,608', 'Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,609', 'Akhil Mathur, Alan Schelten, Amy Yang, Angela610', 'Fan, et al. 2024. The llama 3 herd of models. arXiv611', 'preprint arXiv:2407.21783.612', 'Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,613', 'Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas614', 'Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed615', 'Roman, Ahmed Aly, Beidi Chen, and Carole-Jean616', 'Wu. 2024. LayerSkip: Enabling Early Exit Infer-617', 'ence and Self-Speculative Decoding. In Proceedings618', 'of the 62nd Annual Meeting of the Association for619', 'Computational Linguistics (Volume 1: Long Papers),620', 'pages 12622–12642, Bangkok, Thailand. Association621', 'for Computational Linguistics.622', 'Othmane Friha, Mohamed Amine Ferrag, Burak623', 'Kantarci, Burak Cakmak, Arda Ozgun, and Nassira624', 'Ghoualmi-Zine. 2024. Llm-based edge intelligence:625', 'A comprehensive survey on architectures, applica-626', 'tions, security and trustworthiness. IEEE Open Jour-627', 'nal of the Communications Society.628', 'Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.629', '2024.Break the sequential dependency of LLM630', 'inference using LOOKAHEAD DECODING. In631', 'Proceedings of the 41st International Conference on632', '9', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-633', 'enna, Austria.634', 'Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024.635', 'Minillm: Knowledge distillation of large language636', 'models. In The Twelfth International Conference on637', 'Learning Representations.638', 'Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and639', 'Di He. 2024. REST: Retrieval-Based Speculative640', 'Decoding. In Proceedings of the 2024 Conference of641', 'the North American Chapter of the Association for642', 'Computational Linguistics: Human Language Tech-643', 'nologies (Volume 1: Long Papers), pages 1582–1595,644', 'Mexico City, Mexico. Association for Computational645', 'Linguistics.646', 'Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and647', 'Yejin Choi. 2020. The Curious Case of Neural Text648', 'Degeneration. In International Conference on Learn-649', 'ing Representations.650', 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-651', 'sch, Chris Bamford, Devendra Singh Chaplot, Diego652', 'de las Casas, Florian Bressand, Gianna Lengyel, Guil-653', 'laume Lample, Lucile Saulnier, Lélio Renard Lavaud,654', 'Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,655', 'Thibaut Lavril, Thomas Wang, Timothée Lacroix,656', 'and William El Sayed. 2023. Mistral 7B.657', 'Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI,658', 'Chenghao Mou, Yacine Jernite, Margaret Mitchell,659', 'Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf,660', 'Dzmitry Bahdanau, Leandro Von Werra, and Harm de661', 'Vries. 2023. The Stack: 3 TB of permissively li-662', 'censed source code. Transactions on Machine Learn-663', 'ing Research.664', 'Wouter Kool, Herke van Hoof, and Max Welling. 2020.665', 'Ancestral Gumbel-Top-k Sampling for Sampling666', 'Without Replacement. Journal of Machine Learning667', 'Research, 21(47):1–36.668', 'Khang Nguyen Le, Ryo Sato, Dai Nakashima, Takeshi669', 'Suzuki, and Minh Le Nguyen. 2025. Optiprune: Ef-670', 'fective pruning approach for every target sparsity. In671', 'Proceedings of the 31st International Conference on672', 'Computational Linguistics, pages 3600–3612.673', 'Yaniv Leviathan, Matan Kalman, and Yossi Matias.674', '2023. Fast inference from transformers via spec-675', 'ulative decoding. In Proceedings of the 40th Interna-676', 'tional Conference on Machine Learning, ICML’23.677', 'JMLR.org. Place: Honolulu, Hawaii, USA.678', 'Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen,679', 'Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024a.680', 'Nearest Neighbor Speculative Decoding for LLM681', 'Generation and Attribution. In The Thirty-eighth An-682', 'nual Conference on Neural Information Processing683', 'Systems.684', 'Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang685', 'Zhang. 2024b. EAGLE-2: Faster Inference of Lan-686', 'guage Models with Dynamic Draft Trees. In Proceed-687', 'ings of the 2024 Conference on Empirical Methods688', 'in Natural Language Processing, pages 7421–7432,689', 'Miami, Florida, USA. Association for Computational690', 'Linguistics.691', 'Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-692', 'Ming Chen, Wei-Chen Wang, Guangxuan Xiao,693', 'Xingyu Dang, Chuang Gan, and Song Han. 2024.694', 'Awq: Activation-aware weight quantization for on-695', 'device llm compression and acceleration. Proceed-696', 'ings of Machine Learning and Systems, 6:87–100.697', 'Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng698', 'Ni, Duyu Tang, Kai Han, and Yunhe Wang. 2024a.699', 'Kangaroo: Lossless Self-Speculative Decoding for700', 'Accelerating LLMs via Double Early Exiting. In The701', 'Thirty-eighth Annual Conference on Neural Informa-702', 'tion Processing Systems.703', 'Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan704', 'Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xi-705', 'aohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu,706', 'Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang,707', 'Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu,708', 'and Bao Ge. 2025. Understanding llms: A compre-709', 'hensive overview from training to inference. Neuro-710', 'computing, 620:129190.711', 'Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie712', 'Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,713', 'Raghuraman Krishnamoorthi, and Vikas Chandra.714', '2024b. LLM-QAT: Data-free quantization aware715', 'training for large language models. In Findings of716', 'the Association for Computational Linguistics: ACL717', '2024, pages 467–484, Bangkok, Thailand. Associa-718', 'tion for Computational Linguistics.719', 'Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.720', 'Llm-pruner: On the structural pruning of large lan-721', 'guage models. Advances in neural information pro-722', 'cessing systems, 36:21702–21720.723', 'Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao724', 'Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee725', 'Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan726', 'Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Ab-727', 'hyankar, and Zhihao Jia. 2024. SpecInfer: Accelerat-728', 'ing Large Language Model Serving with Tree-based729', 'Speculative Inference and Verification. In Proceed-730', 'ings of the 29th ACM International Conference on Ar-731', 'chitectural Support for Programming Languages and732', 'Operating Systems, Volume 3, ASPLOS ’24, pages733', '932–949, New York, NY, USA. Association for Com-734', 'puting Machinery. Event-place: La Jolla, CA, USA.735', 'Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,736', 'Ça˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-737', 'tive text summarization using sequence-to-sequence738', 'RNNs and beyond.In Proceedings of the 20th739', 'SIGNLL Conference on Computational Natural Lan-740', 'guage Learning, pages 280–290, Berlin, Germany.741', 'Association for Computational Linguistics.742', 'Shashi Narayan, Shay B Cohen, and Mirella Lap-743', 'ata. 2018.Don’t give me the details, just the744', '10', 'summary!topic-aware convolutional neural net-745', 'works for extreme summarization. arXiv preprint746', 'arXiv:1808.08745.747', 'OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,748', 'Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-749', 'man, Diogo Almeida, Janko Altenschmidt, Sam Alt-750', 'man, Shyamal Anadkat, Red Avila, Igor Babuschkin,751', 'Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-752', 'ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-753', 'wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,754', 'Christopher Berner, Lenny Bogdonoff, Oleg Boiko,755', 'Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-756', 'man, Tim Brooks, Miles Brundage, Kevin Button,757', 'Trevor Cai, Rosie Campbell, Andrew Cann, Brittany758', 'Carey, Chelsea Carlson, Rory Carmichael, Brooke759', 'Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully760', 'Chen, Ruby Chen, Jason Chen, Mark Chen, Ben761', 'Chess, Chester Cho, Casey Chu, Hyung Won Chung,762', 'Dave Cummings, Jeremiah Currier, Yunxing Dai,763', 'Cory Decareaux, Thomas Degry, Noah Deutsch,764', 'Damien Deville, Arka Dhar, David Dohan, Steve765', 'Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,766', 'Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,767', 'Simón Posada Fishman, Juston Forte, Isabella Ful-768', 'ford, Leo Gao, Elie Georges, Christian Gibson, Vik769', 'Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-770', 'Lopes, Jonathan Gordon, Morgan Grafstein, Scott771', 'Gray, Ryan Greene, Joshua Gross, Shixiang Shane772', 'Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,773', 'Yuchen He, Mike Heaton, Johannes Heidecke, Chris774', 'Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,775', 'Brandon Houghton, Kenny Hsu, Shengli Hu, Xin776', 'Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,777', 'Joanne Jang, Angela Jiang, Roger Jiang, Haozhun778', 'Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-779', 'woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-780', 'mali, Ingmar Kanitscheider, Nitish Shirish Keskar,781', 'Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,782', 'Christina Kim, Yongjik Kim, Jan Hendrik Kirch-783', 'ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,784', 'Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-785', 'stantinidis, Kyle Kosic, Gretchen Krueger, Vishal786', 'Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan787', 'Leike, Jade Leung, Daniel Levy, Chak Ming Li,788', 'Rachel Lim, Molly Lin, Stephanie Lin, Mateusz789', 'Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,790', 'Anna Makanju, Kim Malfacini, Sam Manning, Todor791', 'Markov, Yaniv Markovski, Bianca Martin, Katie792', 'Mayer, Andrew Mayne, Bob McGrew, Scott Mayer793', 'McKinney, Christine McLeavey, Paul McMillan,794', 'Jake McNeil, David Medina, Aalok Mehta, Jacob795', 'Menick, Luke Metz, Andrey Mishchenko, Pamela796', 'Mishkin, Vinnie Monaco, Evan Morikawa, Daniel797', 'Mossing, Tong Mu, Mira Murati, Oleg Murk, David798', 'Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,799', 'Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,800', 'Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex801', 'Paino, Joe Palermo, Ashley Pantuliano, Giambat-802', 'tista Parascandolo, Joel Parish, Emy Parparita, Alex803', 'Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-804', 'man, Filipe de Avila Belbute Peres, Michael Petrov,805', 'Henrique Ponde de Oliveira Pinto, Michael, Poko-806', 'rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-807', 'ell, Alethea Power, Boris Power, Elizabeth Proehl,808', 'Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,809', 'Cameron Raymond, Francis Real, Kendra Rimbach,810', 'Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-811', 'der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,812', 'Girish Sastry, Heather Schmidt, David Schnurr, John813', 'Schulman, Daniel Selsam, Kyla Sheppard, Toki814', 'Sherbakov, Jessica Shieh, Sarah Shoker, Pranav815', 'Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,816', 'Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin817', 'Sokolowsky, Yang Song, Natalie Staudacher, Fe-818', 'lipe Petroski Such, Natalie Summers, Ilya Sutskever,819', 'Jie Tang, Nikolas Tezak, Madeleine B. Thompson,820', 'Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,821', 'Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-822', 'lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,823', 'Chelsea Voss, Carroll Wainwright, Justin Jay Wang,824', 'Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,825', 'C. J. Weinmann, Akila Welihinda, Peter Welin-826', 'der, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave827', 'Willner, Clemens Winter, Samuel Wolrich, Hannah828', 'Wong, Lauren Workman, Sherwin Wu, Jeff Wu,829', 'Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin830', 'Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers,831', 'Chong Zhang, Marvin Zhang, Shengjia Zhao, Tian-832', 'hao Zheng, Juntang Zhuang, William Zhuk, and Bar-833', 'ret Zoph. 2024. GPT-4 Technical Report.834', 'Jie Ou, Yueming Chen, and Prof. Tian. 2024. Lossless835', 'Acceleration of Large Language Model via Adap-836', 'tive N-gram Parallel Decoding. In Proceedings of837', 'the 2024 Conference of the North American Chap-838', 'ter of the Association for Computational Linguistics:839', 'Human Language Technologies (Volume 6: Industry840', 'Track), pages 10–22, Mexico City, Mexico. Associa-841', 'tion for Computational Linguistics.842', 'Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun843', 'Sim, and Jae W. Lee. 2024. Any-precision llm: Low-844', 'cost deployment of multiple, different-sized llms. In845', 'Proceedings of the 41st International Conference on846', 'Machine Learning.847', 'Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten848', 'Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,849', 'Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy850', 'Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna851', 'Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron852', 'Grattafiori, Wenhan Xiong, Alexandre Défossez,853', 'Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-854', 'tin, Nicolas Usunier, Thomas Scialom, and Gabriel855', 'Synnaeve. 2024. Code Llama: Open Foundation856', 'Models for Code. _eprint: 2308.12950.857', 'Andrea Santilli, Silvio Severino, Emilian Postolache,858', 'Valentino Maiorca, Michele Mancusi, Riccardo859', 'Marin, and Emanuele Rodola. 2023. Accelerating860', 'transformer inference for translation via parallel de-861', 'coding. In Proceedings of the 61st Annual Meeting of862', 'the Association for Computational Linguistics (Vol-863', 'ume 1: Long Papers), pages 12336–12355, Toronto,864', 'Canada. Association for Computational Linguistics.865', 'Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.866', '11', '2023a. A simple and effective pruning approach for867', 'large language models. ArXiv, abs/2306.11695.868', 'Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-869', 'mad Beirami, Himanshu Jain, and Felix Yu. 2023b.870', 'SpecTr: fast speculative decoding via optimal trans-871', 'port. In Proceedings of the 37th International Con-872', 'ference on Neural Information Processing Systems,873', 'NIPS ’23, Red Hook, NY, USA. Curran Associates874', 'Inc. Event-place: New Orleans, LA, USA.875', 'Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-876', 'bert, Amjad Almahairi, Yasmine Babaei, Nikolay877', 'Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti878', 'Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton879', 'Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,880', 'Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,881', 'Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-882', 'thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan883', 'Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,884', 'Isabel Kloumann, Artem Korenev, Punit Singh Koura,885', 'Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-886', 'ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-887', 'tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-888', 'bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-889', 'stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,890', 'Ruan Silva, Eric Michael Smith, Ranjan Subrama-891', 'nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-892', 'lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,893', 'Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,894', 'Melanie Kambadur, Sharan Narang, Aurelien Ro-895', 'driguez, Robert Stojnic, Sergey Edunov, and Thomas896', 'Scialom. 2023. Llama 2: Open Foundation and Fine-897', 'Tuned Chat Models.898', 'Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi899', 'Chen. 2023. Sheared llama: Accelerating language900', 'model pre-training via structured pruning. ArXiv,901', 'abs/2310.06694.902', 'Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin903', 'Jiang, Linjun Yang, Rangan Majumder, and Furu904', 'Wei. 2023.Inference with Reference: Lossless905', 'Acceleration of Large Language Models. _eprint:906', '2304.04487.907', 'Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris908', 'Papailiopoulos, and Kangwook Lee. 2024. Predictive909', 'Pipelined Decoding: A Compute-Latency Trade-off910', 'for Exact LLM Decoding. Transactions on Machine911', 'Learning Research.912', 'Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,913', 'Gang Chen, and Sharad Mehrotra. 2024a. Draft&914', 'Verify: Lossless Large Language Model Acceleration915', 'via Self-Speculative Decoding. In Proceedings of the916', '62nd Annual Meeting of the Association for Compu-917', 'tational Linguistics (Volume 1: Long Papers), pages918', '11263–11282, Bangkok, Thailand. Association for919', 'Computational Linguistics.920', 'Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng921', 'Chen, and Jinan Xu. 2024b. Dual-space knowledge922', 'distillation for large language models. In Proceed-923', 'ings of the 2024 Conference on Empirical Methods in924', 'Natural Language Processing, pages 18164–18181,925', 'Miami, Florida, USA. Association for Computational926', 'Linguistics.927', 'Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn928', 'Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,929', 'Tianqi Chen, and Baris Kasikci. 2024. Atom: Low-930', 'bit quantization for efficient and accurate llm serv-931', 'ing. Proceedings of Machine Learning and Systems,932', '6:196–209.933', 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan934', 'Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,935', 'Zhuohan Li, Dacheng Li, Eric Xing, and others. 2023.936', 'Judging llm-as-a-judge with mt-bench and chatbot937', 'arena. Advances in Neural Information Processing938', 'Systems, 36:46595–46623.939', 'Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,940', 'Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv941', 'Kumar, Jean-François Kagy, and Rishabh Agarwal.942', '2024. DistillSpec: Improving Speculative Decoding943', 'via Knowledge Distillation. In The Twelfth Interna-944', 'tional Conference on Learning Representations.945', 'AMore on Speculative Decoding946', 'Autoregressive decoding (Touvron et al., 2023; Bai947', 'et al., 2023; Jiang et al., 2023; OpenAI et al., 2024),948', 'suffers from inefficiency because it generates text949', 'one token at a time (Figure 5, Left).Specula-950', 'tive decoding (Chen et al., 2023; Leviathan et al.,951', '2023) follows a guess-and-verify paradigm (Figure952', '5, Right). In speculative decoding, a smaller LLM953', '(draft model) (Chen et al., 2023; Leviathan et al.,954', '2023; Miao et al., 2024; Sun et al., 2023b; Zhou955', 'et al., 2024; Cai et al., 2024) or the original LLM956', 'trained in a specialized manner (self-speculative957', 'decoding) (Elhoushi et al., 2024; Liu et al., 2024a;958', 'Yang et al., 2024; Zhang et al., 2024a; Li et al.,959', '2024b) predicts multiple tokens in advance. The960', 'original LLM then verifies these predictions in par-961', 'allel, improving efficiency.962', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'Autoregressive Decoding', '...', 'Speculate ', '(make guesses)', 'Speculative Decoding', 'A', 'B', 'B', 'C', 'C', 'D', 'T', 'Target LLM', 'AAcceptReject', 'Figure 5: Examples of Autoregressive decoding (Left)and Speculative Decoding (Right). While autoregres-sive decoding generates one token per forward step,speculative decoding generates three tokens with oneforward step.', '12', 'LLMs process discrete integer sequences as in-963', 'puts, where each integer represents a token. We de-964', 'fine the input sequence as x = (x1, x2, . . . , xs) ∈965', 'Ns of length s, and denote a slice of length m at966', 'step t as x1:m = (x1, x2, . . . , xm). The output of967', 'an LLM represents the probability distribution over968', 'the next token. The probability of generating the969', 's-th token, conditioned on all preceding tokens, is970', 'given by PM(xs | x1:s−1). The next token xs is971', 'then sampled from this distribution using various972', 'methods (e.g., greedy, top-k, and top-p sampling;973', 'see (Kool et al., 2020; Holtzman et al., 2020)). In974', 'the case of greedy sampling, the next token is se-975', 'lected as xs = arg max PM(xs | x1:s−1)976', 'Let x0 be the prompt tokens provided by the977', 'user. The LLM generates an output sequence of978', 'length m, with each generated token yi computed979', 'autoregressively. Assuming greedy sampling, the980', 'decoding process follows:981', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y1 = arg max PM(y1 | x) ', 'y2 = arg max PM(y2 | y1, x)... ', 'ym = arg max PM(ym | y1:m−1, x).', '(1)982', 'A.1Speculative Decoding983', 'Speculative decoding follows a Guess-And-Verify984', 'approach, where multiple candidate future to-985', 'kens are speculated and subsequently verified986', 'in a single decoding step.With tree attention987', '(Miao et al., 2024), multiple drafts can be ver-988', 'ified simultaneously. Let G denote the number989', 'of guesses, and define the set of guesses as ˜Y =990', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence991', 'has length K. The j-th token of the i-th guess is992', 'denoted as ˜y(i) ', 'j .993', 'In the case of speculative decoding with greedy994', 'sampling, given the prompt x, a drafting method995', 'is used to generate the draft sequences ˜Y . Using996', 'these drafts, the LLM then computes the true tokens997', '(y′1, y′2, . . . , y′K) in parallel. For instance, for the998', 'guess sequence ˜y(0), the true tokens are determined999', 'as:1000', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y′1 = arg max PM(y1 | x)', 'y′2 = arg max PM(y2 | ˜y(0) ', '1 , x)...', 'y′K = arg max PM(yK | ˜y(0) ', '1:K−1, x).', '(2)1001', 'These generated tokens are then verified. Let h1002', 'be the highest number of correct guessed tokens1003', 'across all guesses. Consequently, h + 1 tokens are1004', 'generated in one forward step. Algorithm 2 out-1005', 'lines speculative decoding with greedy sampling.1006', 'BImplementation Details1007', 'B.1Frameworks and Libraries1008', 'We implement SPECTRA in Python using PyTorch1009', '2.1.0 and the Hugging Face transformers library.1010', 'For large-scale model loading (e.g., LLaMA-2-1011', '70B, LLaMA-3-70B), we employ 16-bit (FP16)1012', 'precision with a pre-allocated key-value cache.1013', 'B.2Models and Checkpoints1014', 'We run our experiments primarily with:1015', '• LLaMA-2-Chat (Touvron et al., 2023) in1016', 'sizes 7B, 13B, 70B.1017', '• CodeLlama (Rozière et al., 2024) in sizes 7B1018', 'and 13B.1019', '• LLaMA-3-Instruct (Dubey et al., 2024) in1020', 'sizes 8B and 70B.1021', 'All checkpoints are obtained from the official or1022', 'Hugging Face repositories without fine-tuning or1023', 'modification. For each model, we enable half-1024', 'precision inference. We also verify numerically (by1025', 'comparing 32-bit and 16-bit outputs) that specula-1026', 'tive decoding preserves exact or near-exact token1027', 'sequences within typical floating-point tolerances.1028', 'B.3Hardware1029', 'Most experiments are conducted on a single1030', 'NVIDIA A100 GPU with 80GB of memory. We1031', 'also evaluate on other NVIDIA GPUs (RTX 3090,1032', 'RTX 8000, A40, A6000) to study hardware-1033', 'specific scaling. For the largest checkpoints (70B)1034', 'that do not fit on a single GPU under certain con-1035', 'figurations, we optionally distribute them across1036', 'multiple GPUs (2x, 4x, or 8x H100) using standard1037', 'pipeline-parallelism from Hugging Face’s library.1038', 'B.4Hyperparameters1039', 'Lookahead, REST, and ANPD.We replicate1040', 'each baseline using their publicly available GitHub1041', 'code, keeping to the default settings and hyperpa-1042', 'rameters outlined in the original papers.1043', 'Spectra.By default, we use a 5-gram setup for1044', 'our forward/backward dictionaries, storing all sub-1045', 'sequences (i.e., sub-ngrams). We also maintain a1046', 'candidate pool of size W = 15 per key to generate1047', '13', 'new n-gram records; after each forward pass, can-1048', 'didate sequences are shifted by one token and then1049', 're-populated. We introduce a threshold τ ∈[0, 1],1050', 'default set to 0.1, to decide when to force the se-1051', 'lection of a token not yet in the forward dictionary.1052', 'This mechanism balances coverage of unseen pre-1053', 'fixes with reinforcing common contexts. At every1054', 'speculative decoding step, we allow up to G = 601055', 'guess tokens. Internal guesses receive priority, and1056', 'if there is still capacity under the guess limit, we1057', 'add external guesses.1058', 'For external lookups, we implement a trie struc-1059', 'ture for rapid prefix queries, following a design1060', 'similar to REST (He et al., 2024). For conver-1061', 'sation tasks (e.g., MT-Bench), we gather approxi-1062', 'mately 100k examples from the UltraChat dataset1063', '(Ding et al., 2023), focusing on those with minimal1064', 'perplexity under the same LLM we aim to accel-1065', 'erate. For code tasks (e.g., HumanEval, MBPP),1066', 'we draw from TheStack (Kocetkov et al., 2023)1067', 'and again refine it to the 100k snippets with the1068', 'lowest perplexity for memory efficiency. We mea-1069', 'sure perplexity by running a single forward pass (in1070', 'streaming mode) over candidate samples and rank-1071', 'ing them. Despite being relatively small (100k),1072', 'this curated corpus achieves robust guess quality.1073', 'All speedup and throughput metrics are com-1074', 'puted at a batch size of 1. In code generation tasks,1075', 'the maximum generation length is typically 5121076', 'tokens, whereas for conversation tasks (MT-Bench,1077', 'GSM8K), we allow up to 1024 tokens or stop early1078', 'if the model outputs an end-of-sequence token. All1079', 'random seeds are set to 0.1080', 'CDetails Results with Throughputs1081', 'We provide a detailed throughput analysis to com-1082', 'plement the speedup ratios reported in the main1083', 'text. Our goal is to demonstrate how SPECTRA1084', 'scales across various model sizes, datasets, and1085', 'GPU architectures. We measure throughput using1086', 'two key metrics:1087', '• Macro Throughput (Mac-TP). Calculated1088', 'as the average of per-generation token-1089', 'processing rates—i.e., for each generation1090', 'step i, we compute tokeni/timei and then1091', 'average over all steps.1092', '• Micro Throughput (Mic-TP). Calculated as1093', 'the total number of generated tokens divided1094', 'by the total elapsed time1095', 'Table 4 focuses on GSM8K and MTBench per-1096', 'formance across four different GPU models, while1097', 'Table 3 provides more granular results on ad-1098', 'ditional datasets and model configurations.In1099', 'all cases, SPECTRA consistently achieves higher1100', 'throughput than both non-speculative baselines and1101', 'other training-free accelerators, as evidenced by im-1102', 'provements in both Mic-TP and Mac-TP. Notably,1103', 'this performance advantage remains stable even on1104', 'older GPUs (e.g., the RTX 3090 and RTX 8000),1105', 'demonstrating SPECTRA’s robustness to varying1106', 'hardware capabilities.1107', 'DEvaluating SPECTRA in Different GPU1108', 'Types1109', 'Different GPU types.Table 5 reports speedups1110', 'on GSM8K and MTBench across four GPUs with1111', 'varying memory throughput and compute capabili-1112', 'ties. While absolute wall-clock times differ across1113', 'GPUs, the relative accelerations remain consistent.1114', 'SPECTRA consistently outperforms other baselines,1115', 'including Lookahead, achieving higher speedups in1116', 'all cases. On older GPUs (e.g., RTX 3090 or RTX1117', '8000), the gap between Lookahead and SPECTRA1118', 'narrows slightly due to less efficient parallelism,1119', 'but SPECTRA maintains its lead. These results1120', 'demonstrate that SPECTRA is robust to hardware1121', 'variations and effective across both data-center and1122', 'consumer-grade GPUs.1123', 'EEvaluating SPECTRA in Multi-GPU1124', 'Environments1125', 'A critical consideration for practical deployment is1126', 'how SPECTRA scales when models are distributed1127', 'across multiple GPUs—a common requirement for1128', 'large LLMs exceeding single-device memory ca-1129', 'pacity. To evaluate this, we measure SPECTRA’s1130', 'performance under three distributed configurations1131', 'of LLaMA-2-70B: (1) 2xH100 with full precision,1132', '(2) 4xH100 with full precision, and (3) 8xH1001133', 'with full precision. We also include a baseline1134', 'of 1xH100 with 8-bit quantization for memory-1135', 'constrained single-GPU inference. Table 6 reports1136', 'throughput and speedup metrics.1137', 'SPECTRA achieves consistent speedups of 2.00—1138', '2.03× across all multi-GPU configurations while1139', 'maintaining a stable compression ratio (τ) of 2.52.1140', 'This demonstrates robust scalability—partitioning1141', 'model weights introduces minimal overhead, and1142', 'the speculative verification process remains effi-1143', 'cient despite inter-GPU communication. Notably,1144', '14', 'ClassevalGSM8KHumanevalMBPPMTBenchModelMethodMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TP', 'Greedy (temperature=0)', 'CL-13BAutoregressive30.8530.8532.0332.0332.3532.3532.0732.0730.6930.63 CL-13BANPD59.7758.0389.9989.1867.4364.6586.7686.4180.1076.68 CL-13BLookahead69.2868.6289.7389.0074.3373.2393.3892.8079.3878.67 CL-13BREST39.5337.7329.9329.4751.1547.4927.4127.3928.9227.18 CL-13BSPECTRA (Ours)73.4772.9893.3693.2384.9184.41105.44105.3981.3280.68', 'CL-7BAutoregressive41.1741.1741.1741.1741.4141.4141.6041.6038.9138.93 CL-7BANPD94.7693.02132.26131.3089.2687.13131.35130.99130.41126.64 CL-7BLookahead106.51105.95123.04121.90103.45103.51120.75120.23125.58124.77 CL-7BREST59.4956.6137.6137.2170.3865.2240.1140.0939.6436.70 CL-7BSPECTRA (Ours)111.09110.68137.24136.86122.54122.41148.32148.07143.98144.32', 'L2-13BAutoregressive31.8531.5632.4032.4332.2732.2732.1932.1931.9331.78 L2-13BANPD43.3044.4447.5445.2243.2442.2836.2035.8437.4434.84 L2-13BLookahead57.4958.9447.4447.6255.7655.5844.4144.1548.1146.62 L2-13BREST38.8137.7430.3630.2240.4739.7030.7030.6736.3937.02 L2-13BSPECTRA (Ours)63.6464.3159.2158.6363.3963.1852.4352.1956.0453.75', 'L2-70BAutoregressive2.602.602.612.612.612.612.632.632.602.60 L2-70BANPD4.724.804.254.104.854.763.073.073.473.30 L2-70BLookahead6.907.164.875.126.716.733.923.935.055.02 L2-70BSPECTRA (Ours)8.078.356.586.758.418.414.884.886.326.22', 'L2-7BAutoregressive40.3340.3241.0141.0341.1441.1341.0041.0440.4840.50 L2-7BANPD65.5468.1062.4059.3863.2759.9848.9447.6752.4750.06 L2-7BLookahead88.4191.0568.0068.2084.6983.8759.7960.7670.0469.07 L2-7BREST54.7453.9341.4341.3857.9956.4141.2840.7450.5851.79 L2-7BSPECTRA (Ours)96.8898.7586.5185.5098.7798.3872.3973.2281.9379.20', 'L3-70BAutoregressive2.582.572.582.582.592.592.592.592.552.55 L3-70BANPD3.974.193.863.724.724.753.773.593.143.03 L3-70BLookahead6.176.473.993.966.636.753.703.664.494.53 L3-70BSPECTRA (Ours)6.877.185.435.347.337.505.014.885.255.16', 'L3-8BAutoregressive36.5936.5836.7436.7436.2036.2135.2435.2036.5536.69 L3-8BANPD77.2178.76141.89141.3666.3165.57118.47112.9541.7740.20 L3-8BLookahead94.9297.09136.32135.9289.9990.47133.67133.1256.0955.49 L3-8BSPECTRA (Ours)103.61105.88142.89142.7292.8693.16143.80142.7261.6960.22', 'Sampling (temperature=1.0)', 'CL-13BAutoregressive30.9030.6431.3831.3731.2431.3931.4631.4530.7130.67 CL-13BANPD35.4834.8633.5432.3432.6434.3631.5730.9570.9265.68 CL-13BLookahead42.5440.7433.7932.4940.2542.1732.0231.1971.5068.46 CL-13BREST35.1533.2225.6725.2439.5838.4926.4325.8928.4126.69 CL-13BSPECTRA (Ours)51.8650.0437.5735.6751.6052.6436.2935.2772.9069.98', 'CL-7BAutoregressive39.6039.5840.8540.8740.0540.1040.8140.8140.4940.50 CL-7BANPD50.8951.7647.4446.6844.1446.3445.8645.81112.29103.57 CL-7BLookahead60.8760.2948.5447.6457.1261.1448.6448.27110.07105.00 CL-7BREST48.6446.4135.9835.4653.3552.2637.0436.5739.3636.51 CL-7BSPECTRA (Ours)71.7071.7855.2452.8167.2769.2054.4852.91112.43108.49', 'L2-13BAutoregressive31.2331.1731.4431.4731.4131.4232.0232.0631.6731.59 L2-13BANPD37.5337.9439.1137.9936.7936.7532.9732.7136.9134.34 L2-13BLookahead47.5947.3541.6041.7646.3346.5137.8237.8247.3545.48 L2-13BREST36.7836.1729.3329.2537.4636.7129.3829.2835.5036.21 L2-13BSPECTRA (Ours)53.1352.2848.6048.1152.9353.1142.9543.0354.9852.42', 'L2-7BAutoregressive39.8939.8840.5840.5940.0940.1040.5940.6640.6540.70 L2-7BANPD52.1452.7854.2352.9051.4050.9744.7343.7750.9248.24 L2-7BLookahead70.8271.1761.1561.3468.7869.0150.8451.8368.2766.77 L2-7BREST50.3549.9940.1940.0950.8650.0638.9438.1849.1250.54 L2-7BSPECTRA (Ours)78.4678.7472.1371.6881.7181.7659.7760.0980.2177.00', 'L3-8BAutoregressive35.7535.7635.1635.1736.0136.0236.0536.0735.3935.48 L3-8BANPD44.7143.7269.1266.7351.4851.5768.0364.5440.8439.23 L3-8BLookahead53.0550.5772.6869.1164.5963.7971.8868.9055.4653.74 L3-8BSPECTRA (Ours)69.5068.9279.8876.5369.0968.6278.9976.6960.3357.69', 'Table 3: Micro throughput (Mic-TP) and Macro throughput (Mac-TP) across multiple tasks and models.', '15', 'GPUMethodGSM8KMTBenchMac-TPMic-TPMac-TPMic-TP', 'A40Autoregressive32.6632.6632.1431.66 ', 'Lookahead48.5948.7349.1347.96 ', 'SPECTRA62.5661.5259.0056.80', 'A6000Autoregressive39.1539.1738.7838.24 ', 'Lookahead58.1358.3058.8457.40 ', 'SPECTRA75.2074.1671.369.28', 'RTX8000Autoregressive34.0334.2734.2134.02 ', 'Lookahead45.2545.4245.7344.16 ', 'SPECTRA57.9557.0954.1652.32', 'RTX3090Autoregressive40.6740.7641.1741.22 ', 'Lookahead53.6953.7553.5152.09 ', 'SPECTRA74.8773.8871.5869.79', 'Table 4: Throughput results for different GPU types on GSM8K and MTBench.', 'GPUMethodGSM8KMTBenchspeedupτspeedupτ', 'A40Lookahead1.491.931.532.07 ', 'SPECTRA1.922.461.842.36', 'A6000Lookahead1.481.921.522.06 ', 'SPECTRA1.922.461.842.36', 'RTX8000Lookahead1.331.931.342.08 ', 'SPECTRA1.702.461.582.35', 'RTX3090Lookahead1.321.921.302.06 ', 'SPECTRA1.842.461.742.36', 'Table 5: Hardware scalability of SPECTRA decoding onGSM8K and MTBench for various GPU architectures.', 'even in the quantized single-GPU setting, SPEC-1145', 'TRA provides a 2.43× speedup, outperforming1146', 'standard autoregressive decoding. These results1147', 'validate SPECTRA’s practicality for large-scale de-1148', 'ployments where memory constraints necessitate1149', 'distributed inference.1150', 'FVerifying Generation Quality with1151', 'SPECTRA Decoding1152', 'Greedy Decoding Performance.To assess the1153', 'quality of greedy decoding, we compare the in-1154', 'ference results of the LLaMA-2-7B Chat model1155', 'using SPECTRA Decoding against Hugging Face’s1156', 'standard greedy search. Our baseline consists of1157', 'single-precision (FP32) inference on 160 conver-1158', 'sational turns from the MT-Bench dataset. Under1159', 'FP32, SPECTRA Decoding produces identical out-1160', 'puts to the baseline.1161', 'However, when transitioning to half-precision1162', '(FP16), even Hugging Face’s native greedy search1163', 'generates 25 discrepancies (out of 160) compared1164', 'to the FP32 baseline. SPECTRA Decoding exhibits1165', 'a similar discrepancy rate (26), confirming that it1166', 'maintains the output distribution within the numer-1167', 'ical error margins typically observed in standard1168', 'half-precision inference libraries.1169', 'Sampling Decoding Performance.We also as-1170', 'sess generation quality under a stochastic sam-1171', 'pling setting (temperature = 1.0). As detailed in1172', 'Table 7, SPECTRA Decoding produces ROUGE-1173', '1, ROUGE-2, and ROUGE-L scores on both the1174', 'CNN/DailyMail (Nallapati et al., 2016) and XSum1175', '(Narayan et al., 2018) summarization datasets1176', 'that are nearly identical to those of standard1177', 'autoregressive sampling.At the same time,1178', 'SPECTRA achieves notable speedups (1.60× on1179', 'CNN/DailyMail and 1.69× on XSum) with com-1180', 'pression ratios of 2.05 and 2.08, respectively.1181', 'These results confirm that SPECTRA Decoding1182', 'accelerates inference while preserving generation1183', 'quality across diverse tasks.1184', 'These findings reaffirm that SPECTRA Decoding,1185', 'does not degrade generation quality compared to1186', 'conventional greedy or sampling-based methods.1187', 'GToken Acceptance Rate Analysis1188', 'Figure 6 plots the cumulative number of accepted1189', 'tokens versus decoding steps for each dataset (MT-1190', 'Bench, HumanEval, MBPP, and GSM8K). The1191', 'steeper ascent of the SPECTRA curve indicates that1192', 'our method requires substantially fewer decoding1193', 'steps compared to alternatives, for example, almost1194', 'two times shorter than ANPD. This improvement is1195', '16', 'GPU & Model SettingMethodMTBenchMac-TPMic-TPSpeedupτ', '1xH100 - Quantized Int8Autoregressive2.602.601.001.00 ', 'SPECTRA6.326.222.432.51', '2xH100 - FP16Autoregressive14.8114.701.001.00 ', 'SPECTRA29.6228.912.002.52', '4xH100 - FP16Autoregressive14.6014.481.001.00 ', 'SPECTRA29.6728.892.032.52', '8xH100 - FP16Autoregressive14.3914.281.001.00 ', 'SPECTRA29.2728.552.032.52', 'Table 6: Results in multi-GPU Enviroments on GSM8K and MTBench using LLama-2-chat-70B.', 'DatasetMethodROUGE-1ROUGE-2ROUGE-LSpeedupτ', 'CNNAutoregressive9.770.397.201.001.00 ', 'SPECTRA9.740.417.181.602.05', 'XSUMAutoregressive18.124.3612.431.001.00 ', 'SPECTRA18.134.4012.491.692.08', 'Table 7: Evaluation of SPECTRA Decoding on CNN/DailyMail and XSum using a temperature of 1.0. ROUGEscores, speedups over autoregressive decoding, and compression ratio (τ) are reported for LLaMA-2-7B-Chat.', 'attributed to a higher token acceptance rate, which1196', 'in turn reduces the overall number of decoding iter-1197', 'ations and enhances the efficiency of the generation1198', 'process.1199', 'HAlgorithms1200', '17', '0250500750', '0', '10', '20', '30', '40', 'MT-Bench', '0200400', '0', '10', '20', '30', '40', 'HumanEval', '050100150', '0', '5', '10', '15', 'MBPP', '0100200', '0', '10', '20', '30', '40', '50', '60', 'GSM8K', '#Accepted tokens (in thousands)', 'LookaheadRESTANPDSpectra (Ours)', 'Figure 6: Total number of accepted tokens across all samples at each decoding step.', 'Algorithm 2 Speculative Decoding (Multiple guesses and Greedy Sampling)', 'Given guess size K, number of guesses G, and target length T. Given initial prompt sequence x. ', 'while n < T do', 'Obtain multiple drafts ˜Y = {˜y(0), ˜y(1), . . . , ˜y(G)}.', 'In parallel, compute K + 1 verification tokens y′: for i = 1 : K do', 'y′(g) ', 'i= arg max PM(yi | ˜y(g) ', 'i−1, x),∀g ∈{0, . . . , G}', 'end forIdentify the sequence ˜y(g∗) with the highest token matches and the corresponding y′(g).', 'for t = 1 : K do', 'if y′(g) ', 't= ˜y(g∗) ', 'tthen', 'Set yn+t ←˜y(g∗) ', 'tand n ←n + 1.', 'else', 'yn+t ←y′(g) ', 'tand exit for loop.', 'end if', 'end for', 'end while', '18', 'Algorithm 3 Greedy Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function GREEDYVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x and all gi2’s outputs – totally N ', 'distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:P ←D[l][i]', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:if sj = arg max P then▷accepted, update all potential speculations and probabilities', '14:o.append(sj)', '15:is_accept ←1', '16:Vnew, Dnew ←∅, ∅', '17:for k = j to size(V ) do', '18:if sj = V [k] then', '19:Vnew.append(V [k])', '20:Dnew.append(D[k])', '21:end if', '22:end for', '23:V, D ←Vnew, Dnew', '24:break', '25:else▷rejected, go to next speculation', '26:j ←j + 1', '27:end if', '28:end while', '29:if is_accept then', '30:continue', '31:else▷guarantee one step movement', '32:o.append(arg max P)', '33:break', '34:end if', '35:end for', '36:if is_accept then', '37:o.append(arg max D[1]N)', '38:end if', '39:return o', '40: end function', '19', 'Algorithm 4 Sample Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function SAMPLEVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x0 and all gi2’s outputs – totally N ', 'probability distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:Pj ←D[1]i', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:sample r ∼U(0, 1)', '14:if r ≤Pj(sj) then▷accepted, update all potential speculations and probabilities', '15:o.append(sj)', '16:is_accept ←1', '17:Vnew, Dnew ←∅, ∅', '18:for k = j to size(V ) do', '19:if sj = V [k] then', '20:Vnew.append(V [k])', '21:Dnew.append(D[k])', '22:end if', '23:end for', '24:V, D ←Vnew, Dnew', '25:break', '26:else▷rejected, go to next speculation', '27:Pj(sj) ←0', '28:Pj+1 = norm(Pj)', '29:j ←j + 1', '30:end if', '31:end while', '32:if is_accept then', '33:continue', '34:else▷guarantee one step movement', '35:sample xnext ∼Pj', '36:o.append(xnext)', '37:break', '38:end if', '39:end for', '40:if is_accept then', '41:o.append(sample xnext ∼D[1]N)', '42:end if', '43:return o', '44: end function', '20']
2025-02-19 16:02:54 - WARNING: done parsing pdf
2025-02-19 16:02:54 - WARNING: start ner pdf
2025-02-19 16:02:57 - INFO: Loading Data
2025-02-19 16:02:57 - WARNING: 2025-02-19 16:02:57 - INFO: Loading Data
2025-02-19 16:03:00 - WARNING: Predicting NER ...
2025-02-19 16:03:06 - WARNING: Finished predicting.
2025-02-19 16:03:06 - WARNING: Converting to Brat format...
2025-02-19 16:03:06 - WARNING: # of discontinuous mentions:
2025-02-19 16:03:06 - WARNING:  
2025-02-19 16:03:06 - WARNING: 0
2025-02-19 16:03:06 - WARNING: Finished.
2025-02-19 16:03:06 - WARNING: start predict re
2025-02-19 16:03:10 - WARNING: Example:   0%|          | 0/39 [00:00<?, ?it/s]
2025-02-19 16:03:10 - WARNING: Example: 100%|##########| 39/39 [00:00<00:00, 1954.27it/s]
2025-02-19 16:03:10 - WARNING: # of documents 39.
2025-02-19 16:03:10 - WARNING: # of positive examples 0.
2025-02-19 16:03:10 - WARNING: # of negative examples 104.
2025-02-19 16:03:10 - WARNING: dict_keys(['511', '541', '544', '1364', '1478'])
2025-02-19 16:03:10 - WARNING: done predict re
2025-02-19 16:03:10 - WARNING: model output text 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 16:03:10 - WARNING: len of model_output_text 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: 51
2025-02-19 16:03:10 - WARNING: original_text 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: and time-consuming. Speculative decoding has003
2025-02-19 16:03:10 - WARNING: mapping dict 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 7, 9: 8, 10: 8, 11: 9, 12: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 17, 21: 18, 22: 19, 23: 20, 24: 21, 25: 22, 26: 23, 27: 24, 28: 25, 29: 26, 30: 27, 31: 28, 32: 29, 33: 30, 34: 31, 35: 32, 36: 33, 37: 34, 38: 35, 39: 36, 40: 37, 41: 38, 42: 39, 43: 40, 44: 41, 45: 42, 46: 43, 47: 44, 48: 45, 49: 46, 50: 46}
2025-02-19 16:03:10 - WARNING: text: 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 16:03:10 - WARNING: origin bbox 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 16:03:10 - WARNING: normalized bbox 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 16:03:10 - WARNING: finalized bbox 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 16:03:10 - WARNING:  final bouding box 
2025-02-19 16:03:10 - WARNING:  
2025-02-19 16:03:10 - WARNING: {'x1': 12.217000007629395, 'y1': 269.133544921875, 'x2': 272.1279296875, 'y2': 281.13848876953125, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 16:03:10 - WARNING: done save re and ner, 
2025-02-19 16:03:10 - WARNING: start count num_rel 
2025-02-19 16:03:10 - WARNING: done count num_rel 
2025-02-19 16:03:10 - WARNING: start update document 
2025-02-19 16:03:12 - WARNING: start matching infor
2025-02-19 16:03:12 - WARNING: done matching infor
2025-02-19 16:03:12 - WARNING: start commit
2025-02-19 16:03:12 - WARNING: done update document 
2025-02-19 16:03:15 - WARNING: Failed to send email. Error: {'an': (553, b'5.1.3 The recipient address <an> is not a valid RFC 5321 address. For more\n5.1.3 information, go to\n5.1.3  https://support.google.com/a/answer/3221692 and review RFC 5321\n5.1.3 specifications. d9443c01a7336-2211893b8c5sm57006375ad.202 - gsmtp')}
2025-02-19 16:03:15 - INFO: Task dev_tasks.process_pdf_task[42dcf943-459e-4a8b-9209-374cd243bb82] succeeded in 22.903442576527596s: {'id': 44020375, 'filename': '_ACL_2025__LLM_Efficiency (2) (1).pdf', 'upload_time': '2025/02/19, 07:02:51', 'entities': 237, 'relations': 6, 'pages': 20, 'status': 'completed'}
2025-02-19 16:03:15 - WARNING: 2025-02-19 16:03:15 - INFO: Task dev_tasks.process_pdf_task[42dcf943-459e-4a8b-9209-374cd243bb82] succeeded in 22.903442576527596s: {'id': 44020375, 'filename': '_ACL_2025__LLM_Efficiency (2) (1).pdf', 'upload_time': '2025/02/19, 07:02:51', 'entities': 237, 'relations': 6, 'pages': 20, 'status': 'completed'}
