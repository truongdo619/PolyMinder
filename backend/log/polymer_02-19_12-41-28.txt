2025-02-19 12:41:28 - WARNING: {'<pad>': 0, '<suc>': 1, 'monomer': 2, 'organic': 3, 'inorganic': 4, 'condition': 5, 'polymer_family': 6, 'syn_method': 7, 'prop_name': 8, 'prop_value': 9, 'ref_exp': 10, 'char_method': 11, 'polymer': 12, 'material_amount': 13, 'composite': 14, 'other_material': 15}
2025-02-19 12:41:28 - INFO: Building Model
2025-02-19 12:41:28 - WARNING: 2025-02-19 12:41:28 - INFO: Building Model
2025-02-19 12:41:35 - WARNING: uploads/namazi2011.pdf
2025-02-19 12:41:35 - WARNING: start parsing pdf
2025-02-19 12:41:36 - WARNING: parsed 102 paragraphs
2025-02-19 12:41:36 - WARNING: ['Journal of Power Sources 196 (2011) 2573–2583', 'Contents lists available at ScienceDirect', 'Journal of Power Sources', 'journal homepage: www.elsevier.com/locate/jpowsour', 'Improving the proton conductivity and water uptake of polybenzimidazole-based proton exchange nanocomposite membranes with TiO2 and SiO2 nanoparticles chemically modified surfaces', 'Hassan Namazi ∗, Hossein Ahmadi', 'Research Laboratory of Dendrimers and Nanopolymers, Faculty of Chemistry, University of Tabriz, 29 Bahman Blv, P.O. Box 5166616471, Tabriz, Iran', 'a r t i c l ei n f o', 'Article history: Received 4 July 2010 Received in revised form 21 October 2010 Accepted 23 October 2010 Available online 4 November 2010', 'Keywords: Polybenzimidazole Proton exchange membrane Modified nanoparticles Sulfonated vinylbenzene Vinylimidazole', 'a b s t r a c t', 'Poly[2,2′-(m-pyrazolidene)-5,5′-bibenzimidazole](PPBI)wassynthesizedfrompyrazole-3,5-dicarboxylic acid and 3,3′,4,4′-tetraaminobiphenyle (TAB) through polycondensation reaction in polyphosphoric acid (PPA) as reaction solvent. And polymer-grafted SiO2 and TiO2 nanoparticles were prepared through radical polymerization of 1-vinylimidazole and sulfonated vinylbenzene on the surface-vinylated nanoparticles. The polymer-grafted SiO2 and TiO2 nanoparticles were utilized as a functional additive to prepare PPBI/polymer-grafted SiO2 and TiO2 nanocomposite membranes. Imida-zole and sulfonated vinylbenzene groups on the surface of modified nanoparticles forming linkages with PPBI chains, improved the compatibility between PPBI and nanoparticles, and enhanced the mechanical strength of the prepared nanocomposite membranes. The prepared nanocomposite membranes showed higher water uptake and acid doping levels comparing to PPBI. Also, after acid doping with phosphoric acid, nanocomposite membranes exhibited enhanced proton conductivity in comparison to the pristine PPBI and PPBI/un-modified SiO2 and TiO2 nanocomposite membranes. The enhancement in proton conductivity of nanocomposite membranes resulted from modified SiO2 nanoparticles showed higher conductivity than modified TiO2 nanoparticles. The above results indicated that the PPBI/modified SiO2 and TiO2 nanocomposite membranes could be utilized as proton exchange membranes for medium temperature fuel cells. © 2010 Elsevier B.V. All rights reserved.', '1. Introduction', 'In recent years, polymeric electrolytes based on hydrocarbons have been considered as an alternative for Nafion® membranes for high temperature polymer electrolyte fuel cells. Due to its high proton conductivity, Nafion® is an appropriate option to use in polymeric electrolyte fuel cells (PEFCs). The great proton conduc-tivity of Nafion® membranes can be resulted from the presence of sulfonate groups in polymer structure and its adsorbed water. Because of water removal at temperatures above 80 ◦C, the pro-ton conductivity of membranes decreases dramatically leading to a poor performance in fuel cells. In addition, the reason for the scientific enthusiasm for utilizing the fuel cells at elevated temper-atures is that there is no need to use high-purity hydrogen at higher temperatures; besides, CO-existing in the H2 fuel does not load to the poisoning of electro-catalysts. Furthermore, at higher temper-atures, the rate of red-ox reaction on electrolytes increases. On the other hand, there would be less need for the use of heat exchang-', '∗Corresponding author. Tel.: +98 411 3393121; fax: +98 411 3340191. E-mail address: namazi@tabrizu.ac.ir (H. Namazi).', 'ers [1–3]. Sulfonated polysulfone (PSF) [4], polyetheretherketone (PEEK) [5,6] and polybenzimidazoles (PBIs) [7–11] are among poly-mers which can be used as alternatives for Nafion in PEFCs. Amongpolybenzimidazolederivatives,poly[2,2-5,5(m-phenylene)-bibenzimidazole](PBI),whichisalsocalled commercial polybenzimidazole have been used as a mem-brane material in PEMFCs. Having high chemical as well as thermal resistance and proton conductivity, polybenzimidazoles are great options to be used in the fuel cells with function at elevated temperatures above 100 ◦C [12]. PBIs should have enough proton conductivity so that it could be used as proton exchange membranes in the fuel cells. To achieve this purpose, they are usually doped with a sufficient acid like H3PO4 to an appropriate doping level [13]. Nanocomposite membranes are a new group of membranes which contain nanopar-ticles such as SiO2, TiO2 and ZrO2 and other compounds [14–21]. The introduction of nanoparticles into the membranes results in a considerable improvement in their features like mechani-cal properties, methanol permeability, but in some cases leads to a decrease in proton conductivity [22–24]. In order to over-come this problem, surface functionalized nanoparticles, which means various compounds with polar groups grafted chemically', '0378-7753/$ – see front matter © 2010 Elsevier B.V. All rights reserved. doi:10.1016/j.jpowsour.2010.10.082', '2574H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–2583', 'Fig. 1. The reaction of PPBI synthesis.', 'to the surface of the nanoparticles [25–28], were used. As men-tioned before, the numerous research works were done concerning grafting various compounds on the SiO2 and TiO2 nanoparticles which used for different purposes. However, in this work, we have used monomer bearing functional groups like imidazole and sulfonated vinylbenzene which can improve the properties of pro-ton conductive membranes. These monomers are polymerized on the surface of the nanoparticles and as a consequence, they can more affect the properties of the membranes than while one functional group is grafted onto the surface of the nanoparticles. In addition, the polymerization of the selected monomers such as vinylimidazole and sulfonated vinylbenzene on the surface of nanoparticle could cause more compatibility between nanopar-ticles and polymer matrix which finally improve the mechanical properties. In this work, nanocomposite membranes were prepared using various amounts of modified TiO2 and SiO2 nanoparticles and newly synthesized polybenzimidazole was depicted poly[2,2-5,5-(m-pyrazolidene)-bibenzimidazole] (PPBI). Modification of the mentioned nanoparticles was performed by radical polymerization of some vinyl monomers such as sulfonated styrene and vinylim-idazole on the surface of the nanoparticles to which vinylic group was bounded covalently. Also, the effect of surface modification on the properties of the nanocomposite membranes such as pro-ton conductivity, mechanical properties and water uptake were studied.', '2. Experimental', '2.1. Materials', '3,3′,4,4′-Tetraaminobiphenyle (TAB) was purchased from Fluka and purified by re-crystallization from hot water. Pyrazole-3,5-dicarboxylic acid, was purchased from Merck and used as received. SiO2 and TiO2 nanoparticles with diameters 13 and 24 nm respectively, were purchased from Degusa Company and used as received. Polyphosphoric acid, sodium vinylbenzene sul-fonate and 1-vinylimidazole were purchased from Aldrich and were used without further treatment. N,N-Dimethylacetamide (DMAC) was purchased from Fluka and used as received. Tetrahydrofuran, trichlorovinylsilane, dimethylformamide (DMF) and dimethylsul-foxide (DMSO) were purchased from Aldrich Chemical and were distilled and dried before using.', '2.2. Preparation of PPBI', 'PPBI was prepared using polycondensation reaction of equimo-lar of pyrazole-3,5-dicarboxylic acid and TAB monomers in polyphosphoric acid medium (Fig. 1). The monomers and polyphos-phoric acid were added to a three-necked flask under argon atmosphere and stirred using mechanical overhead stirrer for 18 h at 170 ◦C and 8 h at 220 ◦C. After reaction completion, the vis-cose mixture was precipitated in water. It was then separated, neutralized with sodium hydroxide solution (10% w/w), washed thoroughly with water and dried in a vacuum oven for 24 h at 60 ◦C to obtain PPBI.', '2.3. Modification of TiO2 and SiO2 nanoparticles', 'One gram of TiO2 nanoparticles dried at 100 ◦C under vacuum was added into a 250 mL flask containing 100 mL of dry THF and 10 mL of dried trichlorovinylsilane (coupling agent) under argon atmosphere. The resulting dispersion was stirred at room tempera-ture for 48 h. After completion of the reaction, the surface vinylated TiO2 nanoparticles (TiO2-vinylated) were separated and washed with THF several times to remove un-reacted trichlorovinylsilane. After drying of the resultant powder at 80 ◦C for 24 h under vacuum, 1.5 g of the dry vinylated TiO2, 0.64 g 2,2′-azobis (isobutyronitrile) (AIBN) and 100 mL of dry DMF were added in to a 250 mL three-necked flask under argon atmosphere equipped with a condenser and stirred for 1 h at 75 ◦C and followed by drop-wise addition of 25 mL of vinylimidazole monomer to the mixture. At the end of the reaction, 50 mL of DMF was added to the mixture followed by stir-ring for 1 h at room temperature. After separation with centrifuge, the obtained powder (TiO2-PVI) was washed several times with NaCl (0.1 M) solution and de-ionized water respectively, in order to remove the un-grafted polyvinylimidazole (PVI) chains from the surface. In order to prepare TiO2 nanoparticles modified by poly (sulfonated vinylbenzene) (TiO2-PSV), 12.5 g of sulfonated vinyl-benzene was dissolved in dry DMSO and then added to the mixture of 1.5 g of vinylated TiO2 nanoparticles, 100 mL of DMSO and 1.0 g of AIBN at 75 ◦C under argon atmosphere while stirring. After 18 h, the obtained powder (TiO2-PSV) was separated by centrifuge and washed several times with DMSO and ethanol and then dried at 60 ◦C under vacuum for 6 h. All of the above explanations were also applied to the chemical modification process of SiO2 nanoparticles (Fig. 2).', '2.4. Preparation of nanocomposite membranes and acid doping', 'Many different methods are used for the production of inorganic nanoparticles. For further manipulations of nanoparticles which usually exist as aggregates, they are dispersed in a liquid or solid medium. In this work the ultrasonic waves were used for dispersing the nanoparticles in polymeric solution for 1 h at 80 ◦C. Nanocomposite membranes were prepared using PPBI and vari-ous amounts (5, 10 and 15% w/w) of both modified and un-modified nanoparticles by casting method from DMSO solvent on glass plates with thicknesses of 40–60 �m. After casting, the membranes were immersed in de-ionized water and then dried for 5 h at 80 ◦C under vacuum. Subsequently, they were immersed in phosphoric acid (85% w/w) at 30 ◦C for 3 days in order to be doped. After washing with water, doped membranes were dried at 80 ◦C for 24 h under vacuum.', '2.5. Characterization techniques', 'Prior to polymer characterization, all polymers were dried at 80 ◦C in vacuum to a constant weight. Fourier transform infrared (FTIR) spectra were obtained with Bruker Fourier Transform Spectrophotometer. 1H NMR spectra were acquired on a Bruker Instrument at 400 MHz in DMSO-d6. Thermogravimetric analy-', 'H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–25832575', 'Fig. 2. The schematic of modification reactions of nanoparticles.', 'sis (TGA) spectra were recorded on a D.T.G. 60 AH Shimadzu Instruments at a heating rate of 10 ◦C min−1 in air atmosphere. Transmission electron microscopy (TEM) analysis was performed using a LEO 906 microscope with an 80 kV voltage. The water uptake values of the prepared membranes were cal-culated using the following equation:', 'Water uptake (%) = ws −wd', 'wd× 100', 'where ws and wd are membrane weights after and before dipping the dried membranes in distilled water for 24 h at 30 ◦C, respec-tively. The membranes were dried at 100 ◦C in vacuum for 3 h. The wet membranes were weighed after wiping with paper tissue in order to remove any surface moisture. Acid doping levels (mole H3PO4 per PPBI repeating unit) were measured by dipping the membranes in an 85% phosphoric acid solution for 48 h. Membranes were weighed after washing with water and drying at 100 ◦C under vacuum for 4 h. Protonconductivitywasmeasuredbyelectrochemical impedance spectroscopy (EIS) using an EG&G Advanced Electro-chemical System (PARSTAT 1263-Princeton Applied Research, USA) with a two electrode setup in the frequency range of 100 kHz–2 Hz (150 data points). The two electrodes were 1 cm × 1 cm stainless steel plates mirror polished with emery type papers and washed', 'with de-ionized water and subsequently, with acetone before each experiment [29]. Studied membranes were sandwiched between the stainless steel electrodes with pressure applied by a clamp, counter and ref-erence electrodes are both connected to one of the plates and the working electrode to the other plate. The mechanical properties of the nanocomposite membranes were carried out by Universal Testing Machine (GOTECH, GT-TCS-208).', '3. Results and discussion', '3.1.1H NMR and FT-IR characterization', 'As seen in Fig. 3, the 1H NMR spectrum of the synthesized poly-mer shows a singlet near 13.5 ppm for imidazole protons (N–H) and multiple signals in the region of 6–8 ppm for aromatic ones. The absence of absorbent peak related to amine group proton (N–H) in the area of 4.0–5.0 ppm and an absorbent related to amide group in the area of 10.3–10.6 ppm show the complete closure of imidazole rings during the polycondensation reaction. In FT-IR characteristics of PPBI, the presence of absorbent bands in 800 cm−1 is related to the vibration of C–H bond of imida-zole, a band in about 1300 cm−1 is related to the vibration of C–N', 'Fig. 3. The 1H NMR spectrum of PPBI.', '2576H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–2583', 'Fig. 4. FT-IR spectra of unmodified (a), PVI grafted (b) and PSV grafted (c) nanopar-ticles.', 'bond of imidazole, the absorbent band in 1450 cm−1 is related to C N and C C vibrations and bands in the area of 3400 cm−1 and 3140 cm−1 are related to N–H vibrations. All the above absorbent bands confirm the synthesis of PPBI. Furthermore, the absence of the absorbent band related to the carbonyl group in the vicinity of 1760 cm−1 can be confirmed as an evidence of the completion of cyclization reaction. The FT-IR spectra of modified and un-modified SiO2 nanoparti-cles are shown in Fig. 4(a and b, c), respectively. The presence of the absorbent bands in 3050 cm−1 and 3100 cm−1 can be related to aromatic C–H bonds and also the existence of bands in the area of 2950 cm−1 can be attributed to aliphatic C–H bonds. Also the peaks in 1619 and 1419 cm−1 can be due to C C and C N bonds. In', 'Fig. 4(c), the presence of the absorbent band in 1257 cm−1 is cor-responded to S O bond of the sulfonic acid group. These prove the existence of polymeric chains that are grafted onto the nanoparticle surface. FT-IR spectra of the modified nanoparticles have been measured after complete washing and drying. To be sure of the chemical bonding of all chains (not physical) onto the particle surface, some un-modified nanoparticles were mixed with polymeric solutions of PVI and PSV and were stirred for 1 h. After filtration, the washing method used to wash the modified nanoparticles, was applied to the resulting powder as well. The FT-IR spectrum of the obtained powder did not show any absorbent bands related to PVI and PSV chains. The absorbent bands, which existed in the spectrum of mod-ified nanoparticles, were not observed in this spectrum. So, it was indicated that the polymeric chains have been attached to the par-ticles surface via covalent bonds.', '3.2. Thermal analysis', 'Thermogravimetric analysis diagrams of both modified and un-modified nanoparticles are shown in Fig. 5. Weight loss in 85 ◦C can be related to the release of physically absorbed water. In un-modified nanoparticles, the minor weight loss in elevated temperatures can be attributed to the loss of hydroxyl groups of the particle surface attached by covalent bonds. Weight loss in modified nanoparticles could be a result of thermal degradation of the polymeric chains grafted to the surface of the nanoparti-cles. Also, the weight loss in 230–300 ◦C was due to the thermal decomposition of the sulfonate groups in the grafted PSV chains [30]. Thermograms of PVI modified nanocomposite with 5, 10 and 15% of TiO2 are shown in Fig. 6. As it is observed, the thermal prop-erties of the nanocomposites prepared with various amounts of TiO2 are almost similar. For this reason, in order to compare the thermal properties of other nanocomposites which prepared with', 'Fig. 5. TGA thermograms of modified and un-modified a) SiO2 and b) TiO2.', 'H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–25832577', 'Fig. 6. The TGA thermograms of PPBI/TiO2-PVI-5%, 10% and 15% nanocomposites.', 'modified nanoparticles having different monomers, only the TGA thermograms of nanocomposite containing of 10% (between 5% and 15%) modified nanoparticles are reported. The remaining weight percentage in 850 ◦C is related to the modified TiO2 that is less than the added amount of PVI-modified TiO2 nanoparticles when the membrane was prepared. This decrease in weight in comparison with the theoretical amount could be related due to the degradation of the chains grafted onto the surface of the nanoparticles. Also, in order to obtain the par-ticular weight of the modified nanoparticles, specific amounts of modified and un-modified nanoparticles were weighed after and before complete burning out all the polymers. They were heated to temperatures about 1000 ◦C for 30 min and the obtained nanoparti-cles were weighed immediately. The difference between the weight of un-modified nanoparticles before and after heating shows the amount of lost water which was either physically absorbed or pro-duced from the condensation reaction of the hydroxyl groups on the nanoparticle surface. Also from the difference between the weight of modified nanoparticles before and after heating and consider-', 'Table 1 The percentages values of grafted chains in nanoparticle surface.', 'SamplesTheoretical values (%)Experimental values (%)', 'SiO2-PVI3828 SiO2-PSV3624 TiO2-PVI2520 TiO2-PSV2518', 'ing the weight of lost water, the percentage of the grafted polymer onto the nanoparticle surface was obtained. The obtained results and theoretical amounts are shown in Table 1. The amounts of the obtained results in all cases are more than theoretical values. This could be related to the presence of free water molecules and hydroxyl groups on the nanoparticles surface in which their values are not subtracted from theoretical values. The diagrams of thermogravimetric analysis of both PPBI and nanocomposites prepared from the modified nanoparticles (10% w/w) are shown in Fig. 7. The weight loss observed in the tempera-ture range of 50–140 ◦C is a result of the release of water adsorbed into the membranes which is less significant in nanocomposites in comparison to pure membrane. The second loss in weight, observed at the temperature range of 500–530 ◦C, is also attributed to the thermal degradation of poly-meric backbone of the nanocomposite membranes which is almost identical to the degradation temperature of pure PPBI. As a result, the addition of both modified and un-modified nanoparticles do not have any noticeable effect on thermal stability of the prepared nanocomposites. As it is observed in Fig. 7, the degradation of PVI-modified nanoparticles occurs at 185 ◦C which is lower than the related tem-perature of PSV-modified ones. Therefore, the former one can be used in fuel cells with performance under 150 ◦C, but PSV-modified membrane is more suitable in high temperature fuel cells.', 'Fig. 7. TGA thermograms of PPBI and PPBI based nanocomposite membranes with modified a) SiO2 and b) TiO2 nanoparticles.', '2578H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–2583', 'Fig. 8. Water uptake of nanocomposite membranes as a function of contents of nanoparticles.', '3.3. Water uptake and doping level', 'Water uptake amount of nanocomposite membranes contain-ing 5, 10 and 15% of modified nanoparticles at 25 ◦C are shown in Fig. 8. Addition of silica nanoparticles to PPBI membranes increased their water uptakes due to the hygroscopic nature of SiO2 and TiO2nanoparticles. The sulfonic and imidazole groups in the nanopar-ticles increased the water uptake to a greater extent. The amount of water uptake for PPBI/TiO2-PSV, PPBI/SiO2-PSV, PPBI/TiO2-PVI and PPBI/SiO2-PVI are 21.6, 22.7, 20.4 and 21.9%, respectively. The effect of sulfonic acid groups of PSV chains grafted onto the nanoparticle surface on the water uptake is more than imidazole groups in PVI chains due to the higher hygroscopic properties of PSV comparing to PVI chains grafted on to the silica nanopar-ticle surface. The water uptake of PPBI/SiO2-PSV is higher than PPBI/TiO2-PSV which can be due to the further increase of the surface of SiO2 nanoparticles in comparison to TiO2 nanoparticle surface. The PPBI-based membranes were doped with 85% H3PO4 to prepare the acid-doped membranes. Fig. 9 shows the acid-doping levels of the PPBI-based nanocomposite membranes. Acid-doping level (H3PO4 molecules per PPBI repeating unit) of pristine PPBI membrane was 13.5, which decreased to 11.8 for PPBI/SiO2, 12.0 for PPBI/SiO2-PSV, 11.7 for PPBI/TiO2 and 12.2 for PPBI/TiO2-PSV', 'Fig. 9. Acid-doping levels of PPBI nanocomposite membranes as a function of con-tents of nanoparticles.', 'Fig. 10. The Nyquist plot of EIS data for nanocomposite membranes.', 'with 10% nanoparticle content and increased to 15.8 for PPBI/SiO2-PVI and 15.2 for PPBI/TiO2-PVI. Some of the amino groups in PPBI chains were blocked with the sulfonic acid groups of TiO2-PSV and SiO2-PSV, to decrease the binding ability of PPBI chains to H3PO4molecules. Among the prepared nanocomposites, the water uptake in PPBI/SiO2-PSV and acid doping in PPBI/SiO2-PVI were the most one. Even in the comparison of these membranes with the simi-lar ones which prepared from modified TiO2 such as PPBI/TiO2-PSV and PPBI/TiO2-PVI, the amount of water uptake and doping level were higher. This shows that using modified SiO2 nanoparticles in fuel cells is more appropriate.', '3.4. Proton conductivity', 'The membrane was sandwiched between two stainless steel electrodes and pressed together by clamp. Then, the MEA (Mem-brane Electrolyte Assembly) was inputted in the steam autoclave equipped with temperature controller. For measurement of pro-ton conductivity, the steam autoclave supplied with steam from a house generator for exposure to saturated steam at desired temperature. As an example, for saturated steam the pressure cor-responding to 120 ◦C is 205 kPa. The time of exposure to saturated steam at 120 ◦C was 30 min that this value for temperature of 140 ◦C was increased to 50 min.', 'Fig. 10 shows the Nyquist plot and equivalent circuit of studied membranes at 25 ◦C and RH 100% for some membranes. After fitting the EIS experimental data on the electrical equiv-alent circuit, the membrane proton conductivity was determined from the intercept of EIS data with real impedance Z′ axis (as the value of Rm element). In equivalent circuit, Rm presents the resis-tance of the polymer film between the two metallic electrodes toward charge carriers (i.e. H+). L1 presents the stray inductance of the setup wires. CPE1 presents the non-ideal double layer capac-itance of the film/metal electrode interface. Rct and W0 show the charge transfer resistance of the (possible) reactions at the film/metal electrode interface and the diffusion processes through the polymers film, respectively. The proton conductivities of the phosphoric acid-doped PPBI based nanocomposite membranes containing SiO2 and TiO2 (Fig. 11) at 100% relative humidity were presented as a function of temperatures up to 80 ◦C. The pro-ton conductivities of all membranes increased with increasing the operation temperature up to 80 ◦C. Addition of the un-modified nanoparticles showed a negative effect on the proton conductivities of nanocomposite membranes. Similar trends were also reported to', 'H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–25832579', 'Fig. 11. Proton conductivities of PPBI based nanocomposite membranes containing SiO2 (a) and TiO2 (b) nanoparticles at 100% relative humidity up to 80 ◦C.', 'other nanocomposite membranes like Nafion®/silica membranes and PBI/silica membranes [31]. On the other hand, comparing the pristine PPBI and Nafion®', '117 membranes, the relatively high proton conductivities of the nanocomposite membranes prepared from modified nanoparticles are noteworthy [32]. Li et al. have used sulfonated organosilica to modify Nafion® membranes and they found that the proton con-ductivities of the modified membranes were still lower than that of the neat Nafion® membrane. The reduction in the proton conductivity was attributed to the silica-induced changes in the tortuous paths and in the dis-tribution of hydrophilic/hydrophobic domains. However, in this work an enhancing effect on the proton conductivity was observed with addition of PSV and PVI grafted nanoparticles to PPBI mem-brane. The sulfonic and imidazole groups of modified nanoparticles also contribute in the acid-doping effect to PPBI polymer so as to enhance the proton conductivity of nanocomposite membranes prepared from modified nanoparticles. Moreover, the presence of modified nanoparticles induces proton conductive pathways lead-ing to an increase in the proton conductivities of PPBI/modified nanoparticles membranes [24]. The proton conductivities of the acid-doped nanocompos-ite membranes containing SiO2 and TiO2 (Fig. 12) were further', 'Fig. 12. Proton conductivities of PPBI based nanocomposite membranes containing SiO2 (a) and TiO2 (b) nanoparticles at dry environment up to 145 ◦C.', 'studied at high temperatures under an anhydrous environment. All the PPBI/modified nanoparticle membranes still exhibited higher proton conductivities than the pristine acid-doped PPBI membrane. Generally, two principle mechanisms of vehicle mechanism and Grotthuss mechanism (hopping) describe proton diffusion through the membrane [33]. It is possible that the bounded water par-ticipates by the Grotthuss mechanism, and the free water takes part mostly by vehicle mechanism [34]. Addition of silica-SO3H to the membranes increases the bounded water content in the membranes (Fig. 13). The bounded water facilitates the proton transport ability through the Grotthuss mechanism, originating by generation of a continuous proton conductive pathway. There-fore, the high proton conductivity of PPBI/modified nanoparticles can be attributed to the high bounded water content in the membrane. Since the Grotthuss-type diffusion mechanism without assis-tance of the vehicle mechanism has been proposed for the protonconductionunderanhydrousenvironments[35,36], theprotonconductivityoftheacid-dopedPPBI/modified nanoparticlemembranescouldbesynergisticallyresulted from their modified nanoparticle contents and the acid-doping levels. Consequently, the highest proton conductivity was obtained withtheacid-dopedPPBI/SiO2-PSV-15%membraneathigh humidity and low temperatures and PPBI/SiO2-PVI-15% mem-brane in high temperatures and low humidity, which possesses the moderate modified nanoparticles content and acid-doping level. As it is seen in Fig. 9, either we use PSV-modified or un-modified nanoparticles, the increase in the amount of the nanoparticles leads', '2580H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–2583', 'Fig. 13. Illustration of the state of water in the membranes, the interaction between the functional groups of nanoparticles and of PPBI to restrict the nanoparticles motion, and the proton-transport mechanism in the membranes [24].', 'to a decrease in the acid doping level, but when PVI-modified nanoparticles were used, doping level increased. Also, according to Fig. 8, water uptake of the membrane shows an increase when using higher amounts of modified nanoparticles. This increase in PSV-modified nanocomposite membranesishigherinPVI-modified one. Proton conductivity of the nanocomposite membranes obtained from PSV-modified nanoparticles shows significant increase in comparison to PVI-modified ones in low temperature and high rel-ative humidity. But in high temperature and low relative humidity, proton conductivity of the nanocomposite membranes obtained from PVI-modified nanoparticles is observed to be more than PSV-modified ones and even at 145 ◦C the proton conductivity of PPBI/SiO2-PVI 15% is more than PPBI/SiO2-PSV-15%. The exis-tence of modified nanoparticles in PPBI nanocomposite membranes results in high proton conductivity in three ways:', '1) With increase in water uptake 2) With increase in the amount of doping level and 3) With increase in the number of proton conductive functional groups such as sulfonic acid group.', 'In PSV-modified nanocomposite membranes, acid doping decreases while the water uptake and the number of sulfonic acid groups shows an increase. Consequently, high proton conductiv-ity can be a result of these changes. It should be mentioned that in temperatures higher than 100 ◦C and in very low relative humidity, water uptake of PSV-modified membranes decreases which leads to low efficiency of the sulfonic groups attached onto the surface of the nanoparticles which in turn results in a decrease in proton conduc-tivity in both membranes, but the increase in proton conductivity resulting from high doping level is significant and also the proton conductivity of PVI-modified nanocomposite membranes increase in high temperatures. It is noteworthy that Nafion® 117 shows relatively low pro-ton conductivities under anhydrous conditions and almost lacks its proton conductivities at high temperatures. Comparing to Nafion®', '117 membranes, the high proton conductivities of PBI/SiO2-PVI 10% indicate that this membrane could be utilized as proton exchange membranes for fuel cell at high temperatures and dry conditions.', '3.5. Mechanical properties', 'The mechanical properties of the PPBI-based membranes were measured. The stress–strain curves are shown in Fig. 14. Pristine', 'PPBI membrane showed an elongation at break of 38%. Mod-ification of nanocomposites with SiO2 and TiO2 nanoparticles results in an increase in the tensile strengths and a subse-quent decrease in the elongations at break for the PPBI-based membranes. For PPBI/modified nanoparticles nanocomposites membranes (PPBI/modified nanocomposite consists of the PPBI and surface-modified SiO2 and TiO2 nanoparticles by PVI and PSV chains and PPBI/unmodified nanocomposite includes PPBI and SiO2 and TiO2 nanoparticles without any treatment), the presence of ionic linkages between PPBI and silica particles can be assigned to their high Young’s modulus. Introduction of modified and un-modified nanoparticles increases the brittleness of the nanocomposite mem-branes with a decrease in their elongations at break. The effects of modified and unmodified nanoparticles on the elongations at break of the nanocomposite membranes were different. The decrease in the elongation at break and increase in the brittleness were widely observed with polymer nanocomposites and could be attributed to the presence of inorganic reinforcement [37–39]. Moreover, the ionic linkages in nanocomposite membranes prepared from modi-fied nanoparticles are higher than hydrogen bonding in PPBI/SiO2and PPBI/TiO2.', 'Fig. 14. Stress–strain curves of PPBI and nanocomposite membranes containing 10% nanoparticles.', 'H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–25832581', 'Fig. 15. The SEM photograph of a) pure PPBI membrane and TiO2-PSV nanocomposite membranes containing of b) 5%, c) 10% and d) 15% nanoparticles.', 'The brittleness of PPBI/TiO2 based nanocomposite membranes is higher than PPBI/SiO2 ones. The brittleness of membranes pre-pared from TiO2-PVI and SiO2-PVI are less than PPBI/SiO2-PSV and PPBI/TiO2-PSV membranes which are perhaps due to more com-patibility of PVI grafted chains onto the nanoparticles surface with PPBI backbone. Comparing the brittleness of the nanocomposite membranes of the PPBI/SiO2-PSV and PPBI/SiO2-PVI with PPBI/TiO2-PSV and PPBI/TiO2-PVI, respectively indicates that the brittleness of the membranes prepared with modified SiO2 nanoparticles is less than the modified TiO2 ones.', '3.6. Morphological analysis', 'The scanning electron microscopy (SEM) photographs of the PPBI/TiO2-PSV nanocomposite membranes surface with various amounts of TiO2-PSV are shown in Fig. 15. A relatively flat surface is observed for thin membranes of pure PPBI (Fig. 15(a)). Fig. 15(b–d) display surface morphological properties of nanocomposite membranes having 5, 10 and 15% of TiO2-PSV, respectively. As seen in Fig. 15, TiO2-PSV nanoparticles are dispersed homo-geneously in PPBI matrixes and are increased with increasing of the percentage of TiO2-PSV nanoparticles. Also the major amounts of nanoparticles size are in the range of nanoscale and the bigger parti-cles as evidence for the formation of aggregations are not observed.', 'The size of the TiO2-PSV particles are about 25–40 nm and the size of particles on the surface of the nanocomposite are determined to be 30–60 nm.', 'Fig. 16(a) shows the SEM micrographs of the modified SiO2nanoparticles. If the interactions between silica nanoparticles are stronger than that of between PPBI molecules and silica nanoparticles, the silica nanoparticles are agglomerated together to form silica domains in micrometer sizes. The compatibility between PPBI and unmodified nanoparticles and the homogene-ity of the PPBI/SiO2 or PPBI/TiO2 membranes are somewhat poor [32]. However, the dispersion homogeneity of silica nanoparticles in PPBI membranes could be significantly improved with using surface modified nanoparticles as the additives. As shown in Fig. 16(b), the dispersion homogeneity of modified nanoparticles is observed in the TEM micrograph of PPBI/PSV-SiO2 nanocomposite membrane, which demonstrates high compatibility between PPBI polymer and PSV-SiO2 nanoparticles and the high dispersion abil-ity of PSV-SiO2 in PPBI matrix. Particle agglomeration is still not observed in the TEM micrograph of PPBI/PSV-SiO2-10%. The ionic linkages between PPBI and modified nanoparticles are contributed to their higher compatibility [32]. However, in nanocompos-ite membranes prepared from un-modified SiO2 nanoparticles, thedispersionofthenanoparticlesinpolymermatrixis not homogenous and probably aggregation has been formed (Fig. 16(c)).', '2582H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–2583', 'Fig. 16. SEM micrograph of SiO2-PSV (a); TEM micrographs of PPBI/SiO2-PSV-10% (b); and PPBI/SiO2-10% (c).', '4. Conclusion', 'The nanocomposite PPBI-based membranes were prepared using various amounts of modified and un-modified TiO2 and SiO2nanoparticles. The thermal stability of the prepared nanocompos-', 'ite membranes was similar to PPBI membrane. The water uptake of the nanocomposite membranes prepared from TiO2/PSV and SiO2/PSV nanoparticles were higher than other nanocomposite membranes. The acid doping level and proton conductivity of the nanocomposite membranes in dry conditions obtained from TiO2-PVI and SiO2-PVI nanoparticles were the highest in comparison to PPBI/SiO2-PSV and PPBI/TiO2-PSV. Formation of nanocomposites with modified and un-modified nanoparticles resulted the increas-ing of tensile strengths. The decreasing in the elongation at break and increase in the brittleness was widely observed with polymer nanocomposites and could be attributed to the presence of inor-ganic reinforcement. The high proton conductivities of PBI/SiO2-PVI 10% indicated that this membrane could be utilized as a proton exchange membrane for fuel cell at high temperatures and dry conditions.', 'Acknowledgments', 'The authors would like to acknowledge Dr. M.-G. Hosseini and I. Ahadzadeh for proton conductivity measurements, Ms. M. Nam-vari for English editing and the University of Tabriz for financial supports of this project.', 'References', '[1] A. Saccà, A. Carbone, E. Passalacqua, A. D’Epifanio, S. Licoccia, E. Traversa, E. Sala, F. Traini, R. Ornelas, J. Power Sources 152 (2005) 16–21. [2] A. Saccà, I. Gatto, A. Carbone, R. Pedicini, E. Passalacqua, J. Power Sources 163 (2006) 47–51. [3] I. Gatto, A. Saccà, A. Carbone, R. Pedicini, F. Urbani, E. Passalacqua, J. Power Sources 171 (2007) 540–545. [4] F. Lufrano, I. Gatto, P. Staiti, B. Antonucci, E. Passallacqua, Solid State Ionics 145 (2001) 47–51. [5] J. Kerres, A. Ullrich, F. Meier, Th. Haring, Solid State Ionics 125 (1999) 243–249. [6] A. Carbone, R. Pedicini, G. Portale, A. Longo, L. D’Ilario, E. Passalacqua, J. Power Sources 163 (2006) 18–26. [7] R.J. Spry, M.D. Alexander Jr., S.J. Bai, T.D. Dang, G.E. Price, D.R. Dean, B. Kumar, J.S. Solomon, F.E. Arnold, J. Polym. Sci., Part B: Polym. Phys. 35 (1997) 2925– 2933. [8] P. Staiti, F. Lufrano, A.S. Aricò, E. Passalacqua, V. Antonucci, J. Membr. Sci. 188 (2001) 71–78. [9] J.M. Bae, I. Honma, M. Murata, T. Yamamoto, M. Rikukawa, N. Ogata, Solid States Ionics 147 (2002) 189–194. [10] D.J. Jones, J. Rozière, J. Membr. Sci. 185 (2001) 41–58. [11] J. Labato, P. Canizares, M.A. Rodrigo, J.J. Linares, G. Manjavacas, J. Membr. Sci. 280 (2006) 351–362. [12] H.-J. Kim, S.J. An, J.-Y. Kim, J.K. Moon, S.Y. Cho, Y.C. Eun, H.-K. Yoon, Y. Park, H.-J. Kweon, E.-M. Shin, Macromol. Rapid Commun. 25 (2004) 1410–1413. [13] Q. Li, H.A. Hjuler, N.J. Bjerrum, J. Appl. Electrochem. 31 (2001) 773–779. [14] M. Watanabe, H. Uchida, Y. Seki, M. Emori, P. Stonehart, J. Electrochem. Soc. 143 (1996) 3847–3852. [15] M. Watanabe, H. Uchida, M. Emori, J. Electrochem. Soc. 145 (1998) 1137–1141. [16] P.L. Antonucci, A.S. Arico, P. Creti, E. Ramunni, V. Antonucci, Solid State Ionics 125 (1999) 431–437. [17] K.T. Adjemian, S.J. Lee, S. Srinivasan, J. Benziger, A.B. Bocarsly, J. Electrochem. Soc. 149 (2002) A256–A261. [18] V. Baglio, A.D. Blasi, A.S. Aric‘o, V. Antonucci, P.L. Antonucci, F. Serraino Fiory, S. Licoccia, E. Traversa, J. New Mater. Electrochem. Syst. 7 (2004) 275–280. [19] D.C. Hague, M.J. Mayo, J. Am. Ceram. Soc. 77 (1994) 1957–1960. [20] A. Carbone, R. Pedicini, A. Saccà, I. Gatto, E. Passalacqua, J. Power Sources 178 (2008) 661–666. [21] A. Carbone, A. Saccà, I. Gatto, R. Pedicini, E. Passalacqua, Int. J. Hydrogen Energy 33 (2008) 3153–3158. [22] C.H. Rhee, H.K. Kim, H. Chang, J.S. Lee, Chem. Mater. 17 (2005) 1691–1697. [23] P. Bébin, M. Caravanier, H. Galiano, J. Membr. Sci. 278 (2006) 35–42. [24] Y.H. Su, Y.L. Liu, Y.M. Sun, J.Y. Lai, D.M. Wang, Y. Gao, B. Liu, M.D. Guiver, J. Membr. Sci. 296 (2007) 21–28. [25] P. Staiti, Mater. Lett. 47 (2001) 241–246. [26] P. Staiti, M. Minutoli, S. Hocevar, J. Power Sources 90 (2000) 231–235. [27] E.B. Easton, Z. Qi, A. Kaufman, P.G. Pickup, Electrochem. Solid-State Lett. 4 (2001) A59–A61. [28] A. Wanda, P. Peter, J. Solid State Electrochem. 8 (2004) 742–747 (6). [29] B. Christele, B. Emmanuel, B. Elodie, C. Philippe, Z. Nathalie, Polymer 46 (2005) 8502–8510. [30] D.S. Kim, G.P. Robertson, M.D. Guiver, Macromolecules 41 (2008) 2126–2134. [31] C.N. Li, G.Q. Sun, S.Z. Ren, J. Liu, Q. Wang, Z.M. Wu, H. Sun, W. Jin, J. Membr. Sci. 272 (2006) 50–57. [32] Suryani, Y.-L. Liu, J. Membr. Sci. 332 (2009) 121–128.', 'H. Namazi, H. Ahmadi / Journal of Power Sources 196 (2011) 2573–25832583', '[33] K.D. Kreuer, Chem. Mater. 8 (1996) 610–641. [34] D.S. Kim, B. Liu, M.D. Guiver, Polymer 47 (2006) 7871–7880. [35] Y.L. Ma, J.S. Wainright, M.H. Litt, R.F. Savinell, J. Electrochem. Soc. 151 (2004) A8–A16. [36] R. He, Q. Li, J.O. Jensen, N.J. Bjerrum, J. Polym. Sci., Part A: Polym. Chem. 45 (2007) 2989–2997.', '[37] S.Md. Mominul Alam, T. Agag, T. Kawauchi, T. Takeichi, React. Funct. Polym. 67 (2007) 1218–1224. [38] G. Ramorino, F. Bignotti, S. Pandini, T. Riccò, Compos. Sci. Technol. 69 (2009) 1206–1211. [39] C. Parka, J.G. Smith Jr., J.W. Connell, S.E. Lowtherb, D.C. Working, E.J. Siochi, Polymer 46 (2005) 9694–9701.']
2025-02-19 12:41:36 - WARNING: done parsing pdf
2025-02-19 12:41:36 - WARNING: start ner pdf
2025-02-19 12:41:36 - INFO: Loading Data
2025-02-19 12:41:36 - WARNING: 2025-02-19 12:41:36 - INFO: Loading Data
2025-02-19 12:41:40 - WARNING: Predicting NER ...
2025-02-19 12:41:41 - WARNING: Finished predicting.
2025-02-19 12:41:41 - WARNING: Converting to Brat format...
2025-02-19 12:41:41 - WARNING: # of discontinuous mentions:
2025-02-19 12:41:41 - WARNING:  
2025-02-19 12:41:41 - WARNING: 13
2025-02-19 12:41:41 - WARNING: Finished.
2025-02-19 12:41:41 - WARNING: start predict re
2025-02-19 12:41:41 - WARNING: Example:   0%|          | 0/55 [00:00<?, ?it/s]
2025-02-19 12:41:42 - WARNING: Example:  20%|##        | 11/55 [00:00<00:00, 108.04it/s]
2025-02-19 12:41:42 - WARNING: Example:  62%|######1   | 34/55 [00:00<00:00, 177.70it/s]
2025-02-19 12:41:42 - WARNING: Example: 100%|##########| 55/55 [00:00<00:00, 182.32it/s]
2025-02-19 12:41:42 - WARNING: Example: 100%|##########| 55/55 [00:00<00:00, 174.10it/s]
2025-02-19 12:41:42 - WARNING: # of documents 55.
2025-02-19 12:41:42 - WARNING: # of positive examples 0.
2025-02-19 12:41:42 - WARNING: # of negative examples 11188.
2025-02-19 12:41:43 - WARNING: dict_keys(['11', '13', '15', '19', '22', '24', '26', '28', '30', '33', '36', '37', '40', '41', '43', '44', '45', '47', '51', '55', '56', '58', '60', '63', '66', '68', '70', '71', '72', '75', '77', '78', '81', '85', '87', '89', '91', '94'])
2025-02-19 12:41:43 - WARNING: done predict re
2025-02-19 12:41:43 - WARNING: model output text 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: Hassan Namazi ∗ , Hossein Ahmadi 
2025-02-19 12:41:43 - WARNING: len of model_output_text 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: 33
2025-02-19 12:41:43 - WARNING: original_text 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: Hassan Namazi ∗, Hossein Ahmadi
2025-02-19 12:41:43 - WARNING: mapping dict 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 30}
2025-02-19 12:41:43 - WARNING: text: 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: Hassan Namazi ∗ , Hossein Ahmadi 
2025-02-19 12:41:43 - WARNING: origin bbox 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: {'x1': 210.01559448242188, 'y1': 223.78575134277344, 'x2': 213.31044006347656, 'y2': 236.84896850585938, 'width': 595.300048828125, 'height': 793.7000122070312, 'pageNumber': 1}
2025-02-19 12:41:43 - WARNING: normalized bbox 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: {'x1': 210.01559448242188, 'y1': 223.78575134277344, 'x2': 213.31044006347656, 'y2': 236.84896850585938, 'width': 595.300048828125, 'height': 793.7000122070312, 'pageNumber': 1}
2025-02-19 12:41:43 - WARNING: finalized bbox 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: {'x1': 210.01559448242188, 'y1': 223.78575134277344, 'x2': 213.31044006347656, 'y2': 236.84896850585938, 'width': 595.300048828125, 'height': 793.7000122070312, 'pageNumber': 1}
2025-02-19 12:41:43 - WARNING:  final bouding box 
2025-02-19 12:41:43 - WARNING:  
2025-02-19 12:41:43 - WARNING: {'x1': 42.731807708740234, 'y1': 223.7856903076172, 'x2': 213.31044006347656, 'y2': 241.25039672851562, 'width': 595.300048828125, 'height': 793.7000122070312, 'pageNumber': 1}
2025-02-19 12:41:43 - WARNING: done save re and ner, 
2025-02-19 12:41:43 - WARNING: start count num_rel 
2025-02-19 12:41:43 - WARNING: done count num_rel 
2025-02-19 12:41:43 - WARNING: start update document 
2025-02-19 12:41:44 - WARNING: start matching infor
2025-02-19 12:41:44 - WARNING: done matching infor
2025-02-19 12:41:44 - WARNING: start commit
2025-02-19 12:41:44 - WARNING: done update document 
2025-02-19 12:41:48 - WARNING: Email sent successfully!
2025-02-19 12:41:48 - INFO: Task dev_tasks.process_pdf_task[09437aad-45de-4971-93b7-45677d9c06e2] succeeded in 19.395553059875965s: {'id': 74252092, 'filename': 'namazi2011.pdf', 'upload_time': '2025/02/19, 03:41:28', 'entities': 891, 'relations': 285, 'pages': 11, 'status': 'completed'}
2025-02-19 12:41:48 - WARNING: 2025-02-19 12:41:48 - INFO: Task dev_tasks.process_pdf_task[09437aad-45de-4971-93b7-45677d9c06e2] succeeded in 19.395553059875965s: {'id': 74252092, 'filename': 'namazi2011.pdf', 'upload_time': '2025/02/19, 03:41:28', 'entities': 891, 'relations': 285, 'pages': 11, 'status': 'completed'}
2025-02-19 13:03:44 - INFO: Task dev_tasks.process_pdf_task[d37b03e6-dc81-4078-891c-092258fb76ff] received
2025-02-19 13:03:44 - WARNING: 2025-02-19 13:03:44 - INFO: Task dev_tasks.process_pdf_task[d37b03e6-dc81-4078-891c-092258fb76ff] received
2025-02-19 13:03:44 - WARNING: uploads/_KSE_2024__Semnatic_Enchancement_Spoken_Language_Understanding (1).pdf
2025-02-19 13:03:44 - WARNING: start parsing pdf
2025-02-19 13:03:44 - WARNING: parsed 169 paragraphs
2025-02-19 13:03:44 - WARNING: ['ConSLU: Constrained Decoding for EnhancedSpoken Language Understanding in JointEnd-to-End Models', 'Dinh-Truong Do, Minh-Phuong Nguyen, Le-Minh Nguyen Japan Advanced Institute of Science and Technology, Japan {truongdo, phuongnm, nguyenml}@jaist.ac.jp', 'Abstract—Spoken language understanding (SLU) commonlyemploys cascading systems, integrating Automatic Speech Recog-nition (ASR) and Natural Language Understanding (NLU) mod-ules. However, these systems often suffer from information loss,latency, and high costs, prompting significant research interestin end-to-end (E2E) SLU. Among E2E methods, joint modelshave emerged as particularly effective in terms of latency andaccuracy. However, prior works on joint E2E SLU often representthe output logical form as a sequence of tokens, lacking aguarantee of producing a correct logical form. In this study, weenhance the joint E2E SLU approach by simplifying the outputsequence and constraining the decoding process to focus oncandidate tokens. Specifically, we categorize tokens in the logicalform into label tokens and normal tokens, applying constrainedcandidates for each token type. Through experiments on theSTOP dataset, our method outperforms previous works (by 1.44exact match improvement compared to the baseline), achievinga 78% exact match score, demonstrating its effectiveness.Index Terms—Spoken language understanding, Constraineddecoding, Joint models', 'I. INTRODUCTION', 'The importance of spoken language understanding (SLU) invirtual assistant technologies, as evidenced by platforms likeSiri, Alexa, and Google Assistant [1], [11]. Previous researchhas explored two primary methodologies in SLU: the cascadesystem, integrating ASR and NLU separately, and the end-to-end (E2E) approach, directly inferring semantic meaningfrom audio inputs. Given the prevalent deployment of virtualassistants on resource-constrained devices, E2E methodologieshave gained popularity due to their efficiency in latency andinformation retention [12]. In E2E SLU system design, there are four main architecturalapproaches [5], [9]: (1) direct models, which learn semanticrepresentations directly from input audio without utterancetranscription; (2) joint models, which predict ASR transcriptsand semantic representations simultaneously using a shareddecoder; (3) multitask models, which employ separate de-coders for ASR and NLU tasks while sharing an acousticencoder; and (4) multistage models, resembling traditionalcascading pipelines with distinct encoder-decoder structuresfor ASR and NLU, interconnected through gradient propaga-tion. Among these, direct models show less promising resultscompared to others [9], indicating the crucial role of predictingASR transcripts in deriving semantic forms due to the lack of', 'information for logical label prediction. Among other architec-tures, joint E2E models offer the most promising approach asthey achieve comparable results without increasing model sizeby adding extra components for ASR prediction, making themfeasible for on-device settings. This paper adopts the jointE2E model, employing constrained decoding and simplifyingoutput sequences to enhance spoken language understandingtasks.Traditionally, most prior studies employing the joint E2Earchitecture have utilized autoregressive (AR) models built onthe Transformer architecture [15]. In this setup, the decodingphase sequentially predicts tokens for both ASR and its logicalform, typically starting with ASR tokens before proceeding tological form tokens for better alignment. However, previousapproaches treat the logical form as a sequence of tokens,necessitating consideration of the entire vocabulary at eachstep, which can lead to mislabeling and invalid logical form.For instance, when predicting a label token under the intent IN:CREATE_ALARM, it’s unnecessary to consider the entirevocabulary; only candidate tokens relevant to the slots withinthe intent IN:CREATE_ALARM, such as SL:DATE_TIME, SL:DURATION, and others, need to be considered (Figure 1).Similarly, when predicting normal tokens, such as the 13thdecoding step in Figure 1, the model do not need to considerthe entire vocabulary; only tokens present in the ASR scriptare relevant. To address these issues, we propose a constrainedmethodology for training and inference in spoken languageunderstanding. Drawing inspiration from prior work usinggrammar to constrain decoding steps [2], [3], we first extractgrammar from the annotated training data to aid in predictinglabel tokens. This grammar informs the possible label tokensbased on the parent node label. For example, if the currentdecoding output is SL:DATE_TIME and the parent node is IN:CREATE_ALARM, then, according to the grammar, thecandidate label tokens should be possible slots for the intent IN:CREATE_ALARM: [SL:DATE_TIME, SL:DURATION, SL:AMOUNT, ...]. For normal tokens, we constrain the modelto predict only those tokens present in the ASR script. Duringtraining, we mask all tokens not in the candidate set, allowingthe model to focus on relevant tokens for better predictions.Additionally, unlike previous work [16], which employs mul-titask sequence spoken language understanding and representsoutput in a structured but challenging-to-predict JSON format,', 'ConSLU Acoustic', 'Encoder', 'ConSLU Constrained', 'Decoder', 'I need to wake up at 6 am | [IN:CREATE_ALARM', '[SL:DATE_TIME at 6 am ] ]', 'audio.wav', 'SemSLU Constrained Decoder', '......', 'Vocab. Prob.', '......', 'Vocab. Prob.', 'X', '[ "SL:DATE_TIME",', '"SL:DURATION",', '"SL:AMOUNT",... ]', 'SOTam|[...', 'SL:DATE_TIME', 'Pruned', 'tokens', 'by', 'Candidates', 'XX', 'SemSLU Constrained Decoder', 'at', 'Decoding Step 12:', 'Label Token', '......', '......', 'Vocab. Prob.', '......', 'Vocab. Prob.', 'XXXXXX', 'Decoding Step 13:', 'Normal Token', 'Predicted', 'Token', 'Label Token Candidates:', 'I[IN:CREATE_ALARM', 'SOT...I[IN:CREATE_ALARMSL:DATE_TIME', '[ "[", "]", "I", "need", "to",', '"wake", "up", "at", "6",', '"am" ]', 'Normal Token', 'Candidates:', 'Fig. 1. Overview of our method', 'we hypothesize that not all multitask tasks are necessary, andthe JSON format is ineffective as it is not consistent with thepre-trained task of pre-trained speech recognition models likeWhisper [10]. Therefore, we simplify the output sequence tocontain only two tasks: ASR and NLU, separated by a specialtoken ”|”. Our contributions in this study are threefold:', '• We propose a constrained decoding approach to enhancethe performance of joint E2E SLU systems.', '• We demonstrate the effectiveness of simplifying the out-put sequence of logical form, which enhances perfor-mance compared to the structured yet challenging-to-predict JSON format.', '• Through experimentation, we showcase the efficacy ofour framework, surpassing current state-of-the-art jointE2E SLU methods with a 1.44 EM score improvement.', 'II. RELATED WORKS', 'The STOP dataset [14] is a well-known benchmark forassessing SLU system performance. This dataset adopts ahierarchical representation [4] for logical forms, enablingthe inclusion of multiple intents within a single utterance.However, this structure also poses a challenge for models toaccurately predict the correct logical form. To address thischallenge, various approaches have been proposed. Kim et al.[8] introduced a method to enhance ASR error robustness byintegrating audio and text representations based on estimatedmodality confidence of ASR hypotheses. Istaiteh et al. [7]employed a pre-trained HuBERT model [6] as an encoderalongside a transformer decoder with layer-drop and ensemblelearning for decoding. Zhang et al. [17] proposed a two-stageapproach for the SLU task. In the first stage, models based onencoder-decoder structures recognize speech utterances into', 'text, while in the second stage, BERT with Conditional Ran-dom Field (CRF) and Byte Pair Encoding (BPE) are utilizedfor intent determination and slot filling in the SLU process.Wang et al. [16] introduced a sequence-level multitask learningparadigm, prioritizing tasks based on semantic complexity andconcatenating their labels into a formatted JSON sequence fordirect model learning. This approach facilitates smoother tasktransfer learning and enhances the main task’s performance byleveraging auxiliary task predictions. In contrast, our methodintroduces a novel approach to constrain the decoding stepof the spoken language understanding task. This approachensures the model returns the correct logical form and en-hances learning during training by focusing solely on thecandidate set, while also improving inference by disregardingunpromising tokens.', 'III. METHODOLOGY', 'Figure 1 provides an overview of our methodology. Given anaudio file, our model predicts both the ASR script and logicalform directly. To accomplish this, the acoustic encoder firstencodes the audio into audio embeddings. Using a constraineddecoder, tokens are predicted step-by-step during decoding.Specifically, for each predicted token in the logical form, theprobability distribution of the next tokens is calculated asusual. Subsequently, a candidate set is generated containingall possible tokens for that step. We then prune the vocabularyprobability distribution, retaining only tokens present in thecandidate set. The token with the highest probability from thepruned vocabulary distribution is selected as the next predictedtoken. Furthermore, the output sequence contains only theASR script and logical form separated by a special token,facilitating fine-tuning steps to closely resemble the pre-trainedtask of models.', 'A. Output sequence simplified', 'Previous research [16] demonstrates the effectiveness oftraining models for spoken language understanding alongsideother tasks, such as speaker gender classification, speakernative-ness classification, ASR, and domain classification, inimproving SLU task performance. However, results from thesestudies also suggest that not all tasks are equally importantfor SLU. Therefore, we hypothesize that only the ASR task isnecessary to enhance SLU performance. Based on this hypoth-esis, we simplified the output sequence text from multitask toinclude only two tasks: ASR and SLU. Additionally, insteadof considering the sequence output as a well-structured butchallenging-to-predict format like JSON, we simplified it byseparating tasks with a special token ”|”, making the fine-tuning task more similar to the pre-trained task of pre-trainedmodels. An example is illustrated in Figure 2.', '<|startoftranscript|> {              "Gender": "Female",              "Native": "Yes",              "Text": "please pause the song",              "Domain": "music",              "Intent": "[IN:PAUSE_MUSIC [SL:MUSIC_TYPE song ] ]" } <|endoftext|>', 'JSON-style Output Sequence Format for Multi-tasking', '<|startoftranscript|> please pause the song | [IN:PAUSE_MUSIC [SL:MUSIC_TYPE song ] ] <|endoftext|>', 'Simpilified Output Sequence', 'Fig. 2. An example of simplified output sequence', 'B. Constrained Decoding', 'The output sequence consists of two parts: the ASR scriptand the SLU logical form. For the ASR script, decodingproceeds normally using the audio embedding and the outputof the previous decoding step. Upon completing the ASR scriptgeneration, we initiate SLU logical form generation, wherewe propose the use of constrained decoding. The problemwith normal decoding when generating the logical form isthat it does not guarantee a valid logical form and insteadof considering the entire vocabulary, it is more efficient toconsider only a small set of candidates at each decoding step.Therefore, we propose to use constrained decoding. Specifically, there are two types of tokens in the hierarchicalrepresentation of the logical form: label tokens and normaltokens. We apply constrained candidates for each type oftoken.', '• Label token: Following previous work [2], [3], weextract label grammar using training annotated data. Thisgrammar serves as a candidate generator, providing a listof label candidates given the parent label of the currentstep. After obtaining the grammar, during the label tokendecoding step, we compute the vocabulary distribution', 'probability for the next token as usual. We then generatethe candidate set using this grammar. Subsequently, weprune all tokens in the vocabulary probability distributionthat do not exist in the candidate set. The token with thehighest probability in the pruned vocabulary probabilitydistribution is selected as the predicted token (referred toas decoding step 12 in Figure 1).', '• Normal token: For normal tokens, we constrain the nexttokens to be tokens in the ASR script. This means that weextract all tokens in the predicted ASR script and set thecandidate of the normal token to be in this set (referredto as decoding step 13 in Figure 1). Additionally, we addtwo special tokens ”[”, ”]” to the candidates, which definethe structure of the logical form. Training:Formathematicalmodeling,givenanaudiospeech A, our model needs to predict the output Y= [y1, ..., ym, ym+1, ..., yn] where YASR = [y1, ..., ym] is theASR script and YSLU = [ym+1, ..., yn] is the output logi-cal form. The loss function is defined as the negative log-likelihood of the true tokens in the output sequence, and it isdefined as follows:', 'LossASR = −', 'm�', 't=1log', '�ezt', '�Vv=1 ezt,v', '�', '(1)', 'LossSLU = −', 'n�', 't=m+1log', '�ezt', '�Cc=1 ezt,c', '�', '(2)', 'Loss = LossASR + LossSLU(3)', 'Here, V denotes the vocabulary, C represents the candidatesfor the current predicted logical form token, where C ⊂V. Inthis context, zt stands for the pre-softmax logit for the truetoken at time step t, and zt,c represents the pre-softmax logitfor token v at time step t.Inference: In the inference step, one challenge is determiningwhether the current token should be the label token or thenormal token. We propose a simple solution based on theresult of previous tokens in the logical form. If the lastpredicted token is ”[”, the next token should be the labeltoken; otherwise, it is the normal token. Additionally, whenpredicting the normal token, the token should come from leftto right of the ASR script. Therefore, if a token is selectedas the normal token for the current step, the candidates forthe next normal token step can only be to the right of thistoken in the ASR script. Inference continues until semanticform validation, where each open bracket token correspondsto a corresponding close bracket.', 'IV. EXPERIMENT', 'A. Dataset and Evaluation Metric', 'To assess the efficacy of our approach, we conducted exper-iments on the widely recognized spoken language understand-ing dataset STOP [14]. This dataset employs a hierarchicalrepresentation [4] to represent logical forms, allowing for theexpression of utterances with multiple intents and nested slots.', 'TABLE IMAIN RESULTS OF OUR METHODS WITH PREVIOUS WORKS ON STOP TEST SET.', 'MethodPre-trained ModelEM (%)EM-Tree (%)', 'wav2vec2.0 + Transformer Decoder [13]wav2vec 2.068.7082.78', 'HuBERT + Transformer Decoder [13]HuBERT69.2382.87', 'Cascade system [13]HuBERT+BART72.3682.78', 'WhiSLU [16]Whisper-large74.4984.89', 'WhiSLU-SML [16]Whisper-large76.6886.37', '(Our methods)', 'ConSLU-largeWhisper-large77.3887.37', 'ConSLU-large-v2Whisper-large-v278.1287.63', 'Covering a diverse range of SLU domains, STOP offers arich variety of scenarios, comprising 82 distinct intents and84 different slots. This diversity enables a comprehensiveevaluation of our model’s SLU capabilities. Consistent withprior research [16], we utilized the full-resource version ofSTOP, which encompasses 120k training, 33K development,and 75K test examples.In line with previous studies [16], our primary evaluationmetric was Exact Match (EM), where a score of 1 indicates acomplete match between all predicted tokens and the groundtruth sequence, and 0 otherwise. Additionally, we employedEM-tree as a secondary metric, which assesses whether thereturned logical form correctly matches the semantic schematree, albeit with potential inaccuracies in the predicted spans.', 'B. Experimental Settings', 'As our backbone models, we employed the pre-trainedspeech recognition Whisper model [10], utilizing various sizesincluding whisper-tiny, whisper-base, whisper-small, whisper-medium, and whisper-large. Initially, we ran each experimentalhyperparameter setting on the whisper-base model to deter-mine optimal values. Hyperparameters for other pre-trainedmodels were kept consistent with those of the whisper-base.Our experimentation proceeded by first fine-tuning on thevalidation set to select the best hyperparameters, followed byevaluation on the test set to obtain final results. Specifically,we fine-tuned our models on the STOP dataset for {1, 3, 5,10}1 epochs, using learning rates of {3e-05, 1e-05, 5e-05},5000 steps of warmup, and a batch size of 16. Training on thewhisper-base model with one GPU A100 took approximately2 hours.Baseline. To establish a strong baseline, we reproduced theWhiSLU model [16], adhering to the hyperparameters speci-fied in the original papers.', 'C. Main Results', 'Table I shows the results of our methods with previousworks on the test set of the STOP dataset. The results indicatethat our best models outperform the top-performing methodamong joint E2E SLU models, WhiSLU [16]. Specifically,', '1Values in bold denote the best performance.', 'when comparing methods utilizing the same pre-trained model,whisper-large, our ConSLU approach exhibits superior per-formance to WhiSLU by 0.7 EM score. This highlights theeffectiveness of simplifying the output sequence and em-ploying constrained decoding in our method. Furthermore,utilizing the whisper-large-v2 model in our approach furtherenhances performance by 0.74 EM score. This underscoresthe scalability of our method to the efficacy of pre-trainedlanguage models; with improved pre-trained models, we canachieve superior performance when employing our method.', 'V. ANALYSIS', 'To gain deeper insights and analyze our proposed frame-work extensively, we conducted a comprehensive analysis.', 'A. Effect of model sizes', 'Table II presents the results obtained with different pa-rameter sizes of Whisper. Larger LLM models consistentlydemonstrate better performance, with the whisper-large-v2model achieving the best results, followed by the whisper-large and whisper-medium models. Notably, smaller modelsalso prove effective. For instance, despite being only one-forty the size of whisper-large-v2, Whisper-tiny still achieves90% of the larger model’s performance. Similarly, Whisper-base, which is 20 times smaller than whisper-large-v2, attains93% of its performance. This suggests that model size canbe chosen based on specific computational constraints withoutsignificant performance trade-offs.', 'TABLE IIIMPACT OF LLM SIZE', 'MethodParametersEM (%)EM-Tree (%)', 'whisper-tiny39 M70.0482.14', 'whisper-base74 M72.5183.89', 'whisper-small244 M75.0285.52', 'whisper-medium769 M76.9886.54', 'whisper-large1550 M77.3887.37', 'whisper-large-v21550 M78.1287.63', 'B. Ablation Study', 'To evaluate the effect of each component in our framework,we compared the model’s performance for each combinationof component settings with that of the WhiSLU baseline modelon the STOP dataset (Table III). Specifically, when utilizingthe same pre-trained model whisper-base, our ConSLU methodoutperforms WhiSLU by 2.32 EM score, demonstrating theeffectiveness of simplifying the output sequence and employ-ing constrained decoding in our method. Using constraintsled to improved performance on the STOP dataset (EM score1.16 points higher than with the baseline model). Additionally,simplifying the sequence outputs improved the EM scorecompared with the baseline (0.98 points higher), highlightingthe effectiveness of the two proposed points in our paper.', 'TABLE IIIABLATION STUDY', 'MethodEM-Tree (%)EM (%)∆(EM)', 'SemSLU-base83.8972.51', '- w/o constrained82.7971.35-1.16', '- w/o simplified82.8871.17-1.34', 'Baseline81.7570.19-2.32', 'C. Logical Form Validation', 'In this section, we compare the validity of the semanticrepresentations generated by our approach and baseline ap-proaches (Table IV). Unlike our approach, which generatesperfectly valid trees when trained on the STOP dataset,WhiSLU struggles to achieve perfect validity. This demon-strates that the WhiSLU model requires larger model abilitiesto learn the structure of semantic representation to generate avalid logical form.', 'TABLE IVTHE ACCURACY OF RETURNING A VALID LOGICAL FORM OF BASELINE AND OUR METHOD CONSLU.', 'MethodPre-trained ModelLogical Form Validation', 'WhiSLUWhisper-base99.59', 'ConSLUWhisper-base100.00', 'D. Case Study', 'Table V presents several examples outputted from ourmodel and the baseline. In the first example, our modelpredicted the slot SL:PERSON_REMINDED after the in-tent IN:DELETE_REMINDER, whereas the baseline modelpredictedtheslotSL:ATTENDEE.Thisdifferenceoc-curred because the extracted grammar does not contain theconstraint (IN:DELETE_REMINDER => SL:ATTENDEE).This demonstrates the effectiveness of using constrained de-coding in our method. In the second example, the baselinecorrectly predicts the ASR script but fails to predict the logicalform by predicting the wrong normal token ”gabbage,” while', 'our model predicts the output correctly. This demonstrates theeffectiveness of considering only tokens in the ASR scriptwhen predicting the normal token in the logical form. Thefinal example is an instance where both our model and thebaseline fail to return the correct output, predicting incorrectASR scripts, leading to incorrect logical forms. This suggeststhat enhancing the ASR transcript can further improve theperformance of our framework for future work.', 'CONCLUSION', 'In conclusion, our study delves into the realm of spokenlanguage understanding (SLU), focusing on the joint E2Earchitecture, a pivotal methodology in virtual assistant tech-nologies. We observe that while previous works predominantlyadopt autoregressive (AR) models using Transformer archi-tecture, the decoding phase of such models often encounterschallenges in accurately predicting logical forms due to theconsideration of the entire vocabulary at each step. To addressthis, we introduce a constrained decoding approach, leveraginggrammar extraction to constrain the prediction space, therebyenhancing the accuracy of logical form predictions whilereducing candidate size during token prediction. Additionally,we simplify the output sequence of logical forms to only con-tain essential information, departing from the intricate JSONstructure typically used. Through experimentation, our frame-work demonstrates superior performance, surpassing currentstate-of-the-art joint E2E SLU methods by achieving a notable1.44 EM score improvement. In essence, our contributionspave the way for more efficient and accurate spoken languageunderstanding in virtual assistant technologies, offering apromising avenue for future research and development in thisdomain.', 'REFERENCES', '[1] Jerome R Bellegarda. Spoken language understanding for natural inter-action: The siri experience. Natural Interaction with Robots, Knowbotsand Smartphones: Putting Spoken Dialog Systems into Practice, pages3–14, 2013.', '[2] Dinh-Truong Do, Minh-Phuong Nguyen, and Le-Minh Nguyen. Gram:Grammar-based refined-label representing mechanism in the hierarchicalsemantic parsing task. In International Conference on Applications ofNatural Language to Information Systems, pages 339–351. Springer,2023.', '[3] Truong Do, Phuong Nguyen, and Minh Nguyen.Structsp: Efficientfine-tuning of task-oriented dialog system by using structure-awareboosting and grammar constraints. In Findings of the Association forComputational Linguistics: ACL 2023, pages 10206–10220, 2023.', '[4] Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Kumar, and MikeLewis.Semantic parsing for task oriented dialog using hierarchicalrepresentations. arXiv preprint arXiv:1810.07942, 2018.', '[5] Parisa Haghani, Arun Narayanan, Michiel Bacchiani, Galen Chuang,Neeraj Gaur, Pedro Moreno, Rohit Prabhavalkar, Zhongdi Qu, andAustin Waters.From audio to semantics: Approaches to end-to-endspoken language understanding.In 2018 IEEE Spoken LanguageTechnology Workshop (SLT), pages 720–726. IEEE, 2018.', '[6] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakho-tia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction ofhidden units. IEEE/ACM Transactions on Audio, Speech, and LanguageProcessing, 29:3451–3460, 2021.', 'TABLE VCOMPARISON OF OUTPUTS OF BASELINE (WHISLU) AND OUR CONSLU MODEL ON THE STOP DATASET.', 'TypeOuput', 'Ground-TruthASR: Please delete the movie reminder from the office groupLogical form: [IN:DELETE REMINDER [SL:TODO movie ] [SL:PERSON REMINDED office ] ]', 'BaselineASR: Please delete the movie reminder from the office groupLogical form: [IN:DELETE REMINDER [SL:TODO movie ] [SL:ATTENDEE office ] ] �', 'ConSLUASR: Please delete the movie reminder from the office groupLogical form: [IN:DELETE REMINDER [SL:TODO movie ] [SL:PERSON REMINDED office ] ] �', 'Ground-TruthASR: Remind me to put garbage outsideLogical form: [IN:CREATE REMINDER [SL:PERSON REMINDED me ] [SL:TODO garbage outside ] ]', 'BaselineASR: Remind me to put garbage outsideLogical form: [IN:CREATE REMINDER [SL:PERSON REMINDED me ] [SL:TODO gabbage outside ] ] �', 'ConSLUASR: Remind me to put garbage outsideLogical form: [IN:CREATE REMINDER [SL:PERSON REMINDED me ] [SL:TODO garbage outside ] ] �', 'Ground-TruthASR: send happy birthday to jerilynLogical form: [IN:SEND MESSAGE [SL:CONTENT EXACT happy birthday [SL:RECIPIENT jerilyn ] ]', 'BaselineASR: send happy birthday to jerrylennLogical form: [IN:SEND MESSAGE [SL:CONTENT EXACT happy birthday [SL:RECIPIENT jerrylenn ] ] �', 'ConSLUASR: send happy birthday to jerrylandLogical form: [IN:SEND MESSAGE [SL:CONTENT EXACT happy birthday [SL:RECIPIENT jerryland ] ] �', '[7] Othman Istaiteh, Yasmeen Kussad, Yahya Daqour, Maria Habib, Mo-hammad Habash, and Dhananjaya Gowda.A transformer-based e2eslu model for improved semantic parsing. In ICASSP 2023-2023 IEEEInternational Conference on Acoustics, Speech and Signal Processing(ICASSP), pages 1–2. IEEE, 2023.', '[8] Suyoun Kim, Akshat Shrivastava, Duc Le, Ju Lin, Ozlem Kalinli, andMichael L Seltzer. Modality confidence aware training for robust end-to-end spoken language understanding. arXiv preprint arXiv:2307.12134,2023.', '[9] Mohan Li and Rama Doddipatla.Non-autoregressive end-to-end ap-proaches for joint automatic speech recognition and spoken languageunderstanding. In 2022 IEEE Spoken Language Technology Workshop(SLT), pages 390–397. IEEE, 2023.', '[10] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, ChristineMcLeavey, and Ilya Sutskever. Robust speech recognition via large-scaleweak supervision. In International Conference on Machine Learning,pages 28492–28518. PMLR, 2023.', '[11] Subendhu Rongali, Beiye Liu, Liwei Cai, Konstantine Arkoudas, Cheng-wei Su, and Wael Hamza. Exploring transfer learning for end-to-endspoken language understanding. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 35, pages 13754–13761, 2021.', '[12] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Kumar,Baiyang Liu, and Yoshua Bengio. Towards end-to-end spoken languageunderstanding. In 2018 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 5754–5758. IEEE, 2018.', '[13] Paden Tomasello, Akshat Shrivastava, Daniel Lazar, Po-Chun Hsu, DucLe, Adithya Sagar, Ali Elkahky, Jade Copet, Wei-Ning Hsu, Yossi Adi,Robin Algayres, Tu Ahn Nguyen, Emmanuel Dupoux, Luke Zettle-moyer, and Abdelrahman Mohamed. Stop: A dataset for spoken taskoriented semantic parsing. In 2022 IEEE Spoken Language TechnologyWorkshop (SLT), pages 991–998, 2023.', '[14] Paden Tomasello, Akshat Shrivastava, Daniel Lazar, Po-Chun Hsu, DucLe, Adithya Sagar, Ali Elkahky, Jade Copet, Wei-Ning Hsu, Yossi Adi,et al. Stop: A dataset for spoken task oriented semantic parsing. In2022 IEEE Spoken Language Technology Workshop (SLT), pages 991–998. IEEE, 2023.', '[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, LlionJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attentionis all you need. Advances in neural information processing systems, 30,2017.', '[16] Minghan Wang, Yinglu Li, Jiaxin Guo, Xiaosong Qiao, Zongyao Li,Hengchao Shang, Daimeng Wei, Shimin Tao, Min Zhang, and Hao Yang.Whislu: End-to-end spoken language understanding with whisper. InProc. Interspeech, volume 2023, pages 770–774, 2023.', '[17] Gaosheng Zhang, Shilei Miao, Linghui Tang, and Peijia Qian. A two-stage system for spoken language understanding.In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and SignalProcessing (ICASSP), pages 1–2. IEEE, 2023.']
2025-02-19 13:03:44 - WARNING: done parsing pdf
2025-02-19 13:03:44 - WARNING: start ner pdf
2025-02-19 13:03:45 - INFO: Loading Data
2025-02-19 13:03:45 - WARNING: 2025-02-19 13:03:45 - INFO: Loading Data
2025-02-19 13:03:48 - WARNING: Predicting NER ...
2025-02-19 13:03:49 - WARNING: Finished predicting.
2025-02-19 13:03:49 - WARNING: Converting to Brat format...
2025-02-19 13:03:49 - WARNING: # of discontinuous mentions:
2025-02-19 13:03:49 - WARNING:  
2025-02-19 13:03:49 - WARNING: 0
2025-02-19 13:03:49 - WARNING: Finished.
2025-02-19 13:03:49 - WARNING: start predict re
2025-02-19 13:03:50 - WARNING: Example:   0%|          | 0/4 [00:00<?, ?it/s]
2025-02-19 13:03:50 - WARNING: Example: 100%|##########| 4/4 [00:00<00:00, 536.99it/s]
2025-02-19 13:03:50 - WARNING: # of documents 4.
2025-02-19 13:03:50 - WARNING: # of positive examples 0.
2025-02-19 13:03:50 - WARNING: # of negative examples 8.
2025-02-19 13:03:50 - WARNING: dict_keys([])
2025-02-19 13:03:50 - WARNING: done predict re
2025-02-19 13:03:50 - WARNING: model output text 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: information for logical label prediction . Among other architec - tures , joint E2E models offer the most promising approach asthey achieve comparable results without increasing model sizeby adding extra components for ASR prediction , making themfeasible for on - device settings . This paper adopts the jointE2E model , employing constrained decoding and simplifyingoutput sequences to enhance spoken language understandingtasks.Traditionally , most prior studies employing the joint E2Earchitecture have utilized autoregressive ( AR ) models built onthe Transformer architecture [ 15 ] . In this setup , the decodingphase sequentially predicts tokens for both ASR and its logicalform , typically starting with ASR tokens before proceeding tological form tokens for better alignment . However , previousapproaches treat the logical form as a sequence of tokens , necessitating consideration of the entire vocabulary at eachstep , which can lead to mislabeling and invalid logical form.For instance , when predicting a label token under the intent IN : CREATE_ALARM , it ’s unnecessary to consider the entirevocabulary ; only candidate tokens relevant to the slots withinthe intent IN : CREATE_ALARM , such as SL : DATE_TIME , SL : DURATION , and others , need to be considered ( Figure 1 ) . Similarly , when predicting normal tokens , such as the 13thdecoding step in Figure 1 , the model do not need to considerthe entire vocabulary ; only tokens present in the ASR scriptare relevant . To address these issues , we propose a constrainedmethodology for training and inference in spoken languageunderstanding . Drawing inspiration from prior work usinggrammar to constrain decoding steps [ 2 ] , [ 3 ] , we first extractgrammar from the annotated training data to aid in predictinglabel tokens . This grammar informs the possible label tokensbased on the parent node label . For example , if the currentdecoding output is SL : DATE_TIME and the parent node is IN : CREATE_ALARM , then , according to the grammar , thecandidate label tokens should be possible slots for the intent IN : CREATE_ALARM : [ SL : DATE_TIME , SL : DURATION , SL : AMOUNT , ... ] . For normal tokens , we constrain the modelto predict only those tokens present in the ASR script . Duringtraining , we mask all tokens not in the candidate set , allowingthe model to focus on relevant tokens for better predictions.Additionally , unlike previous work [ 16 ] , which employs mul - titask sequence spoken language understanding and representsoutput in a structured but challenging - to - predict JSON format , 
2025-02-19 13:03:50 - WARNING: len of model_output_text 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: 2584
2025-02-19 13:03:50 - WARNING: original_text 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: information for logical label prediction. Among other architec-tures, joint E2E models offer the most promising approach asthey achieve comparable results without increasing model sizeby adding extra components for ASR prediction, making themfeasible for on-device settings. This paper adopts the jointE2E model, employing constrained decoding and simplifyingoutput sequences to enhance spoken language understandingtasks.Traditionally, most prior studies employing the joint E2Earchitecture have utilized autoregressive (AR) models built onthe Transformer architecture [15]. In this setup, the decodingphase sequentially predicts tokens for both ASR and its logicalform, typically starting with ASR tokens before proceeding tological form tokens for better alignment. However, previousapproaches treat the logical form as a sequence of tokens,necessitating consideration of the entire vocabulary at eachstep, which can lead to mislabeling and invalid logical form.For instance, when predicting a label token under the intent IN:CREATE_ALARM, it’s unnecessary to consider the entirevocabulary; only candidate tokens relevant to the slots withinthe intent IN:CREATE_ALARM, such as SL:DATE_TIME, SL:DURATION, and others, need to be considered (Figure 1).Similarly, when predicting normal tokens, such as the 13thdecoding step in Figure 1, the model do not need to considerthe entire vocabulary; only tokens present in the ASR scriptare relevant. To address these issues, we propose a constrainedmethodology for training and inference in spoken languageunderstanding. Drawing inspiration from prior work usinggrammar to constrain decoding steps [2], [3], we first extractgrammar from the annotated training data to aid in predictinglabel tokens. This grammar informs the possible label tokensbased on the parent node label. For example, if the currentdecoding output is SL:DATE_TIME and the parent node is IN:CREATE_ALARM, then, according to the grammar, thecandidate label tokens should be possible slots for the intent IN:CREATE_ALARM: [SL:DATE_TIME, SL:DURATION, SL:AMOUNT, ...]. For normal tokens, we constrain the modelto predict only those tokens present in the ASR script. Duringtraining, we mask all tokens not in the candidate set, allowingthe model to focus on relevant tokens for better predictions.Additionally, unlike previous work [16], which employs mul-titask sequence spoken language understanding and representsoutput in a structured but challenging-to-predict JSON format,
2025-02-19 13:03:50 - WARNING: mapping dict 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59, 61: 60, 62: 61, 63: 61, 64: 62, 65: 63, 66: 64, 67: 65, 68: 66, 69: 67, 70: 67, 71: 67, 72: 68, 73: 69, 74: 70, 75: 71, 76: 72, 77: 73, 78: 74, 79: 75, 80: 76, 81: 77, 82: 78, 83: 79, 84: 80, 85: 81, 86: 82, 87: 83, 88: 84, 89: 85, 90: 86, 91: 87, 92: 88, 93: 89, 94: 90, 95: 91, 96: 92, 97: 93, 98: 94, 99: 95, 100: 96, 101: 97, 102: 98, 103: 99, 104: 100, 105: 101, 106: 102, 107: 103, 108: 104, 109: 105, 110: 106, 111: 107, 112: 108, 113: 109, 114: 110, 115: 111, 116: 112, 117: 113, 118: 114, 119: 115, 120: 116, 121: 117, 122: 118, 123: 119, 124: 120, 125: 121, 126: 122, 127: 123, 128: 124, 129: 125, 130: 126, 131: 127, 132: 128, 133: 129, 134: 130, 135: 131, 136: 132, 137: 133, 138: 134, 139: 135, 140: 136, 141: 137, 142: 138, 143: 139, 144: 140, 145: 141, 146: 142, 147: 143, 148: 144, 149: 145, 150: 146, 151: 147, 152: 148, 153: 149, 154: 150, 155: 151, 156: 152, 157: 153, 158: 154, 159: 155, 160: 156, 161: 157, 162: 158, 163: 159, 164: 160, 165: 161, 166: 162, 167: 163, 168: 164, 169: 165, 170: 166, 171: 167, 172: 168, 173: 169, 174: 170, 175: 171, 176: 172, 177: 173, 178: 174, 179: 175, 180: 176, 181: 177, 182: 178, 183: 179, 184: 180, 185: 181, 186: 182, 187: 183, 188: 184, 189: 185, 190: 186, 191: 187, 192: 188, 193: 189, 194: 190, 195: 191, 196: 192, 197: 193, 198: 194, 199: 195, 200: 196, 201: 197, 202: 198, 203: 199, 204: 200, 205: 201, 206: 202, 207: 203, 208: 204, 209: 205, 210: 206, 211: 207, 212: 208, 213: 209, 214: 210, 215: 211, 216: 212, 217: 213, 218: 214, 219: 215, 220: 216, 221: 217, 222: 218, 223: 219, 224: 220, 225: 221, 226: 222, 227: 223, 228: 224, 229: 225, 230: 226, 231: 227, 232: 228, 233: 228, 234: 229, 235: 230, 236: 231, 237: 232, 238: 233, 239: 234, 240: 235, 241: 236, 242: 237, 243: 238, 244: 239, 245: 240, 246: 241, 247: 242, 248: 243, 249: 244, 250: 245, 251: 246, 252: 247, 253: 248, 254: 249, 255: 250, 256: 251, 257: 252, 258: 253, 259: 254, 260: 255, 261: 256, 262: 256, 263: 257, 264: 257, 265: 258, 266: 259, 267: 260, 268: 261, 269: 262, 270: 263, 271: 264, 272: 265, 273: 266, 274: 267, 275: 268, 276: 269, 277: 270, 278: 271, 279: 272, 280: 272, 281: 273, 282: 274, 283: 275, 284: 276, 285: 277, 286: 278, 287: 279, 288: 280, 289: 281, 290: 282, 291: 283, 292: 284, 293: 285, 294: 286, 295: 287, 296: 288, 297: 289, 298: 290, 299: 291, 300: 292, 301: 293, 302: 294, 303: 295, 304: 296, 305: 297, 306: 298, 307: 299, 308: 300, 309: 301, 310: 302, 311: 303, 312: 304, 313: 305, 314: 306, 315: 307, 316: 308, 317: 309, 318: 310, 319: 310, 320: 311, 321: 312, 322: 313, 323: 314, 324: 315, 325: 316, 326: 317, 327: 318, 328: 319, 329: 320, 330: 321, 331: 322, 332: 323, 333: 324, 334: 325, 335: 326, 336: 327, 337: 328, 338: 329, 339: 330, 340: 331, 341: 332, 342: 333, 343: 334, 344: 335, 345: 336, 346: 337, 347: 338, 348: 339, 349: 340, 350: 341, 351: 342, 352: 343, 353: 344, 354: 345, 355: 346, 356: 347, 357: 348, 358: 349, 359: 350, 360: 351, 361: 352, 362: 353, 363: 354, 364: 355, 365: 356, 366: 357, 367: 358, 368: 359, 369: 360, 370: 361, 371: 362, 372: 363, 373: 364, 374: 365, 375: 366, 376: 367, 377: 368, 378: 369, 379: 370, 380: 371, 381: 372, 382: 373, 383: 374, 384: 375, 385: 376, 386: 377, 387: 378, 388: 379, 389: 380, 390: 381, 391: 382, 392: 383, 393: 384, 394: 385, 395: 386, 396: 387, 397: 388, 398: 389, 399: 390, 400: 391, 401: 392, 402: 393, 403: 394, 404: 395, 405: 396, 406: 397, 407: 398, 408: 399, 409: 400, 410: 401, 411: 402, 412: 403, 413: 404, 414: 405, 415: 406, 416: 407, 417: 408, 418: 409, 419: 410, 420: 411, 421: 412, 422: 413, 423: 414, 424: 415, 425: 416, 426: 417, 427: 418, 428: 419, 429: 420, 430: 421, 431: 422, 432: 423, 433: 424, 434: 425, 435: 426, 436: 427, 437: 428, 438: 429, 439: 430, 440: 431, 441: 432, 442: 433, 443: 434, 444: 434, 445: 435, 446: 436, 447: 437, 448: 438, 449: 439, 450: 440, 451: 441, 452: 442, 453: 443, 454: 444, 455: 445, 456: 446, 457: 447, 458: 448, 459: 449, 460: 450, 461: 451, 462: 452, 463: 453, 464: 454, 465: 455, 466: 456, 467: 457, 468: 458, 469: 459, 470: 460, 471: 461, 472: 462, 473: 463, 474: 464, 475: 465, 476: 466, 477: 467, 478: 468, 479: 469, 480: 470, 481: 471, 482: 472, 483: 473, 484: 474, 485: 475, 486: 476, 487: 477, 488: 478, 489: 479, 490: 480, 491: 481, 492: 482, 493: 483, 494: 484, 495: 485, 496: 486, 497: 487, 498: 488, 499: 489, 500: 490, 501: 491, 502: 492, 503: 493, 504: 494, 505: 495, 506: 496, 507: 497, 508: 498, 509: 499, 510: 500, 511: 501, 512: 502, 513: 503, 514: 504, 515: 505, 516: 506, 517: 507, 518: 508, 519: 509, 520: 510, 521: 511, 522: 512, 523: 513, 524: 514, 525: 515, 526: 516, 527: 517, 528: 518, 529: 519, 530: 520, 531: 521, 532: 521, 533: 522, 534: 523, 535: 523, 536: 524, 537: 525, 538: 526, 539: 527, 540: 528, 541: 529, 542: 530, 543: 531, 544: 532, 545: 533, 546: 534, 547: 535, 548: 536, 549: 537, 550: 538, 551: 539, 552: 540, 553: 541, 554: 542, 555: 543, 556: 544, 557: 545, 558: 546, 559: 547, 560: 548, 561: 549, 562: 550, 563: 551, 564: 552, 565: 553, 566: 554, 567: 555, 568: 556, 569: 557, 570: 558, 571: 559, 572: 560, 573: 561, 574: 562, 575: 563, 576: 564, 577: 565, 578: 566, 579: 567, 580: 568, 581: 569, 582: 570, 583: 570, 584: 571, 585: 572, 586: 572, 587: 573, 588: 573, 589: 574, 590: 575, 591: 576, 592: 577, 593: 578, 594: 579, 595: 580, 596: 581, 597: 582, 598: 583, 599: 584, 600: 585, 601: 586, 602: 587, 603: 588, 604: 588, 605: 589, 606: 590, 607: 591, 608: 592, 609: 593, 610: 594, 611: 595, 612: 596, 613: 597, 614: 598, 615: 599, 616: 600, 617: 601, 618: 602, 619: 603, 620: 604, 621: 605, 622: 606, 623: 607, 624: 608, 625: 609, 626: 610, 627: 611, 628: 612, 629: 613, 630: 614, 631: 615, 632: 616, 633: 617, 634: 618, 635: 619, 636: 620, 637: 621, 638: 622, 639: 623, 640: 624, 641: 625, 642: 626, 643: 627, 644: 628, 645: 629, 646: 630, 647: 631, 648: 632, 649: 633, 650: 634, 651: 635, 652: 636, 653: 637, 654: 638, 655: 639, 656: 640, 657: 641, 658: 642, 659: 643, 660: 644, 661: 645, 662: 646, 663: 647, 664: 648, 665: 649, 666: 650, 667: 651, 668: 652, 669: 653, 670: 654, 671: 655, 672: 656, 673: 657, 674: 658, 675: 659, 676: 660, 677: 661, 678: 662, 679: 663, 680: 664, 681: 665, 682: 666, 683: 667, 684: 668, 685: 669, 686: 669, 687: 670, 688: 671, 689: 672, 690: 673, 691: 674, 692: 675, 693: 676, 694: 677, 695: 678, 696: 679, 697: 680, 698: 681, 699: 682, 700: 683, 701: 684, 702: 685, 703: 686, 704: 687, 705: 688, 706: 689, 707: 690, 708: 691, 709: 692, 710: 693, 711: 694, 712: 695, 713: 696, 714: 697, 715: 698, 716: 699, 717: 700, 718: 701, 719: 702, 720: 703, 721: 704, 722: 705, 723: 706, 724: 707, 725: 708, 726: 709, 727: 710, 728: 711, 729: 712, 730: 713, 731: 714, 732: 715, 733: 716, 734: 717, 735: 718, 736: 719, 737: 720, 738: 721, 739: 722, 740: 723, 741: 724, 742: 725, 743: 726, 744: 727, 745: 728, 746: 729, 747: 730, 748: 731, 749: 732, 750: 733, 751: 734, 752: 735, 753: 736, 754: 737, 755: 738, 756: 739, 757: 740, 758: 741, 759: 742, 760: 743, 761: 744, 762: 745, 763: 746, 764: 747, 765: 748, 766: 749, 767: 750, 768: 751, 769: 752, 770: 753, 771: 754, 772: 755, 773: 756, 774: 757, 775: 758, 776: 759, 777: 760, 778: 761, 779: 762, 780: 763, 781: 764, 782: 765, 783: 766, 784: 766, 785: 767, 786: 768, 787: 769, 788: 770, 789: 771, 790: 772, 791: 773, 792: 774, 793: 775, 794: 775, 795: 776, 796: 777, 797: 778, 798: 779, 799: 780, 800: 781, 801: 782, 802: 783, 803: 784, 804: 785, 805: 786, 806: 787, 807: 788, 808: 789, 809: 790, 810: 791, 811: 792, 812: 793, 813: 794, 814: 795, 815: 796, 816: 797, 817: 798, 818: 799, 819: 800, 820: 801, 821: 802, 822: 803, 823: 804, 824: 805, 825: 806, 826: 807, 827: 808, 828: 809, 829: 810, 830: 811, 831: 812, 832: 813, 833: 814, 834: 815, 835: 816, 836: 817, 837: 818, 838: 819, 839: 820, 840: 821, 841: 822, 842: 823, 843: 824, 844: 825, 845: 826, 846: 827, 847: 828, 848: 829, 849: 830, 850: 831, 851: 832, 852: 833, 853: 834, 854: 835, 855: 836, 856: 837, 857: 838, 858: 839, 859: 840, 860: 841, 861: 842, 862: 843, 863: 843, 864: 843, 865: 844, 866: 845, 867: 846, 868: 847, 869: 848, 870: 849, 871: 850, 872: 851, 873: 852, 874: 853, 875: 854, 876: 855, 877: 856, 878: 857, 879: 858, 880: 859, 881: 860, 882: 861, 883: 862, 884: 863, 885: 864, 886: 865, 887: 866, 888: 867, 889: 868, 890: 869, 891: 870, 892: 871, 893: 872, 894: 873, 895: 874, 896: 875, 897: 876, 898: 877, 899: 878, 900: 879, 901: 880, 902: 881, 903: 882, 904: 883, 905: 884, 906: 885, 907: 886, 908: 887, 909: 888, 910: 889, 911: 890, 912: 891, 913: 892, 914: 893, 915: 894, 916: 895, 917: 896, 918: 897, 919: 898, 920: 899, 921: 900, 922: 901, 923: 902, 924: 903, 925: 904, 926: 905, 927: 906, 928: 907, 929: 907, 930: 908, 931: 909, 932: 910, 933: 911, 934: 912, 935: 913, 936: 914, 937: 915, 938: 916, 939: 917, 940: 918, 941: 919, 942: 920, 943: 921, 944: 922, 945: 923, 946: 924, 947: 925, 948: 926, 949: 927, 950: 928, 951: 929, 952: 930, 953: 931, 954: 932, 955: 933, 956: 934, 957: 935, 958: 936, 959: 937, 960: 938, 961: 939, 962: 940, 963: 941, 964: 942, 965: 943, 966: 944, 967: 945, 968: 946, 969: 947, 970: 948, 971: 949, 972: 950, 973: 951, 974: 952, 975: 953, 976: 954, 977: 955, 978: 956, 979: 957, 980: 958, 981: 959, 982: 960, 983: 961, 984: 962, 985: 963, 986: 964, 987: 965, 988: 966, 989: 967, 990: 968, 991: 969, 992: 970, 993: 971, 994: 972, 995: 973, 996: 974, 997: 975, 998: 976, 999: 976, 1000: 977, 1001: 978, 1002: 979, 1003: 980, 1004: 981, 1005: 982, 1006: 983, 1007: 984, 1008: 985, 1009: 986, 1010: 987, 1011: 988, 1012: 989, 1013: 990, 1014: 991, 1015: 992, 1016: 993, 1017: 994, 1018: 995, 1019: 996, 1020: 997, 1021: 998, 1022: 999, 1023: 1000, 1024: 1001, 1025: 1002, 1026: 1003, 1027: 1004, 1028: 1005, 1029: 1006, 1030: 1007, 1031: 1008, 1032: 1009, 1033: 1010, 1034: 1011, 1035: 1012, 1036: 1013, 1037: 1014, 1038: 1015, 1039: 1016, 1040: 1017, 1041: 1018, 1042: 1019, 1043: 1020, 1044: 1021, 1045: 1022, 1046: 1023, 1047: 1024, 1048: 1025, 1049: 1026, 1050: 1027, 1051: 1027, 1052: 1028, 1053: 1028, 1054: 1029, 1055: 1030, 1056: 1031, 1057: 1032, 1058: 1033, 1059: 1034, 1060: 1035, 1061: 1036, 1062: 1037, 1063: 1038, 1064: 1039, 1065: 1040, 1066: 1041, 1067: 1042, 1068: 1043, 1069: 1044, 1070: 1044, 1071: 1044, 1072: 1045, 1073: 1046, 1074: 1047, 1075: 1048, 1076: 1049, 1077: 1050, 1078: 1051, 1079: 1052, 1080: 1053, 1081: 1054, 1082: 1055, 1083: 1056, 1084: 1057, 1085: 1058, 1086: 1059, 1087: 1060, 1088: 1061, 1089: 1062, 1090: 1063, 1091: 1064, 1092: 1065, 1093: 1066, 1094: 1067, 1095: 1068, 1096: 1069, 1097: 1070, 1098: 1071, 1099: 1072, 1100: 1073, 1101: 1074, 1102: 1075, 1103: 1076, 1104: 1077, 1105: 1078, 1106: 1079, 1107: 1080, 1108: 1081, 1109: 1082, 1110: 1083, 1111: 1084, 1112: 1085, 1113: 1086, 1114: 1087, 1115: 1088, 1116: 1089, 1117: 1090, 1118: 1091, 1119: 1091, 1120: 1092, 1121: 1093, 1122: 1094, 1123: 1095, 1124: 1096, 1125: 1097, 1126: 1098, 1127: 1099, 1128: 1100, 1129: 1101, 1130: 1102, 1131: 1103, 1132: 1104, 1133: 1105, 1134: 1106, 1135: 1107, 1136: 1108, 1137: 1109, 1138: 1110, 1139: 1111, 1140: 1112, 1141: 1113, 1142: 1114, 1143: 1115, 1144: 1116, 1145: 1117, 1146: 1118, 1147: 1119, 1148: 1120, 1149: 1121, 1150: 1122, 1151: 1123, 1152: 1124, 1153: 1125, 1154: 1126, 1155: 1127, 1156: 1128, 1157: 1129, 1158: 1130, 1159: 1131, 1160: 1132, 1161: 1133, 1162: 1134, 1163: 1135, 1164: 1136, 1165: 1137, 1166: 1138, 1167: 1139, 1168: 1140, 1169: 1141, 1170: 1142, 1171: 1143, 1172: 1144, 1173: 1145, 1174: 1146, 1175: 1147, 1176: 1148, 1177: 1149, 1178: 1150, 1179: 1151, 1180: 1152, 1181: 1153, 1182: 1154, 1183: 1155, 1184: 1156, 1185: 1156, 1186: 1157, 1187: 1157, 1188: 1158, 1189: 1159, 1190: 1160, 1191: 1161, 1192: 1162, 1193: 1163, 1194: 1164, 1195: 1165, 1196: 1166, 1197: 1167, 1198: 1168, 1199: 1169, 1200: 1169, 1201: 1170, 1202: 1171, 1203: 1172, 1204: 1173, 1205: 1174, 1206: 1175, 1207: 1176, 1208: 1177, 1209: 1178, 1210: 1179, 1211: 1180, 1212: 1181, 1213: 1181, 1214: 1182, 1215: 1182, 1216: 1183, 1217: 1184, 1218: 1185, 1219: 1186, 1220: 1187, 1221: 1188, 1222: 1189, 1223: 1190, 1224: 1191, 1225: 1191, 1226: 1192, 1227: 1193, 1228: 1194, 1229: 1195, 1230: 1195, 1231: 1196, 1232: 1196, 1233: 1197, 1234: 1198, 1235: 1199, 1236: 1200, 1237: 1201, 1238: 1202, 1239: 1203, 1240: 1204, 1241: 1205, 1242: 1206, 1243: 1207, 1244: 1208, 1245: 1209, 1246: 1210, 1247: 1211, 1248: 1212, 1249: 1213, 1250: 1214, 1251: 1215, 1252: 1216, 1253: 1216, 1254: 1216, 1255: 1217, 1256: 1218, 1257: 1219, 1258: 1220, 1259: 1221, 1260: 1222, 1261: 1223, 1262: 1224, 1263: 1225, 1264: 1226, 1265: 1227, 1266: 1228, 1267: 1229, 1268: 1230, 1269: 1231, 1270: 1232, 1271: 1233, 1272: 1234, 1273: 1235, 1274: 1236, 1275: 1237, 1276: 1238, 1277: 1239, 1278: 1240, 1279: 1241, 1280: 1241, 1281: 1242, 1282: 1243, 1283: 1244, 1284: 1245, 1285: 1246, 1286: 1247, 1287: 1248, 1288: 1249, 1289: 1249, 1290: 1250, 1291: 1250, 1292: 1251, 1293: 1251, 1294: 1252, 1295: 1253, 1296: 1254, 1297: 1255, 1298: 1256, 1299: 1257, 1300: 1258, 1301: 1259, 1302: 1260, 1303: 1260, 1304: 1261, 1305: 1262, 1306: 1263, 1307: 1264, 1308: 1265, 1309: 1266, 1310: 1267, 1311: 1268, 1312: 1269, 1313: 1270, 1314: 1271, 1315: 1272, 1316: 1273, 1317: 1274, 1318: 1275, 1319: 1276, 1320: 1277, 1321: 1278, 1322: 1279, 1323: 1280, 1324: 1281, 1325: 1282, 1326: 1283, 1327: 1284, 1328: 1285, 1329: 1286, 1330: 1287, 1331: 1288, 1332: 1289, 1333: 1290, 1334: 1291, 1335: 1291, 1336: 1292, 1337: 1293, 1338: 1294, 1339: 1295, 1340: 1296, 1341: 1297, 1342: 1298, 1343: 1299, 1344: 1300, 1345: 1301, 1346: 1302, 1347: 1303, 1348: 1304, 1349: 1305, 1350: 1306, 1351: 1307, 1352: 1308, 1353: 1309, 1354: 1310, 1355: 1311, 1356: 1312, 1357: 1313, 1358: 1314, 1359: 1315, 1360: 1316, 1361: 1317, 1362: 1318, 1363: 1319, 1364: 1320, 1365: 1321, 1366: 1322, 1367: 1323, 1368: 1324, 1369: 1325, 1370: 1326, 1371: 1327, 1372: 1328, 1373: 1329, 1374: 1330, 1375: 1331, 1376: 1332, 1377: 1333, 1378: 1334, 1379: 1334, 1380: 1335, 1381: 1336, 1382: 1337, 1383: 1338, 1384: 1339, 1385: 1340, 1386: 1341, 1387: 1342, 1388: 1343, 1389: 1344, 1390: 1345, 1391: 1346, 1392: 1347, 1393: 1348, 1394: 1349, 1395: 1350, 1396: 1351, 1397: 1352, 1398: 1353, 1399: 1354, 1400: 1355, 1401: 1356, 1402: 1357, 1403: 1358, 1404: 1359, 1405: 1360, 1406: 1361, 1407: 1362, 1408: 1363, 1409: 1364, 1410: 1365, 1411: 1366, 1412: 1367, 1413: 1368, 1414: 1369, 1415: 1370, 1416: 1371, 1417: 1372, 1418: 1373, 1419: 1374, 1420: 1375, 1421: 1376, 1422: 1377, 1423: 1378, 1424: 1379, 1425: 1380, 1426: 1381, 1427: 1382, 1428: 1383, 1429: 1384, 1430: 1385, 1431: 1386, 1432: 1387, 1433: 1388, 1434: 1389, 1435: 1390, 1436: 1390, 1437: 1391, 1438: 1392, 1439: 1393, 1440: 1394, 1441: 1395, 1442: 1396, 1443: 1397, 1444: 1398, 1445: 1399, 1446: 1400, 1447: 1401, 1448: 1402, 1449: 1403, 1450: 1404, 1451: 1405, 1452: 1406, 1453: 1407, 1454: 1408, 1455: 1409, 1456: 1410, 1457: 1411, 1458: 1412, 1459: 1413, 1460: 1414, 1461: 1415, 1462: 1416, 1463: 1417, 1464: 1418, 1465: 1419, 1466: 1420, 1467: 1421, 1468: 1422, 1469: 1423, 1470: 1424, 1471: 1425, 1472: 1426, 1473: 1427, 1474: 1428, 1475: 1429, 1476: 1430, 1477: 1431, 1478: 1432, 1479: 1433, 1480: 1434, 1481: 1435, 1482: 1436, 1483: 1437, 1484: 1438, 1485: 1439, 1486: 1440, 1487: 1441, 1488: 1441, 1489: 1442, 1490: 1443, 1491: 1444, 1492: 1445, 1493: 1446, 1494: 1447, 1495: 1448, 1496: 1449, 1497: 1450, 1498: 1451, 1499: 1452, 1500: 1453, 1501: 1454, 1502: 1455, 1503: 1456, 1504: 1457, 1505: 1458, 1506: 1459, 1507: 1460, 1508: 1461, 1509: 1462, 1510: 1463, 1511: 1464, 1512: 1465, 1513: 1466, 1514: 1466, 1515: 1467, 1516: 1468, 1517: 1469, 1518: 1470, 1519: 1471, 1520: 1472, 1521: 1473, 1522: 1474, 1523: 1475, 1524: 1476, 1525: 1477, 1526: 1478, 1527: 1479, 1528: 1480, 1529: 1481, 1530: 1482, 1531: 1483, 1532: 1484, 1533: 1485, 1534: 1486, 1535: 1487, 1536: 1488, 1537: 1489, 1538: 1490, 1539: 1491, 1540: 1492, 1541: 1493, 1542: 1494, 1543: 1495, 1544: 1496, 1545: 1497, 1546: 1498, 1547: 1499, 1548: 1500, 1549: 1501, 1550: 1502, 1551: 1503, 1552: 1504, 1553: 1505, 1554: 1506, 1555: 1507, 1556: 1508, 1557: 1509, 1558: 1510, 1559: 1511, 1560: 1512, 1561: 1513, 1562: 1514, 1563: 1515, 1564: 1516, 1565: 1517, 1566: 1518, 1567: 1519, 1568: 1520, 1569: 1521, 1570: 1522, 1571: 1523, 1572: 1524, 1573: 1525, 1574: 1526, 1575: 1527, 1576: 1528, 1577: 1529, 1578: 1530, 1579: 1531, 1580: 1532, 1581: 1533, 1582: 1534, 1583: 1535, 1584: 1536, 1585: 1537, 1586: 1538, 1587: 1539, 1588: 1540, 1589: 1541, 1590: 1542, 1591: 1543, 1592: 1544, 1593: 1545, 1594: 1546, 1595: 1547, 1596: 1548, 1597: 1549, 1598: 1550, 1599: 1551, 1600: 1552, 1601: 1553, 1602: 1554, 1603: 1555, 1604: 1556, 1605: 1557, 1606: 1558, 1607: 1559, 1608: 1560, 1609: 1561, 1610: 1562, 1611: 1562, 1612: 1563, 1613: 1564, 1614: 1565, 1615: 1566, 1616: 1567, 1617: 1568, 1618: 1569, 1619: 1570, 1620: 1571, 1621: 1572, 1622: 1573, 1623: 1574, 1624: 1575, 1625: 1576, 1626: 1577, 1627: 1578, 1628: 1579, 1629: 1580, 1630: 1581, 1631: 1582, 1632: 1583, 1633: 1584, 1634: 1585, 1635: 1586, 1636: 1587, 1637: 1588, 1638: 1589, 1639: 1590, 1640: 1591, 1641: 1592, 1642: 1593, 1643: 1594, 1644: 1595, 1645: 1596, 1646: 1597, 1647: 1598, 1648: 1599, 1649: 1600, 1650: 1601, 1651: 1602, 1652: 1603, 1653: 1604, 1654: 1605, 1655: 1606, 1656: 1607, 1657: 1608, 1658: 1609, 1659: 1610, 1660: 1611, 1661: 1612, 1662: 1613, 1663: 1614, 1664: 1615, 1665: 1616, 1666: 1617, 1667: 1618, 1668: 1619, 1669: 1620, 1670: 1621, 1671: 1622, 1672: 1623, 1673: 1624, 1674: 1625, 1675: 1626, 1676: 1627, 1677: 1628, 1678: 1629, 1679: 1630, 1680: 1631, 1681: 1632, 1682: 1633, 1683: 1634, 1684: 1635, 1685: 1636, 1686: 1637, 1687: 1638, 1688: 1639, 1689: 1640, 1690: 1641, 1691: 1642, 1692: 1642, 1693: 1643, 1694: 1643, 1695: 1644, 1696: 1644, 1697: 1645, 1698: 1646, 1699: 1647, 1700: 1647, 1701: 1648, 1702: 1648, 1703: 1649, 1704: 1649, 1705: 1650, 1706: 1651, 1707: 1652, 1708: 1653, 1709: 1654, 1710: 1655, 1711: 1656, 1712: 1657, 1713: 1658, 1714: 1659, 1715: 1660, 1716: 1661, 1717: 1662, 1718: 1663, 1719: 1664, 1720: 1665, 1721: 1666, 1722: 1667, 1723: 1668, 1724: 1669, 1725: 1670, 1726: 1671, 1727: 1672, 1728: 1673, 1729: 1674, 1730: 1675, 1731: 1676, 1732: 1677, 1733: 1678, 1734: 1679, 1735: 1680, 1736: 1681, 1737: 1682, 1738: 1683, 1739: 1684, 1740: 1685, 1741: 1686, 1742: 1687, 1743: 1688, 1744: 1689, 1745: 1690, 1746: 1691, 1747: 1692, 1748: 1693, 1749: 1694, 1750: 1695, 1751: 1696, 1752: 1697, 1753: 1698, 1754: 1699, 1755: 1700, 1756: 1701, 1757: 1702, 1758: 1703, 1759: 1704, 1760: 1705, 1761: 1706, 1762: 1707, 1763: 1708, 1764: 1709, 1765: 1710, 1766: 1711, 1767: 1712, 1768: 1713, 1769: 1714, 1770: 1715, 1771: 1716, 1772: 1717, 1773: 1718, 1774: 1719, 1775: 1720, 1776: 1721, 1777: 1722, 1778: 1723, 1779: 1724, 1780: 1725, 1781: 1726, 1782: 1727, 1783: 1728, 1784: 1729, 1785: 1730, 1786: 1731, 1787: 1732, 1788: 1733, 1789: 1734, 1790: 1735, 1791: 1736, 1792: 1737, 1793: 1738, 1794: 1739, 1795: 1740, 1796: 1740, 1797: 1741, 1798: 1742, 1799: 1743, 1800: 1744, 1801: 1745, 1802: 1746, 1803: 1747, 1804: 1748, 1805: 1749, 1806: 1750, 1807: 1751, 1808: 1752, 1809: 1753, 1810: 1754, 1811: 1755, 1812: 1756, 1813: 1757, 1814: 1758, 1815: 1759, 1816: 1760, 1817: 1761, 1818: 1762, 1819: 1763, 1820: 1764, 1821: 1765, 1822: 1766, 1823: 1767, 1824: 1768, 1825: 1769, 1826: 1770, 1827: 1771, 1828: 1772, 1829: 1773, 1830: 1774, 1831: 1775, 1832: 1776, 1833: 1777, 1834: 1778, 1835: 1779, 1836: 1780, 1837: 1781, 1838: 1782, 1839: 1783, 1840: 1784, 1841: 1785, 1842: 1786, 1843: 1787, 1844: 1788, 1845: 1789, 1846: 1790, 1847: 1791, 1848: 1792, 1849: 1793, 1850: 1794, 1851: 1795, 1852: 1796, 1853: 1797, 1854: 1798, 1855: 1799, 1856: 1800, 1857: 1801, 1858: 1802, 1859: 1803, 1860: 1804, 1861: 1805, 1862: 1806, 1863: 1807, 1864: 1808, 1865: 1809, 1866: 1810, 1867: 1811, 1868: 1812, 1869: 1813, 1870: 1814, 1871: 1815, 1872: 1816, 1873: 1817, 1874: 1818, 1875: 1818, 1876: 1819, 1877: 1820, 1878: 1821, 1879: 1822, 1880: 1823, 1881: 1824, 1882: 1825, 1883: 1826, 1884: 1827, 1885: 1828, 1886: 1829, 1887: 1830, 1888: 1831, 1889: 1831, 1890: 1832, 1891: 1833, 1892: 1834, 1893: 1835, 1894: 1836, 1895: 1837, 1896: 1838, 1897: 1839, 1898: 1840, 1899: 1841, 1900: 1842, 1901: 1843, 1902: 1844, 1903: 1845, 1904: 1846, 1905: 1847, 1906: 1848, 1907: 1849, 1908: 1850, 1909: 1851, 1910: 1852, 1911: 1853, 1912: 1854, 1913: 1855, 1914: 1856, 1915: 1857, 1916: 1858, 1917: 1859, 1918: 1860, 1919: 1861, 1920: 1862, 1921: 1863, 1922: 1864, 1923: 1865, 1924: 1866, 1925: 1867, 1926: 1868, 1927: 1868, 1928: 1869, 1929: 1869, 1930: 1870, 1931: 1871, 1932: 1872, 1933: 1873, 1934: 1874, 1935: 1875, 1936: 1876, 1937: 1877, 1938: 1878, 1939: 1879, 1940: 1880, 1941: 1881, 1942: 1882, 1943: 1883, 1944: 1884, 1945: 1885, 1946: 1886, 1947: 1887, 1948: 1888, 1949: 1889, 1950: 1890, 1951: 1891, 1952: 1892, 1953: 1893, 1954: 1894, 1955: 1895, 1956: 1896, 1957: 1897, 1958: 1898, 1959: 1899, 1960: 1900, 1961: 1901, 1962: 1902, 1963: 1903, 1964: 1904, 1965: 1904, 1966: 1905, 1967: 1905, 1968: 1906, 1969: 1907, 1970: 1908, 1971: 1909, 1972: 1910, 1973: 1911, 1974: 1912, 1975: 1913, 1976: 1914, 1977: 1915, 1978: 1916, 1979: 1917, 1980: 1918, 1981: 1919, 1982: 1920, 1983: 1921, 1984: 1922, 1985: 1923, 1986: 1924, 1987: 1925, 1988: 1926, 1989: 1927, 1990: 1928, 1991: 1929, 1992: 1930, 1993: 1931, 1994: 1932, 1995: 1933, 1996: 1934, 1997: 1935, 1998: 1936, 1999: 1937, 2000: 1938, 2001: 1939, 2002: 1940, 2003: 1941, 2004: 1942, 2005: 1943, 2006: 1944, 2007: 1945, 2008: 1946, 2009: 1947, 2010: 1948, 2011: 1949, 2012: 1949, 2013: 1949, 2014: 1949, 2015: 1950, 2016: 1951, 2017: 1952, 2018: 1953, 2019: 1954, 2020: 1955, 2021: 1956, 2022: 1957, 2023: 1958, 2024: 1959, 2025: 1960, 2026: 1961, 2027: 1962, 2028: 1963, 2029: 1964, 2030: 1965, 2031: 1966, 2032: 1967, 2033: 1968, 2034: 1969, 2035: 1970, 2036: 1971, 2037: 1972, 2038: 1973, 2039: 1974, 2040: 1975, 2041: 1976, 2042: 1977, 2043: 1978, 2044: 1979, 2045: 1980, 2046: 1981, 2047: 1982, 2048: 1983, 2049: 1984, 2050: 1985, 2051: 1986, 2052: 1987, 2053: 1988, 2054: 1989, 2055: 1990, 2056: 1991, 2057: 1992, 2058: 1993, 2059: 1994, 2060: 1995, 2061: 1996, 2062: 1997, 2063: 1998, 2064: 1999, 2065: 2000, 2066: 2001, 2067: 2002, 2068: 2003, 2069: 2004, 2070: 2005, 2071: 2006, 2072: 2007, 2073: 2008, 2074: 2009, 2075: 2010, 2076: 2011, 2077: 2012, 2078: 2013, 2079: 2014, 2080: 2015, 2081: 2016, 2082: 2017, 2083: 2018, 2084: 2019, 2085: 2019, 2086: 2020, 2087: 2020, 2088: 2021, 2089: 2022, 2090: 2023, 2091: 2024, 2092: 2025, 2093: 2026, 2094: 2027, 2095: 2028, 2096: 2029, 2097: 2030, 2098: 2031, 2099: 2032, 2100: 2032, 2101: 2033, 2102: 2034, 2103: 2035, 2104: 2035, 2105: 2036, 2106: 2037, 2107: 2037, 2108: 2038, 2109: 2038, 2110: 2039, 2111: 2040, 2112: 2041, 2113: 2042, 2114: 2043, 2115: 2044, 2116: 2045, 2117: 2046, 2118: 2047, 2119: 2047, 2120: 2048, 2121: 2049, 2122: 2050, 2123: 2051, 2124: 2051, 2125: 2052, 2126: 2052, 2127: 2053, 2128: 2054, 2129: 2055, 2130: 2056, 2131: 2057, 2132: 2058, 2133: 2059, 2134: 2060, 2135: 2060, 2136: 2061, 2137: 2062, 2138: 2063, 2139: 2064, 2140: 2064, 2141: 2065, 2142: 2065, 2143: 2066, 2144: 2067, 2145: 2068, 2146: 2069, 2147: 2070, 2148: 2071, 2149: 2071, 2150: 2072, 2151: 2073, 2152: 2074, 2153: 2075, 2154: 2076, 2155: 2076, 2156: 2077, 2157: 2077, 2158: 2078, 2159: 2079, 2160: 2080, 2161: 2081, 2162: 2082, 2163: 2083, 2164: 2084, 2165: 2085, 2166: 2086, 2167: 2087, 2168: 2088, 2169: 2089, 2170: 2090, 2171: 2091, 2172: 2092, 2173: 2093, 2174: 2094, 2175: 2095, 2176: 2096, 2177: 2096, 2178: 2097, 2179: 2098, 2180: 2099, 2181: 2100, 2182: 2101, 2183: 2102, 2184: 2103, 2185: 2104, 2186: 2105, 2187: 2106, 2188: 2107, 2189: 2108, 2190: 2109, 2191: 2110, 2192: 2111, 2193: 2112, 2194: 2113, 2195: 2114, 2196: 2115, 2197: 2116, 2198: 2117, 2199: 2118, 2200: 2119, 2201: 2120, 2202: 2121, 2203: 2122, 2204: 2123, 2205: 2124, 2206: 2125, 2207: 2126, 2208: 2127, 2209: 2128, 2210: 2129, 2211: 2130, 2212: 2131, 2213: 2132, 2214: 2133, 2215: 2134, 2216: 2135, 2217: 2136, 2218: 2137, 2219: 2138, 2220: 2139, 2221: 2140, 2222: 2141, 2223: 2142, 2224: 2143, 2225: 2144, 2226: 2145, 2227: 2146, 2228: 2147, 2229: 2148, 2230: 2149, 2231: 2150, 2232: 2151, 2233: 2152, 2234: 2153, 2235: 2154, 2236: 2155, 2237: 2156, 2238: 2157, 2239: 2158, 2240: 2159, 2241: 2160, 2242: 2161, 2243: 2162, 2244: 2163, 2245: 2164, 2246: 2165, 2247: 2166, 2248: 2167, 2249: 2168, 2250: 2169, 2251: 2170, 2252: 2171, 2253: 2172, 2254: 2173, 2255: 2174, 2256: 2174, 2257: 2175, 2258: 2176, 2259: 2177, 2260: 2178, 2261: 2179, 2262: 2180, 2263: 2181, 2264: 2182, 2265: 2183, 2266: 2184, 2267: 2185, 2268: 2186, 2269: 2187, 2270: 2188, 2271: 2189, 2272: 2190, 2273: 2190, 2274: 2191, 2275: 2192, 2276: 2193, 2277: 2194, 2278: 2195, 2279: 2196, 2280: 2197, 2281: 2198, 2282: 2199, 2283: 2200, 2284: 2201, 2285: 2202, 2286: 2203, 2287: 2204, 2288: 2205, 2289: 2206, 2290: 2207, 2291: 2208, 2292: 2209, 2293: 2210, 2294: 2211, 2295: 2212, 2296: 2213, 2297: 2214, 2298: 2215, 2299: 2216, 2300: 2217, 2301: 2218, 2302: 2219, 2303: 2220, 2304: 2221, 2305: 2222, 2306: 2223, 2307: 2224, 2308: 2225, 2309: 2226, 2310: 2227, 2311: 2228, 2312: 2229, 2313: 2230, 2314: 2231, 2315: 2232, 2316: 2233, 2317: 2234, 2318: 2235, 2319: 2235, 2320: 2236, 2321: 2237, 2322: 2238, 2323: 2239, 2324: 2240, 2325: 2241, 2326: 2242, 2327: 2243, 2328: 2244, 2329: 2245, 2330: 2246, 2331: 2247, 2332: 2248, 2333: 2249, 2334: 2250, 2335: 2251, 2336: 2252, 2337: 2253, 2338: 2254, 2339: 2255, 2340: 2256, 2341: 2257, 2342: 2258, 2343: 2259, 2344: 2260, 2345: 2261, 2346: 2262, 2347: 2263, 2348: 2264, 2349: 2265, 2350: 2266, 2351: 2267, 2352: 2268, 2353: 2269, 2354: 2270, 2355: 2271, 2356: 2272, 2357: 2273, 2358: 2274, 2359: 2275, 2360: 2276, 2361: 2277, 2362: 2278, 2363: 2279, 2364: 2280, 2365: 2281, 2366: 2282, 2367: 2283, 2368: 2284, 2369: 2285, 2370: 2286, 2371: 2287, 2372: 2288, 2373: 2289, 2374: 2290, 2375: 2291, 2376: 2292, 2377: 2293, 2378: 2294, 2379: 2295, 2380: 2296, 2381: 2297, 2382: 2298, 2383: 2299, 2384: 2300, 2385: 2301, 2386: 2302, 2387: 2303, 2388: 2304, 2389: 2305, 2390: 2306, 2391: 2307, 2392: 2308, 2393: 2309, 2394: 2310, 2395: 2311, 2396: 2312, 2397: 2313, 2398: 2314, 2399: 2315, 2400: 2316, 2401: 2317, 2402: 2318, 2403: 2318, 2404: 2319, 2405: 2320, 2406: 2321, 2407: 2322, 2408: 2323, 2409: 2324, 2410: 2325, 2411: 2326, 2412: 2327, 2413: 2328, 2414: 2329, 2415: 2330, 2416: 2331, 2417: 2332, 2418: 2333, 2419: 2334, 2420: 2335, 2421: 2336, 2422: 2337, 2423: 2338, 2424: 2339, 2425: 2340, 2426: 2341, 2427: 2342, 2428: 2342, 2429: 2343, 2430: 2344, 2431: 2344, 2432: 2345, 2433: 2345, 2434: 2346, 2435: 2347, 2436: 2348, 2437: 2349, 2438: 2350, 2439: 2351, 2440: 2352, 2441: 2353, 2442: 2354, 2443: 2355, 2444: 2356, 2445: 2357, 2446: 2358, 2447: 2359, 2448: 2360, 2449: 2361, 2450: 2362, 2451: 2363, 2452: 2364, 2453: 2364, 2454: 2365, 2455: 2365, 2456: 2366, 2457: 2367, 2458: 2368, 2459: 2369, 2460: 2370, 2461: 2371, 2462: 2372, 2463: 2373, 2464: 2374, 2465: 2375, 2466: 2376, 2467: 2377, 2468: 2378, 2469: 2379, 2470: 2380, 2471: 2381, 2472: 2382, 2473: 2383, 2474: 2384, 2475: 2385, 2476: 2386, 2477: 2387, 2478: 2388, 2479: 2389, 2480: 2390, 2481: 2391, 2482: 2392, 2483: 2393, 2484: 2394, 2485: 2395, 2486: 2396, 2487: 2397, 2488: 2398, 2489: 2399, 2490: 2400, 2491: 2401, 2492: 2402, 2493: 2403, 2494: 2404, 2495: 2405, 2496: 2406, 2497: 2407, 2498: 2408, 2499: 2409, 2500: 2410, 2501: 2411, 2502: 2412, 2503: 2413, 2504: 2414, 2505: 2415, 2506: 2416, 2507: 2417, 2508: 2418, 2509: 2419, 2510: 2420, 2511: 2421, 2512: 2422, 2513: 2423, 2514: 2424, 2515: 2425, 2516: 2426, 2517: 2427, 2518: 2428, 2519: 2429, 2520: 2430, 2521: 2431, 2522: 2432, 2523: 2433, 2524: 2434, 2525: 2435, 2526: 2436, 2527: 2437, 2528: 2438, 2529: 2439, 2530: 2440, 2531: 2441, 2532: 2442, 2533: 2443, 2534: 2444, 2535: 2445, 2536: 2446, 2537: 2447, 2538: 2448, 2539: 2449, 2540: 2450, 2541: 2451, 2542: 2452, 2543: 2453, 2544: 2454, 2545: 2455, 2546: 2456, 2547: 2457, 2548: 2458, 2549: 2459, 2550: 2460, 2551: 2461, 2552: 2462, 2553: 2463, 2554: 2463, 2555: 2464, 2556: 2465, 2557: 2466, 2558: 2466, 2559: 2466, 2560: 2467, 2561: 2467, 2562: 2468, 2563: 2469, 2564: 2470, 2565: 2471, 2566: 2472, 2567: 2473, 2568: 2474, 2569: 2475, 2570: 2476, 2571: 2477, 2572: 2478, 2573: 2479, 2574: 2480, 2575: 2481, 2576: 2482, 2577: 2483, 2578: 2484, 2579: 2485, 2580: 2486, 2581: 2487, 2582: 2487, 2583: 2487}
2025-02-19 13:03:50 - WARNING: text: 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: information for logical label prediction . Among other architec - tures , joint E2E models offer the most promising approach asthey achieve comparable results without increasing model sizeby adding extra components for ASR prediction , making themfeasible for on - device settings . This paper adopts the jointE2E model , employing constrained decoding and simplifyingoutput sequences to enhance spoken language understandingtasks.Traditionally , most prior studies employing the joint E2Earchitecture have utilized autoregressive ( AR ) models built onthe Transformer architecture [ 15 ] . In this setup , the decodingphase sequentially predicts tokens for both ASR and its logicalform , typically starting with ASR tokens before proceeding tological form tokens for better alignment . However , previousapproaches treat the logical form as a sequence of tokens , necessitating consideration of the entire vocabulary at eachstep , which can lead to mislabeling and invalid logical form.For instance , when predicting a label token under the intent IN : CREATE_ALARM , it ’s unnecessary to consider the entirevocabulary ; only candidate tokens relevant to the slots withinthe intent IN : CREATE_ALARM , such as SL : DATE_TIME , SL : DURATION , and others , need to be considered ( Figure 1 ) . Similarly , when predicting normal tokens , such as the 13thdecoding step in Figure 1 , the model do not need to considerthe entire vocabulary ; only tokens present in the ASR scriptare relevant . To address these issues , we propose a constrainedmethodology for training and inference in spoken languageunderstanding . Drawing inspiration from prior work usinggrammar to constrain decoding steps [ 2 ] , [ 3 ] , we first extractgrammar from the annotated training data to aid in predictinglabel tokens . This grammar informs the possible label tokensbased on the parent node label . For example , if the currentdecoding output is SL : DATE_TIME and the parent node is IN : CREATE_ALARM , then , according to the grammar , thecandidate label tokens should be possible slots for the intent IN : CREATE_ALARM : [ SL : DATE_TIME , SL : DURATION , SL : AMOUNT , ... ] . For normal tokens , we constrain the modelto predict only those tokens present in the ASR script . Duringtraining , we mask all tokens not in the candidate set , allowingthe model to focus on relevant tokens for better predictions.Additionally , unlike previous work [ 16 ] , which employs mul - titask sequence spoken language understanding and representsoutput in a structured but challenging - to - predict JSON format , 
2025-02-19 13:03:50 - WARNING: origin bbox 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: {'x1': 560.544677734375, 'y1': 707.2394409179688, 'x2': 563.0353393554688, 'y2': 719.244384765625, 'width': 612.0, 'height': 792.0, 'pageNumber': 1}
2025-02-19 13:03:50 - WARNING: normalized bbox 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: {'x1': 560.544677734375, 'y1': 707.2394409179688, 'x2': 563.0353393554688, 'y2': 719.244384765625, 'width': 612.0, 'height': 792.0, 'pageNumber': 1}
2025-02-19 13:03:50 - WARNING: finalized bbox 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: {'x1': 560.544677734375, 'y1': 707.2394409179688, 'x2': 563.0353393554688, 'y2': 719.244384765625, 'width': 612.0, 'height': 792.0, 'pageNumber': 1}
2025-02-19 13:03:50 - WARNING:  final bouding box 
2025-02-19 13:03:50 - WARNING:  
2025-02-19 13:03:50 - WARNING: {'x1': 311.97796630859375, 'y1': 205.12261962890625, 'x2': 563.0408935546875, 'y2': 719.244384765625, 'width': 612.0, 'height': 792.0, 'pageNumber': 1}
2025-02-19 13:03:50 - WARNING: done save re and ner, 
2025-02-19 13:03:50 - WARNING: start count num_rel 
2025-02-19 13:03:50 - WARNING: done count num_rel 
2025-02-19 13:03:50 - WARNING: start update document 
2025-02-19 13:03:51 - WARNING: start matching infor
2025-02-19 13:03:51 - WARNING: done matching infor
2025-02-19 13:03:51 - WARNING: start commit
2025-02-19 13:03:51 - WARNING: done update document 
2025-02-19 13:03:55 - WARNING: Email sent successfully!
2025-02-19 13:03:55 - INFO: Task dev_tasks.process_pdf_task[d37b03e6-dc81-4078-891c-092258fb76ff] succeeded in 10.7276148237288s: {'id': 17281804, 'filename': '_KSE_2024__Semnatic_Enchancement_Spoken_Language_Understanding (1).pdf', 'upload_time': '2025/02/19, 04:03:44', 'entities': 16, 'relations': 0, 'pages': 6, 'status': 'completed'}
2025-02-19 13:03:55 - WARNING: 2025-02-19 13:03:55 - INFO: Task dev_tasks.process_pdf_task[d37b03e6-dc81-4078-891c-092258fb76ff] succeeded in 10.7276148237288s: {'id': 17281804, 'filename': '_KSE_2024__Semnatic_Enchancement_Spoken_Language_Understanding (1).pdf', 'upload_time': '2025/02/19, 04:03:44', 'entities': 16, 'relations': 0, 'pages': 6, 'status': 'completed'}
2025-02-19 13:05:02 - INFO: Task dev_tasks.process_pdf_task[d3291a90-996a-414d-8445-ccada568f049] received
2025-02-19 13:05:02 - WARNING: 2025-02-19 13:05:02 - INFO: Task dev_tasks.process_pdf_task[d3291a90-996a-414d-8445-ccada568f049] received
2025-02-19 13:05:02 - WARNING: uploads/_ACL_2025__LLM_Efficiency (2).pdf
2025-02-19 13:05:02 - WARNING: start parsing pdf
2025-02-19 13:05:04 - WARNING: parsed 1576 paragraphs
2025-02-19 13:05:04 - WARNING: ['SPECTRA: Faster Large Language Model Inference withOptimized Internal and External Speculation', 'Anonymous ACL submission', 'Abstract', 'Inference with modern Large Language Mod-001', 'els (LLMs) is both computationally expensive002', 'and time-consuming. Speculative decoding has003', 'emerged as a promising solution, but existing004', 'approaches face key limitations: training-based005', 'methods require a draft model that is challeng-006', 'ing to obtain and lacks generalizability, while007', 'non-training methods offer limited speedup008', 'gains. In this work, we present SPECTRA, a009', 'novel framework for accelerating LLM infer-010', 'ence without the need for additional training.011', 'SPECTRA introduces two innovative techniques012', 'for efficiently managing internal and external013', 'knowledge, each outperforming corresponding014', 'state-of-the-art (SOTA) methods independently.015', 'When combined, these techniques achieve up016', 'to a 4.08x speedup across various benchmarks017', 'and LLM architectures, significantly surpassing018', 'existing non-training approaches. The imple-019', 'mentation of SPECTRA is publicly available.020', '1Introduction021', 'Generating long sequences with low latency using022', 'Large Language Models (LLMs) is a critical re-023', 'quirement. Current LLMs rely on autoregressive024', 'decoding (Touvron et al., 2023; Bai et al., 2023;025', 'Jiang et al., 2023; OpenAI et al., 2024), which026', 'suffers from inefficiency because it generates text027', 'one token at a time. This results in generation028', 'time scaling linearly with the sequence length and029', 'underutilizes the parallel processing capabilities030', 'of modern GPUs. A widely studied approach to031', 'mitigate this issue is speculative decoding (Chen032', 'et al., 2023; Leviathan et al., 2023), which fol-033', 'lows a guess-and-verify paradigm. In this approach,034', 'a smaller LLM (draft model) (Chen et al., 2023;035', 'Leviathan et al., 2023; Miao et al., 2024; Sun et al.,036', '2023b; Zhou et al., 2024; Cai et al., 2024) or the037', 'original LLM trained in a specialized manner (self-038', 'speculative decoding) (Elhoushi et al., 2024; Liu039', 'et al., 2024a; Yang et al., 2024; Zhang et al., 2024a;040', 'Li et al., 2024b) predicts multiple tokens in ad-041', 'vance. The original LLM then verifies these pre-042', 'dictions in parallel, improving efficiency. However,043', 'these approaches require additional training, which044', 'demands substantial computational resources and045', 'may degrade the original model’s capabilities.046', 'Another line of research focuses on speculat-047', 'ing subsequent tokens without requiring additional048', 'training. This approach eliminates the need for049', 'training new models or modifying the original large050', 'language model (LLM), making it practical for051', 'off-the-shelf deployment. Some methods leverage052', 'specialized mechanisms to generate speculative to-053', 'kens directly from the LLM’s predictions (Fu et al.,054', '2024; Ou et al., 2024), while others rely on ex-055', 'ternal information sources to derive these tokens056', '(Yang et al., 2023; He et al., 2024; Li et al., 2024a).057', 'However, the speedup gain in these approaches re-058', 'mains limited due to the quality of the speculative059', 'guesses.060', 'We introduce SPECTRA (Figure 1a), a specu-061', 'lative decoding method that improves generation062', 'speed without requiring any training or modifica-063', 'tions to the original LLM. SPECTRA consists of064', 'two main components: a core module (SPECTRA-065', 'CORE, Figure 1c), which integrates seamlessly into066', 'LLMs in a plug-and-play manner, and an optional067', 'retrieval module (SPECTRA-RETRIEVAL, Figure068', '1e) that further enhances performance. The core069', 'module SPECTRA-CORE improves speculative de-070', 'coding by leveraging the token distribution pre-071', 'dicted by the LLM to generate high-quality guesses.072', 'Specifically, it employs two multi-level N-gram073', 'dictionaries that enable bi-directional search for074', 'dynamic-length guesses, balancing both quality075', 'and quantity. Additionally, SPECTRA optimizes a076', 'candidate pool to continuously update the N-gram077', 'dictionaries, ensuring broad token coverage. All078', 'updates to these resources, along with guess verifi-079', 'cation, are performed efficiently in a single forward080', 'pass. The retrieval module, SPECTRA-RETRIEVAL,081', '1', 'SPECTRA-RETRIEVAL', 'Module', 'N-gram Store', 'Input', 'Lookahead', 'Candidate', 'LLM', '(b) Lookahead decoding(c) SPECTRA-CORE', 'Guesses', 'Input', 'SPECTRA', 'Candidate', 'LLM', 'Bi-directional', 'Guesses', 'Multi-level', 'N-gram Store', 'InputCorpus', 'Guesses', 'LLM', 'Trie', 'Input', 'Guesses', 'LLM', 'Trie', '(d) REST', 'Corpus Refined', 'by Perplexity', '(e) SPECTRA-RETRIEVAL', '(a) Ours: SPECTRA', 'SPECTRA-CORE', 'Module', 'Input', 'GuessesLLM', 'Figure 1: Overview of Spectra and comparison with other non-training SOTA approaches. (a) Overview of SPECTRA.(b) Overview of Lookahead Decoding (Fu et al., 2024). (c) Overview of the SPECTRA-CORE module, which utilizesthe knowledge inside LLM for obtaining guesses. (d) Overview of REST (He et al., 2024). (e) Overview of theSPECTRA-RETRIEVAL module, which is designed to be integrated efficiently with SPECTRA-CORE to boost thespeedup. The results in the bar chart are measured on HumanEval.', 'can be integrated to further enhance speedup. Ex-082', 'isting approaches that rely on external sources for083', 'generating guesses (He et al., 2024) struggle to in-084', 'tegrate with other speculative decoding methods,085', 'as the search time outweighs the speedup gains.086', 'SPECTRA-RETRIEVAL addresses this issue by re-087', 'ducing the search space, selecting only high-quality088', 'content from the corpus based on perplexity scores089', 'computed by the target LLM. This optimization en-090', 'ables seamless integration with SPECTRA-CORE,091', 'maximizing efficiency.092', 'Empirical results on six tasks—including multi-093', 'turn conversation, code generation, and mathemati-094', 'cal reasoning—across three LLM families (Llama095', '2 (Touvron et al., 2023), Llama 3 (Dubey et al.,096', '2024), and CodeLlama (Rozière et al., 2024)) with097', 'model sizes ranging from 7B to 70B demonstrate098', 'that SPECTRA outperforms other non-training spec-099', 'ulative decoding methods, achieving speedups of100', 'up to 4x. We publicly release the code and data.101', 'The key contributions of this paper are as follows:102', '• We introduce SPECTRA, which improves spec-103', 'ulative decoding by effectively leveraging the104', 'LLM’s predicted token distribution. SPEC-105', 'TRA is a plug-and-play solution that requires106', 'no modifications to the LLM (Section 3.1).107', '• SPECTRA’s retrieval module refines external108', 'corpora using perplexity scores computed by109', 'the target LLM, providing a general frame-110', 'work that enables speculative decoding ap-111', 'proaches relying on external information to112', 'be seamlessly integrated with other specula-113', 'tive decoding techniques (Section 3.2).114', '• Extensive experiments across diverse tasks,115', 'LLM architectures, GPU types, and settings116', 'demonstrate the efficiency of SPECTRA, out-117', 'performing other non-training speculative de-118', 'coding approaches (Section 5).SPECTRA119', 'also integrates with acceleration tools such as120', 'FlashAttention and pipeline parallelism (Sec-121', 'tion 5.2). The code and data are available.122', '2Preliminaries123', '2.1Autoregressive Decoding in LLMs124', 'Given an input sequence x = (x1, x2, . . . , xs)125', 'of length s, and a slice of length m as x1:m =126', '(x1, x2, . . . , xm), the output of an LLM repre-127', 'sents a probability distribution over the next to-128', 'ken. The probability of generating the s-th token,129', 'conditioned on all preceding tokens, is given by130', 'PM(xs | x1:s−1). The next token xs is sampled131', 'from this distribution using methods such as greedy,132', 'top-k, or top-p sampling (see (Kool et al., 2020;133', 'Holtzman et al., 2020)). For greedy sampling, the134', 'next token is selected as xs = arg max PM(xs |135', 'x1:s−1). Consequently, the LLM generates an out-136', 'put sequence (y1, y2, . . . , ym) of length m autore-137', 'gressively, where each token yi is computed as138', '2', 'yi = argmax PM(yi | y1:i−1, x).139', '2.2Speculative Decoding140', 'Speculative decoding follows a guess-and-verify141', 'approach, where multiple candidate future to-142', 'kens are speculated and subsequently verified143', 'in a single decoding step.With tree attention144', '(Miao et al., 2024), multiple drafts can be ver-145', 'ified simultaneously. Let G denote the number146', 'of guesses, and define the set of guesses as ˜Y =147', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence148', 'has length K. The j-th token of the i-th guess is149', 'denoted as ˜y(i) j .150', 'In the case of speculative decoding with greedy151', 'sampling, given the prompt x, a drafting method152', 'generates the draft sequences ˜Y . Using these drafts,153', 'the LLM computes the true tokens (y′1, y′2, . . . , y′K)154', 'in parallel. These tokens are then verified, and155', 'h is defined as the highest number of correctly156', 'guessed tokens across all guesses. Consequently,157', 'h + 1 tokens are generated in a single forward158', 'step. Algorithm 2 outlines speculative decoding159', 'with greedy sampling, and additional details are160', 'provided in Appendix A.161', '3SPECTRA DECODING162', 'SPECTRA consists of two modules (SPECTRA-163', 'CORE and SPECTRA-RETRIEVAL) that can func-164', 'tion independently or together. The core module165', '(SPECTRA-CORE) improves speedup by leveraging166', 'the LLM’s predicted token distribution to gener-167', 'ate high-quality guesses and integrates into LLMs168', 'in a plug-and-play manner. The retrieval module169', '(SPECTRA-RETRIEVAL) derives guesses from a re-170', 'fined external information source and is designed to171', 'integrate with SPECTRA-CORE to further enhance172', 'performance.173', '3.1SPECTRA-CORE174', 'SPECTRA-CORE maintains an N-gram storage and175', 'a candidate pool. The candidate pool C contains176', 'W sequences, {c(0), c(1), . . . , c(W−1)}, with each177', 'sequence consisting of N tokens. Let c(i) jrepresent178', 'the j-th token in the i-th sequence. The N-gram179', 'storage includes two dictionaries: the forward dic-180', 'tionary Sfwd and the backward dictionary Sbwd. At181', 'each time step, guesses G are obtained through a182', 'bidirectional search using Sfwd and Sbwd. A sin-183', 'gle forward pass to the LLM retrieves all neces-184', 'sary distributions, which are used to generate new185', 'candidate tokens for C and verify the guesses G.186', 'Algorithm 1 SPECTRA Internal Knowledge', 'Require: Sequence x = (x1, x2, . . . , xn), model PM, maxN-gram size N, candidate pool size W, max guesses G,max number of new tokens m. Refine threshold τ', '1: Initialize N-gram Forward-dictionary Sfwd ←∅2: Initialize N-gram Backward-dictionary Sbwd ←∅3: Random c(i) j , ∀j ∈[0, N −1], ∀i ∈[0..W −1]', '4: t ←n + 15: while t ≤m do6:{Obtain the guesses}', '7:G ←Sfwd[xt−1]', '8:u = ∅', '9:for j = 0 to N −1 do', '10:for k = N −1 to 1 do', '11:uj ←Sbwd[xt+j−k:t−1 ⊕u0:j−1]', '12:break if found value for uj', '13:end for', '14:end for', '15:G.append(u)', '16:{Foward in LLM}', '17:Obtain necessary distributions of PM in parallel.', '18:{Verification}', '19:{Greedy verify (Alg. 3) or Sampling verify (Alg. 4)}', '20:hits ←VerificationFunction(x, PM, g)', '21:x = x ⊕hits', '22:t ←t + size(hits)', '23:{Predict Candidates}', '24:for i = 0 to W −1 do', '25:r ∼Uniform[0, 1]', '26:Pc(c(i) N−1) ←PM(c(i) N−1 | c(i) :N−2, x)', '27:if r > τ then', '28:c(i) N−1 ←argmax c/∈SfwdPc(c(i) N−1)', '29:else', '30:c(i) N−1 ←argmax Pc(c(i) N−1)', '31:end if', '32:end for', '33:{Update N-gram dictionaries}', '34:for i = 0 to W −1 do', '35:for j = 0 to N −2 do', '36:Sfwd[c(i) j ].append(c(i) j+1:)', '37:Sbwd[c(i) 0:j] ←c(i) j+1', '38:end for', '39:end for', '40:{Update Candidates}', '41:c(i) j←c(i) j+1, ∀j ∈[0, N −2], ∀i', '42: end while43: Output: xn+1:n+m = (y1, y2, . . . , ym)', 'The dictionaries Sfwd and Sbwd are updated with N-187', 'grams from the candidate pool. The details of the188', 'SPECTRA-CORE decoding process are described189', 'in Algorithm 1.190', 'Bi-directional Search for GuessesAt each step,191', 'SPECTRA generates G guess sequences G=192', '{˜y(0), ˜y(1), . . . , ˜y(G)}. Unlike previous work (Fu193', 'et al., 2024), which enforces uniform guess lengths,194', 'SPECTRA supports variable-length guesses, im-195', 'proving both flexibility and efficiency. The for-196', 'ward dictionary Sfwd maps a token to a list of197', 'sequences, while the backward dictionary Sbwd198', 'maps a sequence to a single token. At time step199', '3', 'Input', 'Input', 'Large Language Model', 'Token distribution', 'Figure 2: Details of SPECTRA forward step in LLM.The dashed arrow indicates interactions between thetokens, which are realized by the LLM’s attention mask.', 't, the set of guesses is obtained through a bidi-200', 'rectional search (Alg. 1, lines 7–15). This search201', 'operates in two directions: (1) the forward direc-202', 'tion, which prioritizes the quantity of guesses, and203', '(2) the backward direction, which prioritizes the204', 'quality of guesses. In the forward direction, the205', 'last generated token xt−1 is used to search Sfwd206', 'for guess sequences (Alg. 1, line 7). In the back-207', 'ward direction, a high-quality guess is constructed208', 'by iteratively predicting one token at a time using209', 'Sbwd, repeating the process until a desired sequence210', 'length N is reached (Alg. 1, lines 8–14).211', 'Predict & Verify in One Forward PassAll dis-212', 'tributions required for predicting candidates and213', 'verifying guesses are obtained in a single forward214', 'pass to the LLM, leveraging parallel processing215', '(Figure 2). This is achieved using a specially de-216', 'signed attention mask that specifies the allowed217', 'interactions between tokens. For instance, the to-218', 'ken c(1) 2attends only to c(1) 1 , c(1) 0 , and the input.219', 'Predict Tokens for Candidate PoolWe predict220', 'the next candidate tokens c(i) N−1 for the candidate221', 'pool using the distribution obtained from the for-222', 'ward pass (Alg. 1, lines 24–32). A straightfor-223', 'ward approach is to select tokens with the highest224', 'probability in the token distribution. However, we225', 'observe that when searching for guesses in the for-226', 'ward dictionary Sfwd, it is crucial for the search to-227', 'ken to exist in the dictionary; otherwise, no guesses228', 'can be retrieved. To address this, we introduce a229', 'randomness-based mechanism to increase the cov-230', 'erage of Sfwd. Specifically, we probabilistically231', 'encourage the selection of unseen tokens in Sfwd232', 'using a hyperparameter τ ∈[0, 1]. Let r be a233', 'random draw from [0, 1]. If r > τ, we select to-234', 'kens with the highest probability that are not in235', 'Sfwd; otherwise, we choose tokens with the high-236', 'est probability regardless of their presence in Sfwd.237', 'Although c(i) N−1 does not immediately affect the238', 'coverage of Sfwd, it contributes to coverage expan-239', 'sion in subsequent time steps through our candidate240', 'updating mechanism. At the end of each time step,241', 'all candidate sequences are shifted left by one to-242', 'ken: c(i) j←c(i) j+1, leaving c(i) N−1 empty and ready243', 'for prediction in the next time step (Alg. 1, line 41).244', 'Update N-gram DictionariesAt the end of each245', 'time step, candidate tokens from the pool C are246', 'used to update the N-gram dictionaries Sfwd and247', 'Sbwd. While previous work (Fu et al., 2024) only248', 'adds the full N-gram (c(i) 0 , c(i) 1 , . . . , c(i) N ), we ob-249', 'serve that subsequences within N-grams often ap-250', 'pear later in the generation process. By including251', 'these subsequences in the N-gram storage, we im-252', 'prove both the quality of guesses and the coverage253', 'of the dictionaries. Specifically, we add subse-254', 'quences to Sfwd using the first token as the key,255', 'and update Sbwd by mapping the preceding part of256', 'the sequence to the last token (Alg. 1, lines 33–39).257', '3.2SPECTRA-RETRIEVAL258', 'SPECTRA-RETRIEVALleveragesanexternal259', 'knowledge source to generate guesses. This in-260', 'volves processing a text corpus and indexing it into261', 'a structure that supports fast prefix search, such as a262', 'trie. At each time step, the last generated tokens are263', 'used as input to this structure to retrieve guesses for264', 'speculative decoding. However, we observe that us-265', 'ing random texts from the corpus without selection266', 'can limit the speedup gain. To address this, we pro-267', 'pose a method to identify and select high-quality,268', 'relevant texts from the corpus tailored to the spe-269', 'cific LLM. This improves the speedup gain and270', 'enables seamless integration with other speculative271', 'decoding approaches, including SPECTRA-CORE.272', 'Corpus Refinement by PerplexityGiven a text sequence u = (u0, u1, u2, . . . ), perplexity quan-tifies the average uncertainty of the model when predicting the next token, conditioned on the pre-ceding tokens. It is calculated as:', 'PPL(u) = exp', '�', '−1', 't', 't�', 'i=1log pθ(ui | u<i)', '�', 'A lower perplexity indicates that the model assigns273', 'higher probabilities to the sequence, suggesting274', 'that the sequence is well-aligned with the model’s275', 'predictions and can produce high-quality guesses276', 'for speculative decoding. To optimize the retrieval277', 'process, we select texts with the lowest perplexity278', '4', 'from the corpus to form a smaller, high-quality sub-279', 'set, which is then used to construct the trie structure280', 'for generating guesses.281', 'Integration with SPECTRA-COREOur exper-282', 'iments (Section 5.2, Table 2) demonstrate that283', 'naively integrating guesses from external sources284', '(e.g., REST (He et al., 2024)) into other specula-285', 'tive methods (e.g., Lookahead (Fu et al., 2024))286', 'can lead to a noticeable drop in speedup. This oc-287', 'curs because the forward pass in the LLM can only288', 'handle a limited number of guesses, and exceeding289', 'this limit increases memory usage and slows down290', 'generation. With a limited guess budget, guesses291', 'from external sources can only account for a frac-292', 'tion of the total guesses, causing the search time293', 'in the indexing structure (e.g., a trie) to outweigh294', 'the speedup gain. To address this, it is crucial295', 'to limit the size of the external knowledge while296', 'maintaining the quality of the guesses. By refining297', 'the corpus using perplexity, SPECTRA-RETRIEVAL298', 'seamlessly integrates with SPECTRA-CORE, further299', 'boosting the speedup gain. Specifically, we inte-300', 'grate SPECTRA-RETRIEVAL into SPECTRA-CORE301', 'by including its guesses in the set of guesses during302', 'the guess generation step (Alg. 1, lines 7–15).303', '4Experiments304', 'Models.We evaluate LLaMA-2-Chat 7B, 13B,305', '70B (Touvron et al., 2023), CodeLlama 7B, 13B306', '(Rozière et al., 2024), and LLaMA-3-Instruct 8B,307', '70B (Dubey et al., 2024).308', 'Tasks.Weconductcomprehensiveevalua-309', 'tions on various generation tasks.MT-Bench310', '(Zheng et al., 2023) for multi-turn conversation;311', 'GSM8K(Cobbe et al., 2021) for mathemati-312', 'cal reasoning; HumanEval(Chen et al., 2021),313', 'MBPP(Austin et al., 2021) and ClassEval (Du314', 'et al., 2023) for code generation.315', 'Metrics.SPECTRA does not modify the original316', 'LLM and the acceptance conditions, making it a317', 'lossless acceleration method. Therefore, the gener-318', 'ation quality remains the same as the original LLM.319', 'We only evaluate the acceleration performance us-320', 'ing the following metrics.321', '• Speedup Ratio: The speedup ratio relative to322', 'autoregressive decoding.323', '• Compression ratio: The ratio of the total324', 'number of autoregressive steps to the number325', 'of Spectra decoding steps needed to produce326', 'the same sequence length.327', 'Baselines.We use standard autoregressive decod-328', 'ing as the baseline (speed-up ratio = 1.00x). We fur-329', 'ther compare SPECTRA with leading non-training330', 'speculative decoding approaches, namely Adaptive331', 'N-gram (Ou et al., 2024), REST (He et al., 2024),332', 'and Lookahead (Fu et al., 2024). For details regard-333', 'ing implementation settings of both SPECTRA and334', 'these baselines, please refer to Appendix B.335', '5Results336', '5.1Main Results337', 'Overall Performance.The top portion of Table 1338', 'presents the speedup ratios of all evaluated meth-339', 'ods under a greedy decoding setup. Our approach,340', 'SPECTRA, consistently yields the highest accelera-341', 'tion across the entire range of datasets and LLMs.342', 'In particular, SPECTRA achieves speedups up to343', '4.08× with LLama-3-8B-Instruct on the MBPP344', 'dataset.345', 'For smaller models (7B), SPECTRA often sur-346', 'passes 3× acceleration, underscoring the effective-347', 'ness of multi-token compression. By contrast, for348', '13B models, while the boost remains strong, it is349', 'relatively more moderate, typically falling in the350', '1.6×–3× band. We attribute this trend to the in-351', 'creased overhead of each forward pass in larger net-352', 'works, which can dampen the proportional gains of353', 'fewer decoding iterations per token. Despite this,354', 'SPECTRA continues to outperform baselines across355', 'all parameter settings.356', 'Significant advantages are evident in tasks such357', 'as GSM8K and ClassEval, where outputs often358', 'follow recurring patterns (e.g., repeated variable359', 'names or class definitions). In these scenarios,360', 'SPECTRA combines internal knowledge of par-361', 'tial sequences with external retrieval suggestions,362', 'thereby proposing accurate multi-token guesses.363', 'On the other hand, in domains featuring more var-364', 'ied or unpredictable responses—such as complex365', 'multi-turn conversations in MT-Bench—the accep-366', 'tance rate is somewhat lower, although still com-367', 'petitive.368', 'Compression Ratio.Table 1 also reports each369', 'method’s compression rate, a measure agnostic370', 'to specific hardware configurations. Across ev-371', 'ery dataset and LLM tested, SPECTRA delivers the372', 'highest average compression ratio. Each of SPEC-373', 'TRA’s draft-and-verify iterations typically yields374', '5', 'ClassevalGSM8KHumanevalMBPPMTBenchAVGModelMethodspeedupτspeedupτspeedupτspeedupτspeedupτspeedup', 'Greedy (temperature=0)', 'CL-13BANPD1.942.522.813.722.082.502.713.582.613.412.43 CL-13BLookahead2.253.612.804.242.303.162.914.442.594.042.57 CL-13BREST1.282.140.931.541.582.310.851.400.941.531.12 CL-13BSPECTRA (Ours)2.384.062.914.652.633.953.294.462.654.402.77', 'CL-7BANPD2.302.683.213.752.162.473.163.783.353.832.84 CL-7BLookahead2.593.662.993.832.503.052.903.673.234.272.84 CL-7BREST1.452.220.911.391.702.340.961.451.021.441.21 CL-7BSPECTRA (Ours)2.704.103.334.592.963.903.564.453.704.523.25', 'L2-13BANPD1.361.781.471.721.341.611.121.321.171.371.29 L2-13BLookahead1.812.761.461.871.732.321.381.691.512.041.58 L2-13BREST1.222.010.941.461.251.940.951.441.141.901.10 L2-13BSPECTRA (Ours)2.003.241.832.621.962.911.632.241.752.601.83', 'L2-70BANPD1.821.901.631.611.861.871.171.201.341.301.56 L2-70BLookahead2.652.871.862.022.572.671.491.541.942.002.10 L2-70BSPECTRA (Ours)3.103.402.522.693.223.371.861.932.432.512.62', 'L2-7BANPD1.621.951.521.681.541.671.191.331.301.371.43 L2-7BLookahead2.192.941.661.932.062.421.461.691.732.051.82 L2-7BREST1.362.121.011.471.412.041.011.461.251.901.21 L2-7BSPECTRA (Ours)2.403.432.112.642.403.051.772.162.022.592.14', 'L3-70BANPD1.541.671.501.471.831.881.461.411.231.231.51 L3-70BLookahead2.402.621.541.582.562.701.431.451.761.861.94 L3-70BSPECTRA (Ours)2.672.912.102.142.843.021.941.942.062.132.32', 'L3-8BANPD2.112.493.864.571.832.093.363.581.141.232.46 L3-8BLookahead2.593.443.714.612.492.893.794.651.531.852.82 L3-8BSPECTRA (Ours)2.833.493.894.772.573.024.084.761.692.103.01', 'Sampling (temperature=1.0)', 'CL-13BANPD1.151.461.071.311.051.301.001.242.312.891.31 CL-13BLookahead1.382.001.081.431.291.751.021.342.333.481.42 CL-13BREST1.141.870.821.351.271.960.841.390.931.501.00 CL-13BSPECTRA (Ours)1.682.221.201.751.652.121.151.702.373.801.61', 'CL-7BANPD1.291.501.161.301.101.321.121.272.773.051.49 CL-7BLookahead1.542.031.191.411.431.811.191.432.723.501.61 CL-7BREST1.231.860.881.331.331.980.911.400.971.441.06 CL-7BSPECTRA (Ours)1.812.251.351.731.682.121.331.722.783.941.79', 'L2-13BANPD1.201.521.241.461.171.401.031.221.171.351.16 L2-13BLookahead1.522.221.321.691.482.001.181.481.492.011.40 L2-13BREST1.181.960.931.451.191.880.921.441.121.881.07 L2-13BSPECTRA (Ours)1.702.751.552.231.692.591.341.891.742.571.60', 'L2-7BANPD1.311.511.341.481.281.461.101.221.251.361.26 L2-7BLookahead1.782.301.511.761.722.091.251.491.682.021.59 L2-7BREST1.262.030.991.461.271.930.961.411.211.881.14 L2-7BSPECTRA (Ours)1.972.831.782.282.042.751.471.841.972.541.85', 'L3-8BANPD1.251.371.972.181.431.651.892.071.151.211.54 L3-8BLookahead1.481.782.072.411.792.211.992.401.571.811.78 L3-8BSPECTRA (Ours)1.942.842.272.781.922.512.192.781.702.052.01', 'Table 1: Overall performance of speculative decoding methods across multiple tasks. “CL-xB” denotes CodeLlamawith xB parameters, “L2-xB” denotes LLaMA-2-Chat of size xB, and “L3-xB” denotes LLaMA-3-Instruct of size xB. We report the speedup ratio (vs. autoregressive) and the compression ratio τ.', '2.1–4.8 tokens, substantially outpacing alternative375', 'approaches and nearly doubling the acceptance376', 'length achieved by REST.377', 'Acceleration in Sampling Decoding.The lower378', 'section of Table 1 investigates the performance379', 'of SPECTRA under sampling-based decoding with380', 'a temperature of 1.0. The results highlight how381', '6', 'SPECTRA continues to accelerate generation rel-382', 'ative to baselines, offering roughly 1.15–2.77×383', 'speedups over standard autoregressive decoding.384', 'These gains are more modest than in greedy decod-385', 'ing, reflecting the lower acceptance rate under the386', 'sampling-based verification phase, which is consis-387', 'tent with earlier findings (Fu et al., 2024; Leviathan388', 'et al., 2023).389', '5.2Analysis390', 'Ablationstudy.Weconductedadetailed391', 'component-wise analysis to determine the contri-392', 'bution of each module to the framework’s over-393', 'all performance (Table 2). Specifically, the re-394', 'sults on LLaMA2-7B-chat reveal that removing395', 'different components yields varying impacts on396', 'GSM8K speedups.Under the “CORE Module”397', 'configuration, excluding multi-level n-grams low-398', 'ers the speedup from 2.04× to 1.95× (a 4% de-399', 'crease), whereas turning off forward information400', 'reduces it from 2.04× to 1.50× (a 26% drop). Sim-401', 'ilarly, omitting backward information results in402', 'a speedup of 1.94×, down from 2.04×. In con-403', 'trast, the “RETRIEVAL Module” setting shows that404', 'leaving out perplexity-based filtering decreases the405', 'speedup from 1.18× to 1.16×. Our fully integrated406', 'approach, SPECTRA, achieves a 2.14× speedup407', 'on GSM8K—outperforming both the “CORE Mod-408', 'ule” (2.04×) and “RETRIEVAL Module” (1.18×)409', 'variants. This improvement demonstrates the im-410', 'portance of combining multi-level n-grams, for-411', 'ward/backward drafting, and perplexity-based re-412', 'finement in boosting acceptance rates and enhanc-413', 'ing overall speedups. A similar trend was also414', 'observed in the results of the MTBench dataset.415', 'Additionally,wecomparedourmethod416', 'against a naive combination of Lookahead and417', 'REST—where guess sequences from REST are418', 'added to Lookahead. This combined approach419', 'falls significantly short of our SPECTRA method,420', 'highlighting that a simple merger of two techniques421', 'is insufficient without our carefully optimized422', 'integration strategy and components.423', 'Priority for source of guessesSince verifying424', 'too many candidate tokens at once can strain GPU425', 'resources and reduce speedups (Fu et al., 2024; Li426', 'et al., 2024b), SPECTRA limits how many guesses427', 'proceed to verification in each step (Appendix B).428', 'To understand whether internal or external guesses429', 'are more valuable, we temporarily remove this430', 'cap and measure acceptance rates (Figure 3). We431', 'GSM8KMTBenchMethodspeedupτspeedupτ', 'REST1.011.471.251.90', 'Lookahead1.661.931.732.05', 'Lookahead + REST1.081.471.271.90', 'SPECTRA’s ablation', 'CORE Module2.042.501.922.35 - w/o forward info1.501.681.201.37 - w/o backward info1.942.211.742.12 - w/o Sub-Ngram1.952.341.752.18', 'RETRIEVAL Module1.181.311.241.50 - w/o PPL refine1.161.291.201.45', 'SPECTRA (ours)2.142.642.022.59', 'Table 2: Ablation of SPECTRA’s components (greedydecoding, LLaMA2-7B-Chat).“Sub-Ngram” aug-ments each n-gram with its sub-sequences; “for-ward/backward info” uses internal expansions; and“PPL refine” applies perplexity-based filtering for ex-ternal retrieval. “Lookahead + REST” denotes a naivecombination where guess sequences from REST are di-rectly added to Lookahead', 'observe that sequences generated via internal ex-432', 'pansions—particularly forward and backward pre-433', 'dictions—have a higher acceptance probability434', 'than those retrieved from external sources. Con-435', 'sequently, SPECTRA prioritizes internal guesses436', 'for verification. Interestingly, in code-generation437', 'tasks like HumanEval, external suggestions be-438', 'come more influential, likely due to code’s repeti-439', 'tive structure and the retrieval of similar snippets.440', 'This observation indicates that a strategic blend441', 'of backward internal knowledge and external re-442', 'trieval can be particularly fruitful in these domains,443', 'especially when computational resources limit ex-444', 'tensive forward expansions.445', 'FlashAttention.Figure 3 shows that enabling446', 'FlashAttention consistently boosts the speedup of447', 'all methods, albeit to varying degrees. Notably,448', 'we observe an additional 0.24× speedup gain for449', 'SPECTRA on both GSM8K and MTBench. This450', 'is because FlashAttention better exploits the paral-451', 'lel structure of speculative decoding by reducing452', 'attention overheads, especially when verifying mul-453', 'tiple guessed tokens in parallel. Although smaller454', 'gains are also seen for other methods, SPECTRA455', 'benefits the most, as it presents the longest verifica-456', 'tion branches and thus stands to profit significantly457', 'from more efficient attention implementations.458', '7', '020406080100', 'Total accept tokens (%)', 'ClassEval', 'GSM8K', 'HumanEval', 'MBPP', 'MT-Bench', 'Internal-FwdInternal-BwdExternal', 'Figure 3: Acceptance rates for different guess sources(e.g., SPECTRA-CORE forward dictionary, backwarddictionary, SPECTRA-RETRIEVAL’s guesses). The ac-ceptance rate is the fraction of guessed tokens that passverification and are appended to the final output.', '0', '1', '2', 'Speedup', '1.00x', '1.68x1.55x', '1.03x', '2.08x', '1.07x', '1.86x1.70x', '1.09x', '2.32x', 'GSM8K', 'W/o flashWith flash', 'Autoreg.LookaheadANPDRESTSpectra', '0', '1', '2', 'Speedup', '1.00x', '1.75x', '1.31x1.24x', '2.01x', '1.10x', '1.96x', '1.42x1.38x', '2.25x', 'MTBench', 'W/o flashWith flash', 'Figure 4: Effect of FlashAttention on speculative de-coding speed: Measured speedups on GSM8K andMTBench (LLama2-7B-Chat, greedy decoding). “NoFlash” uses standard attention; “With Flash” usesFlashAttention for faster parallel verification.', '6Related Works459', 'Large language models (LLMs) are increasingly460', 'deployed in a range of applications, motivating on-461', 'going research into more efficient inference (Liu462', 'et al., 2025). Common strategies include quan-463', 'tizing model weights into lower-precision formats464', '(Liu et al., 2024b; Lin et al., 2024; Zhao et al., 2024;465', 'Park et al., 2024), pruning redundant parameters466', '(Ma et al., 2023; Xia et al., 2023; Sun et al., 2023a;467', 'Le et al., 2025), and employing knowledge distilla-468', 'tion (Gu et al., 2024; Friha et al., 2024; Zhang et al.,469', '2024b). These techniques help reduce the compu-470', 'tational load per forward pass, thereby lowering471', 'generation latency. However, they often introduce472', 'some degradation in model performance, forcing473', 'practitioners to balance quality with efficiency.474', 'A growing line of work explores speculative de-475', 'coding as a strategy for accelerating generation476', 'while maintaining the exact output distribution477', '(Chen et al., 2023; Leviathan et al., 2023). Some478', 'speculative decoding approaches train a smaller479', 'LLM (referred to as a draft model) (Chen et al.,480', '2023; Leviathan et al., 2023; Miao et al., 2024; Sun481', 'et al., 2023b; Zhou et al., 2024; Cai et al., 2024),482', 'or train the original LLM itself in a special man-483', 'ner (referred to as self-speculative) (Elhoushi et al.,484', '2024; Liu et al., 2024a; Yang et al., 2024; Zhang485', 'et al., 2024a; Li et al., 2024b) to guess several sub-486', 'sequent tokens and then verify them parallelly us-487', 'ing the original LLM. As these approaches require488', 'training, they pose limitations, such as requiring489', 'heavy computational resources and losing the orig-490', 'inal model capabilities.491', 'To avoid additional training, alternative specula-492', 'tive decoding methods leverage external resources493', 'or structural properties of language generation.494', 'Retrieval-based methods sidestep draft model train-495', 'ing by using a datastore indexed with observed496', 'prefixes to retrieve guess sequences (Yang et al.,497', '2023; He et al., 2024; Li et al., 2024a). Other498', 'approaches, such as Jacobi-like parallel decoding499', '(Santilli et al., 2023) and lookahead decoding (Fu500', 'et al., 2024), mitigate left-to-right dependencies by501', 'generating and validating multiple candidate tokens502', 'in parallel. These training-free techniques achieve503', 'comparable speedups to learned methods without504', 'requiring model optimization, making them ideal505', 'for scenarios with computational or deployment506', 'constraints.507', '7Conclusions508', 'In this work, we introduced SPECTRA, a hybrid509', 'speculative decoding framework that combines510', 'multi-level n-grams (internal knowledge) with511', 'perplexity-based retrieval (external knowledge) to512', 'achieve speedups of up to 4.08× across various513', 'LLMs and benchmarks, without additional train-514', 'ing or compromising exact output fidelity. Our515', 'ablation studies show that each module (multi-516', 'level n-grams, forward/backward expansions, and517', 'perplexity-based datastore curation) substantially518', 'boosts acceptance rates, and their synergy outper-519', 'forms existing non-training methods. By offering a520', 'lossless speedup that efficiently exploits both inter-521', 'nal patterns and external texts, SPECTRA provides522', 'a practical, high-impact solution for accelerating523', 'inference in large language models.524', '8', '8Limitations525', '(1) Cost of Building External Datastores.Al-526', 'though our internal-knowledge strategy only relies527', 'on sequences observed during generation (and thus528', 'requires no extra data), our external-knowledge529', 'approach depends on constructing and indexing a530', 'sizeable datastore from potentially large corpora.531', 'This process can be time-consuming and memory-532', 'intensive, particularly in domains where data up-533', 'dates frequently or storage is constrained. While534', 'this additional investment can yield substantial535', 'speedups by increasing token acceptance rates, it536', 'may not be universally feasible or cost-effective.537', '(2) Limited Evaluation Scope.Our experiments538', 'center primarily on English-language benchmarks539', 'in conversational and coding tasks using LLaMA-540', 'based models. Although SPECTRA can, in princi-541', 'ple, be applied to other models or languages, addi-542', 'tional factors such as domain-specific tokenization543', 'or specialized textual structures may affect the ac-544', 'ceptance rate and overall speedup. Future work is545', 'needed to assess the generality of SPECTRA across546', 'diverse linguistic settings (e.g., low-resource lan-547', 'guages or specialized technical documents) and for548', 'a wider range of model families (beyond LLaMA-549', 'based architectures) to confirm and refine its appli-550', 'cability.551', 'References552', 'Jacob Austin, Augustus Odena, Maxwell Nye, Maarten553', 'Bosma, Henryk Michalewski, David Dohan, Ellen554', 'Jiang, Carrie Cai, Michael Terry, Quoc Le, and others.555', '2021. Program synthesis with large language models.556', 'arXiv preprint arXiv:2108.07732.557', 'Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,558', 'Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei559', 'Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,560', 'Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,561', 'Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,562', 'Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong563', 'Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-564', 'guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,565', 'Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,566', 'Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-567', 'uan Zhang, Yichang Zhang, Zhenru Zhang, Chang568', 'Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang569', 'Zhu. 2023. Qwen Technical Report.570', 'Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu571', 'Peng, Jason D. Lee, Deming Chen, and Tri Dao.572', '2024. MEDUSA: Simple LLM inference acceler-573', 'ation framework with multiple decoding heads. In574', 'Proceedings of the 41st International Conference on575', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-576', 'enna, Austria.577', 'Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,578', 'Jean-Baptiste Lespiau, Laurent Sifre, and John579', 'Jumper. 2023. Accelerating Large Language Model580', 'Decoding with Speculative Sampling.581', 'Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,582', 'Henrique Ponde De Oliveira Pinto, Jared Kaplan,583', 'Harri Edwards, Yuri Burda, Nicholas Joseph, Greg584', 'Brockman, and others. 2021.Evaluating large585', 'language models trained on code. arXiv preprint586', 'arXiv:2107.03374.587', 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,588', 'Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias589', 'Plappert, Jerry Tworek, Jacob Hilton, Reiichiro590', 'Nakano, and others. 2021.Training verifiers591', 'to solve math word problems.arXiv preprint592', 'arXiv:2110.14168.593', 'Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin,594', 'Shengding Hu, Zhiyuan Liu, Maosong Sun, and595', 'Bowen Zhou. 2023. Enhancing Chat Language Mod-596', 'els by Scaling High-quality Instructional Conversa-597', 'tions. In Proceedings of the 2023 Conference on598', 'Empirical Methods in Natural Language Processing,599', 'pages 3029–3051, Singapore. Association for Com-600', 'putational Linguistics.601', 'Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang,602', 'Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng603', 'Sha, Xin Peng, and Yiling Lou. 2023.Classe-604', 'val: A manually-crafted benchmark for evaluating605', 'llms on class-level code generation. arXiv preprint606', 'arXiv:2308.01861.607', 'Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,608', 'Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,609', 'Akhil Mathur, Alan Schelten, Amy Yang, Angela610', 'Fan, et al. 2024. The llama 3 herd of models. arXiv611', 'preprint arXiv:2407.21783.612', 'Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,613', 'Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas614', 'Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed615', 'Roman, Ahmed Aly, Beidi Chen, and Carole-Jean616', 'Wu. 2024. LayerSkip: Enabling Early Exit Infer-617', 'ence and Self-Speculative Decoding. In Proceedings618', 'of the 62nd Annual Meeting of the Association for619', 'Computational Linguistics (Volume 1: Long Papers),620', 'pages 12622–12642, Bangkok, Thailand. Association621', 'for Computational Linguistics.622', 'Othmane Friha, Mohamed Amine Ferrag, Burak623', 'Kantarci, Burak Cakmak, Arda Ozgun, and Nassira624', 'Ghoualmi-Zine. 2024. Llm-based edge intelligence:625', 'A comprehensive survey on architectures, applica-626', 'tions, security and trustworthiness. IEEE Open Jour-627', 'nal of the Communications Society.628', 'Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.629', '2024.Break the sequential dependency of LLM630', 'inference using LOOKAHEAD DECODING. In631', 'Proceedings of the 41st International Conference on632', '9', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-633', 'enna, Austria.634', 'Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024.635', 'Minillm: Knowledge distillation of large language636', 'models. In The Twelfth International Conference on637', 'Learning Representations.638', 'Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and639', 'Di He. 2024. REST: Retrieval-Based Speculative640', 'Decoding. In Proceedings of the 2024 Conference of641', 'the North American Chapter of the Association for642', 'Computational Linguistics: Human Language Tech-643', 'nologies (Volume 1: Long Papers), pages 1582–1595,644', 'Mexico City, Mexico. Association for Computational645', 'Linguistics.646', 'Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and647', 'Yejin Choi. 2020. The Curious Case of Neural Text648', 'Degeneration. In International Conference on Learn-649', 'ing Representations.650', 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-651', 'sch, Chris Bamford, Devendra Singh Chaplot, Diego652', 'de las Casas, Florian Bressand, Gianna Lengyel, Guil-653', 'laume Lample, Lucile Saulnier, Lélio Renard Lavaud,654', 'Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,655', 'Thibaut Lavril, Thomas Wang, Timothée Lacroix,656', 'and William El Sayed. 2023. Mistral 7B.657', 'Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI,658', 'Chenghao Mou, Yacine Jernite, Margaret Mitchell,659', 'Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf,660', 'Dzmitry Bahdanau, Leandro Von Werra, and Harm de661', 'Vries. 2023. The Stack: 3 TB of permissively li-662', 'censed source code. Transactions on Machine Learn-663', 'ing Research.664', 'Wouter Kool, Herke van Hoof, and Max Welling. 2020.665', 'Ancestral Gumbel-Top-k Sampling for Sampling666', 'Without Replacement. Journal of Machine Learning667', 'Research, 21(47):1–36.668', 'Khang Nguyen Le, Ryo Sato, Dai Nakashima, Takeshi669', 'Suzuki, and Minh Le Nguyen. 2025. Optiprune: Ef-670', 'fective pruning approach for every target sparsity. In671', 'Proceedings of the 31st International Conference on672', 'Computational Linguistics, pages 3600–3612.673', 'Yaniv Leviathan, Matan Kalman, and Yossi Matias.674', '2023. Fast inference from transformers via spec-675', 'ulative decoding. In Proceedings of the 40th Interna-676', 'tional Conference on Machine Learning, ICML’23.677', 'JMLR.org. Place: Honolulu, Hawaii, USA.678', 'Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen,679', 'Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024a.680', 'Nearest Neighbor Speculative Decoding for LLM681', 'Generation and Attribution. In The Thirty-eighth An-682', 'nual Conference on Neural Information Processing683', 'Systems.684', 'Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang685', 'Zhang. 2024b. EAGLE-2: Faster Inference of Lan-686', 'guage Models with Dynamic Draft Trees. In Proceed-687', 'ings of the 2024 Conference on Empirical Methods688', 'in Natural Language Processing, pages 7421–7432,689', 'Miami, Florida, USA. Association for Computational690', 'Linguistics.691', 'Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-692', 'Ming Chen, Wei-Chen Wang, Guangxuan Xiao,693', 'Xingyu Dang, Chuang Gan, and Song Han. 2024.694', 'Awq: Activation-aware weight quantization for on-695', 'device llm compression and acceleration. Proceed-696', 'ings of Machine Learning and Systems, 6:87–100.697', 'Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng698', 'Ni, Duyu Tang, Kai Han, and Yunhe Wang. 2024a.699', 'Kangaroo: Lossless Self-Speculative Decoding for700', 'Accelerating LLMs via Double Early Exiting. In The701', 'Thirty-eighth Annual Conference on Neural Informa-702', 'tion Processing Systems.703', 'Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan704', 'Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xi-705', 'aohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu,706', 'Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang,707', 'Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu,708', 'and Bao Ge. 2025. Understanding llms: A compre-709', 'hensive overview from training to inference. Neuro-710', 'computing, 620:129190.711', 'Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie712', 'Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,713', 'Raghuraman Krishnamoorthi, and Vikas Chandra.714', '2024b. LLM-QAT: Data-free quantization aware715', 'training for large language models. In Findings of716', 'the Association for Computational Linguistics: ACL717', '2024, pages 467–484, Bangkok, Thailand. Associa-718', 'tion for Computational Linguistics.719', 'Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.720', 'Llm-pruner: On the structural pruning of large lan-721', 'guage models. Advances in neural information pro-722', 'cessing systems, 36:21702–21720.723', 'Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao724', 'Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee725', 'Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan726', 'Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Ab-727', 'hyankar, and Zhihao Jia. 2024. SpecInfer: Accelerat-728', 'ing Large Language Model Serving with Tree-based729', 'Speculative Inference and Verification. In Proceed-730', 'ings of the 29th ACM International Conference on Ar-731', 'chitectural Support for Programming Languages and732', 'Operating Systems, Volume 3, ASPLOS ’24, pages733', '932–949, New York, NY, USA. Association for Com-734', 'puting Machinery. Event-place: La Jolla, CA, USA.735', 'Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,736', 'Ça˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-737', 'tive text summarization using sequence-to-sequence738', 'RNNs and beyond.In Proceedings of the 20th739', 'SIGNLL Conference on Computational Natural Lan-740', 'guage Learning, pages 280–290, Berlin, Germany.741', 'Association for Computational Linguistics.742', 'Shashi Narayan, Shay B Cohen, and Mirella Lap-743', 'ata. 2018.Don’t give me the details, just the744', '10', 'summary!topic-aware convolutional neural net-745', 'works for extreme summarization. arXiv preprint746', 'arXiv:1808.08745.747', 'OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,748', 'Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-749', 'man, Diogo Almeida, Janko Altenschmidt, Sam Alt-750', 'man, Shyamal Anadkat, Red Avila, Igor Babuschkin,751', 'Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-752', 'ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-753', 'wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,754', 'Christopher Berner, Lenny Bogdonoff, Oleg Boiko,755', 'Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-756', 'man, Tim Brooks, Miles Brundage, Kevin Button,757', 'Trevor Cai, Rosie Campbell, Andrew Cann, Brittany758', 'Carey, Chelsea Carlson, Rory Carmichael, Brooke759', 'Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully760', 'Chen, Ruby Chen, Jason Chen, Mark Chen, Ben761', 'Chess, Chester Cho, Casey Chu, Hyung Won Chung,762', 'Dave Cummings, Jeremiah Currier, Yunxing Dai,763', 'Cory Decareaux, Thomas Degry, Noah Deutsch,764', 'Damien Deville, Arka Dhar, David Dohan, Steve765', 'Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,766', 'Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,767', 'Simón Posada Fishman, Juston Forte, Isabella Ful-768', 'ford, Leo Gao, Elie Georges, Christian Gibson, Vik769', 'Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-770', 'Lopes, Jonathan Gordon, Morgan Grafstein, Scott771', 'Gray, Ryan Greene, Joshua Gross, Shixiang Shane772', 'Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,773', 'Yuchen He, Mike Heaton, Johannes Heidecke, Chris774', 'Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,775', 'Brandon Houghton, Kenny Hsu, Shengli Hu, Xin776', 'Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,777', 'Joanne Jang, Angela Jiang, Roger Jiang, Haozhun778', 'Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-779', 'woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-780', 'mali, Ingmar Kanitscheider, Nitish Shirish Keskar,781', 'Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,782', 'Christina Kim, Yongjik Kim, Jan Hendrik Kirch-783', 'ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,784', 'Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-785', 'stantinidis, Kyle Kosic, Gretchen Krueger, Vishal786', 'Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan787', 'Leike, Jade Leung, Daniel Levy, Chak Ming Li,788', 'Rachel Lim, Molly Lin, Stephanie Lin, Mateusz789', 'Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,790', 'Anna Makanju, Kim Malfacini, Sam Manning, Todor791', 'Markov, Yaniv Markovski, Bianca Martin, Katie792', 'Mayer, Andrew Mayne, Bob McGrew, Scott Mayer793', 'McKinney, Christine McLeavey, Paul McMillan,794', 'Jake McNeil, David Medina, Aalok Mehta, Jacob795', 'Menick, Luke Metz, Andrey Mishchenko, Pamela796', 'Mishkin, Vinnie Monaco, Evan Morikawa, Daniel797', 'Mossing, Tong Mu, Mira Murati, Oleg Murk, David798', 'Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,799', 'Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,800', 'Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex801', 'Paino, Joe Palermo, Ashley Pantuliano, Giambat-802', 'tista Parascandolo, Joel Parish, Emy Parparita, Alex803', 'Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-804', 'man, Filipe de Avila Belbute Peres, Michael Petrov,805', 'Henrique Ponde de Oliveira Pinto, Michael, Poko-806', 'rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-807', 'ell, Alethea Power, Boris Power, Elizabeth Proehl,808', 'Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,809', 'Cameron Raymond, Francis Real, Kendra Rimbach,810', 'Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-811', 'der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,812', 'Girish Sastry, Heather Schmidt, David Schnurr, John813', 'Schulman, Daniel Selsam, Kyla Sheppard, Toki814', 'Sherbakov, Jessica Shieh, Sarah Shoker, Pranav815', 'Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,816', 'Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin817', 'Sokolowsky, Yang Song, Natalie Staudacher, Fe-818', 'lipe Petroski Such, Natalie Summers, Ilya Sutskever,819', 'Jie Tang, Nikolas Tezak, Madeleine B. Thompson,820', 'Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,821', 'Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-822', 'lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,823', 'Chelsea Voss, Carroll Wainwright, Justin Jay Wang,824', 'Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,825', 'C. J. Weinmann, Akila Welihinda, Peter Welin-826', 'der, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave827', 'Willner, Clemens Winter, Samuel Wolrich, Hannah828', 'Wong, Lauren Workman, Sherwin Wu, Jeff Wu,829', 'Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin830', 'Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers,831', 'Chong Zhang, Marvin Zhang, Shengjia Zhao, Tian-832', 'hao Zheng, Juntang Zhuang, William Zhuk, and Bar-833', 'ret Zoph. 2024. GPT-4 Technical Report.834', 'Jie Ou, Yueming Chen, and Prof. Tian. 2024. Lossless835', 'Acceleration of Large Language Model via Adap-836', 'tive N-gram Parallel Decoding. In Proceedings of837', 'the 2024 Conference of the North American Chap-838', 'ter of the Association for Computational Linguistics:839', 'Human Language Technologies (Volume 6: Industry840', 'Track), pages 10–22, Mexico City, Mexico. Associa-841', 'tion for Computational Linguistics.842', 'Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun843', 'Sim, and Jae W. Lee. 2024. Any-precision llm: Low-844', 'cost deployment of multiple, different-sized llms. In845', 'Proceedings of the 41st International Conference on846', 'Machine Learning.847', 'Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten848', 'Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,849', 'Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy850', 'Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna851', 'Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron852', 'Grattafiori, Wenhan Xiong, Alexandre Défossez,853', 'Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-854', 'tin, Nicolas Usunier, Thomas Scialom, and Gabriel855', 'Synnaeve. 2024. Code Llama: Open Foundation856', 'Models for Code. _eprint: 2308.12950.857', 'Andrea Santilli, Silvio Severino, Emilian Postolache,858', 'Valentino Maiorca, Michele Mancusi, Riccardo859', 'Marin, and Emanuele Rodola. 2023. Accelerating860', 'transformer inference for translation via parallel de-861', 'coding. In Proceedings of the 61st Annual Meeting of862', 'the Association for Computational Linguistics (Vol-863', 'ume 1: Long Papers), pages 12336–12355, Toronto,864', 'Canada. Association for Computational Linguistics.865', 'Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.866', '11', '2023a. A simple and effective pruning approach for867', 'large language models. ArXiv, abs/2306.11695.868', 'Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-869', 'mad Beirami, Himanshu Jain, and Felix Yu. 2023b.870', 'SpecTr: fast speculative decoding via optimal trans-871', 'port. In Proceedings of the 37th International Con-872', 'ference on Neural Information Processing Systems,873', 'NIPS ’23, Red Hook, NY, USA. Curran Associates874', 'Inc. Event-place: New Orleans, LA, USA.875', 'Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-876', 'bert, Amjad Almahairi, Yasmine Babaei, Nikolay877', 'Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti878', 'Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton879', 'Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,880', 'Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,881', 'Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-882', 'thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan883', 'Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,884', 'Isabel Kloumann, Artem Korenev, Punit Singh Koura,885', 'Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-886', 'ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-887', 'tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-888', 'bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-889', 'stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,890', 'Ruan Silva, Eric Michael Smith, Ranjan Subrama-891', 'nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-892', 'lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,893', 'Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,894', 'Melanie Kambadur, Sharan Narang, Aurelien Ro-895', 'driguez, Robert Stojnic, Sergey Edunov, and Thomas896', 'Scialom. 2023. Llama 2: Open Foundation and Fine-897', 'Tuned Chat Models.898', 'Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi899', 'Chen. 2023. Sheared llama: Accelerating language900', 'model pre-training via structured pruning. ArXiv,901', 'abs/2310.06694.902', 'Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin903', 'Jiang, Linjun Yang, Rangan Majumder, and Furu904', 'Wei. 2023.Inference with Reference: Lossless905', 'Acceleration of Large Language Models. _eprint:906', '2304.04487.907', 'Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris908', 'Papailiopoulos, and Kangwook Lee. 2024. Predictive909', 'Pipelined Decoding: A Compute-Latency Trade-off910', 'for Exact LLM Decoding. Transactions on Machine911', 'Learning Research.912', 'Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,913', 'Gang Chen, and Sharad Mehrotra. 2024a. Draft&914', 'Verify: Lossless Large Language Model Acceleration915', 'via Self-Speculative Decoding. In Proceedings of the916', '62nd Annual Meeting of the Association for Compu-917', 'tational Linguistics (Volume 1: Long Papers), pages918', '11263–11282, Bangkok, Thailand. Association for919', 'Computational Linguistics.920', 'Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng921', 'Chen, and Jinan Xu. 2024b. Dual-space knowledge922', 'distillation for large language models. In Proceed-923', 'ings of the 2024 Conference on Empirical Methods in924', 'Natural Language Processing, pages 18164–18181,925', 'Miami, Florida, USA. Association for Computational926', 'Linguistics.927', 'Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn928', 'Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,929', 'Tianqi Chen, and Baris Kasikci. 2024. Atom: Low-930', 'bit quantization for efficient and accurate llm serv-931', 'ing. Proceedings of Machine Learning and Systems,932', '6:196–209.933', 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan934', 'Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,935', 'Zhuohan Li, Dacheng Li, Eric Xing, and others. 2023.936', 'Judging llm-as-a-judge with mt-bench and chatbot937', 'arena. Advances in Neural Information Processing938', 'Systems, 36:46595–46623.939', 'Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,940', 'Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv941', 'Kumar, Jean-François Kagy, and Rishabh Agarwal.942', '2024. DistillSpec: Improving Speculative Decoding943', 'via Knowledge Distillation. In The Twelfth Interna-944', 'tional Conference on Learning Representations.945', 'AMore on Speculative Decoding946', 'Autoregressive decoding (Touvron et al., 2023; Bai947', 'et al., 2023; Jiang et al., 2023; OpenAI et al., 2024),948', 'suffers from inefficiency because it generates text949', 'one token at a time (Figure 5, Left).Specula-950', 'tive decoding (Chen et al., 2023; Leviathan et al.,951', '2023) follows a guess-and-verify paradigm (Figure952', '5, Right). In speculative decoding, a smaller LLM953', '(draft model) (Chen et al., 2023; Leviathan et al.,954', '2023; Miao et al., 2024; Sun et al., 2023b; Zhou955', 'et al., 2024; Cai et al., 2024) or the original LLM956', 'trained in a specialized manner (self-speculative957', 'decoding) (Elhoushi et al., 2024; Liu et al., 2024a;958', 'Yang et al., 2024; Zhang et al., 2024a; Li et al.,959', '2024b) predicts multiple tokens in advance. The960', 'original LLM then verifies these predictions in par-961', 'allel, improving efficiency.962', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'Autoregressive Decoding', '...', 'Speculate (make guesses)', 'Speculative Decoding', 'A', 'B', 'B', 'C', 'C', 'D', 'T', 'Target LLM', 'AAcceptReject', 'Figure 5: Examples of Autoregressive decoding (Left)and Speculative Decoding (Right). While autoregres-sive decoding generates one token per forward step,speculative decoding generates three tokens with oneforward step.', '12', 'LLMs process discrete integer sequences as in-963', 'puts, where each integer represents a token. We de-964', 'fine the input sequence as x = (x1, x2, . . . , xs) ∈965', 'Ns of length s, and denote a slice of length m at966', 'step t as x1:m = (x1, x2, . . . , xm). The output of967', 'an LLM represents the probability distribution over968', 'the next token. The probability of generating the969', 's-th token, conditioned on all preceding tokens, is970', 'given by PM(xs | x1:s−1). The next token xs is971', 'then sampled from this distribution using various972', 'methods (e.g., greedy, top-k, and top-p sampling;973', 'see (Kool et al., 2020; Holtzman et al., 2020)). In974', 'the case of greedy sampling, the next token is se-975', 'lected as xs = arg max PM(xs | x1:s−1)976', 'Let x0 be the prompt tokens provided by the977', 'user. The LLM generates an output sequence of978', 'length m, with each generated token yi computed979', 'autoregressively. Assuming greedy sampling, the980', 'decoding process follows:981', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y1 = arg max PM(y1 | x) y2 = arg max PM(y2 | y1, x)... ym = arg max PM(ym | y1:m−1, x).', '(1)982', 'A.1Speculative Decoding983', 'Speculative decoding follows a Guess-And-Verify984', 'approach, where multiple candidate future to-985', 'kens are speculated and subsequently verified986', 'in a single decoding step.With tree attention987', '(Miao et al., 2024), multiple drafts can be ver-988', 'ified simultaneously. Let G denote the number989', 'of guesses, and define the set of guesses as ˜Y =990', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence991', 'has length K. The j-th token of the i-th guess is992', 'denoted as ˜y(i) j .993', 'In the case of speculative decoding with greedy994', 'sampling, given the prompt x, a drafting method995', 'is used to generate the draft sequences ˜Y . Using996', 'these drafts, the LLM then computes the true tokens997', '(y′1, y′2, . . . , y′K) in parallel. For instance, for the998', 'guess sequence ˜y(0), the true tokens are determined999', 'as:1000', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y′1 = arg max PM(y1 | x)', 'y′2 = arg max PM(y2 | ˜y(0) 1 , x)...', 'y′K = arg max PM(yK | ˜y(0) 1:K−1, x).', '(2)1001', 'These generated tokens are then verified. Let h1002', 'be the highest number of correct guessed tokens1003', 'across all guesses. Consequently, h + 1 tokens are1004', 'generated in one forward step. Algorithm 2 out-1005', 'lines speculative decoding with greedy sampling.1006', 'BImplementation Details1007', 'B.1Frameworks and Libraries1008', 'We implement SPECTRA in Python using PyTorch1009', '2.1.0 and the Hugging Face transformers library.1010', 'For large-scale model loading (e.g., LLaMA-2-1011', '70B, LLaMA-3-70B), we employ 16-bit (FP16)1012', 'precision with a pre-allocated key-value cache.1013', 'B.2Models and Checkpoints1014', 'We run our experiments primarily with:1015', '• LLaMA-2-Chat (Touvron et al., 2023) in1016', 'sizes 7B, 13B, 70B.1017', '• CodeLlama (Rozière et al., 2024) in sizes 7B1018', 'and 13B.1019', '• LLaMA-3-Instruct (Dubey et al., 2024) in1020', 'sizes 8B and 70B.1021', 'All checkpoints are obtained from the official or1022', 'Hugging Face repositories without fine-tuning or1023', 'modification. For each model, we enable half-1024', 'precision inference. We also verify numerically (by1025', 'comparing 32-bit and 16-bit outputs) that specula-1026', 'tive decoding preserves exact or near-exact token1027', 'sequences within typical floating-point tolerances.1028', 'B.3Hardware1029', 'Most experiments are conducted on a single1030', 'NVIDIA A100 GPU with 80GB of memory. We1031', 'also evaluate on other NVIDIA GPUs (RTX 3090,1032', 'RTX 8000, A40, A6000) to study hardware-1033', 'specific scaling. For the largest checkpoints (70B)1034', 'that do not fit on a single GPU under certain con-1035', 'figurations, we optionally distribute them across1036', 'multiple GPUs (2x, 4x, or 8x H100) using standard1037', 'pipeline-parallelism from Hugging Face’s library.1038', 'B.4Hyperparameters1039', 'Lookahead, REST, and ANPD.We replicate1040', 'each baseline using their publicly available GitHub1041', 'code, keeping to the default settings and hyperpa-1042', 'rameters outlined in the original papers.1043', 'Spectra.By default, we use a 5-gram setup for1044', 'our forward/backward dictionaries, storing all sub-1045', 'sequences (i.e., sub-ngrams). We also maintain a1046', 'candidate pool of size W = 15 per key to generate1047', '13', 'new n-gram records; after each forward pass, can-1048', 'didate sequences are shifted by one token and then1049', 're-populated. We introduce a threshold τ ∈[0, 1],1050', 'default set to 0.1, to decide when to force the se-1051', 'lection of a token not yet in the forward dictionary.1052', 'This mechanism balances coverage of unseen pre-1053', 'fixes with reinforcing common contexts. At every1054', 'speculative decoding step, we allow up to G = 601055', 'guess tokens. Internal guesses receive priority, and1056', 'if there is still capacity under the guess limit, we1057', 'add external guesses.1058', 'For external lookups, we implement a trie struc-1059', 'ture for rapid prefix queries, following a design1060', 'similar to REST (He et al., 2024). For conver-1061', 'sation tasks (e.g., MT-Bench), we gather approxi-1062', 'mately 100k examples from the UltraChat dataset1063', '(Ding et al., 2023), focusing on those with minimal1064', 'perplexity under the same LLM we aim to accel-1065', 'erate. For code tasks (e.g., HumanEval, MBPP),1066', 'we draw from TheStack (Kocetkov et al., 2023)1067', 'and again refine it to the 100k snippets with the1068', 'lowest perplexity for memory efficiency. We mea-1069', 'sure perplexity by running a single forward pass (in1070', 'streaming mode) over candidate samples and rank-1071', 'ing them. Despite being relatively small (100k),1072', 'this curated corpus achieves robust guess quality.1073', 'All speedup and throughput metrics are com-1074', 'puted at a batch size of 1. In code generation tasks,1075', 'the maximum generation length is typically 5121076', 'tokens, whereas for conversation tasks (MT-Bench,1077', 'GSM8K), we allow up to 1024 tokens or stop early1078', 'if the model outputs an end-of-sequence token. All1079', 'random seeds are set to 0.1080', 'CDetails Results with Throughputs1081', 'We provide a detailed throughput analysis to com-1082', 'plement the speedup ratios reported in the main1083', 'text. Our goal is to demonstrate how SPECTRA1084', 'scales across various model sizes, datasets, and1085', 'GPU architectures. We measure throughput using1086', 'two key metrics:1087', '• Macro Throughput (Mac-TP). Calculated1088', 'as the average of per-generation token-1089', 'processing rates—i.e., for each generation1090', 'step i, we compute tokeni/timei and then1091', 'average over all steps.1092', '• Micro Throughput (Mic-TP). Calculated as1093', 'the total number of generated tokens divided1094', 'by the total elapsed time1095', 'Table 4 focuses on GSM8K and MTBench per-1096', 'formance across four different GPU models, while1097', 'Table 3 provides more granular results on ad-1098', 'ditional datasets and model configurations.In1099', 'all cases, SPECTRA consistently achieves higher1100', 'throughput than both non-speculative baselines and1101', 'other training-free accelerators, as evidenced by im-1102', 'provements in both Mic-TP and Mac-TP. Notably,1103', 'this performance advantage remains stable even on1104', 'older GPUs (e.g., the RTX 3090 and RTX 8000),1105', 'demonstrating SPECTRA’s robustness to varying1106', 'hardware capabilities.1107', 'DEvaluating SPECTRA in Different GPU1108', 'Types1109', 'Different GPU types.Table 5 reports speedups1110', 'on GSM8K and MTBench across four GPUs with1111', 'varying memory throughput and compute capabili-1112', 'ties. While absolute wall-clock times differ across1113', 'GPUs, the relative accelerations remain consistent.1114', 'SPECTRA consistently outperforms other baselines,1115', 'including Lookahead, achieving higher speedups in1116', 'all cases. On older GPUs (e.g., RTX 3090 or RTX1117', '8000), the gap between Lookahead and SPECTRA1118', 'narrows slightly due to less efficient parallelism,1119', 'but SPECTRA maintains its lead. These results1120', 'demonstrate that SPECTRA is robust to hardware1121', 'variations and effective across both data-center and1122', 'consumer-grade GPUs.1123', 'EEvaluating SPECTRA in Multi-GPU1124', 'Environments1125', 'A critical consideration for practical deployment is1126', 'how SPECTRA scales when models are distributed1127', 'across multiple GPUs—a common requirement for1128', 'large LLMs exceeding single-device memory ca-1129', 'pacity. To evaluate this, we measure SPECTRA’s1130', 'performance under three distributed configurations1131', 'of LLaMA-2-70B: (1) 2xH100 with full precision,1132', '(2) 4xH100 with full precision, and (3) 8xH1001133', 'with full precision. We also include a baseline1134', 'of 1xH100 with 8-bit quantization for memory-1135', 'constrained single-GPU inference. Table 6 reports1136', 'throughput and speedup metrics.1137', 'SPECTRA achieves consistent speedups of 2.00—1138', '2.03× across all multi-GPU configurations while1139', 'maintaining a stable compression ratio (τ) of 2.52.1140', 'This demonstrates robust scalability—partitioning1141', 'model weights introduces minimal overhead, and1142', 'the speculative verification process remains effi-1143', 'cient despite inter-GPU communication. Notably,1144', '14', 'ClassevalGSM8KHumanevalMBPPMTBenchModelMethodMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TP', 'Greedy (temperature=0)', 'CL-13BAutoregressive30.8530.8532.0332.0332.3532.3532.0732.0730.6930.63 CL-13BANPD59.7758.0389.9989.1867.4364.6586.7686.4180.1076.68 CL-13BLookahead69.2868.6289.7389.0074.3373.2393.3892.8079.3878.67 CL-13BREST39.5337.7329.9329.4751.1547.4927.4127.3928.9227.18 CL-13BSPECTRA (Ours)73.4772.9893.3693.2384.9184.41105.44105.3981.3280.68', 'CL-7BAutoregressive41.1741.1741.1741.1741.4141.4141.6041.6038.9138.93 CL-7BANPD94.7693.02132.26131.3089.2687.13131.35130.99130.41126.64 CL-7BLookahead106.51105.95123.04121.90103.45103.51120.75120.23125.58124.77 CL-7BREST59.4956.6137.6137.2170.3865.2240.1140.0939.6436.70 CL-7BSPECTRA (Ours)111.09110.68137.24136.86122.54122.41148.32148.07143.98144.32', 'L2-13BAutoregressive31.8531.5632.4032.4332.2732.2732.1932.1931.9331.78 L2-13BANPD43.3044.4447.5445.2243.2442.2836.2035.8437.4434.84 L2-13BLookahead57.4958.9447.4447.6255.7655.5844.4144.1548.1146.62 L2-13BREST38.8137.7430.3630.2240.4739.7030.7030.6736.3937.02 L2-13BSPECTRA (Ours)63.6464.3159.2158.6363.3963.1852.4352.1956.0453.75', 'L2-70BAutoregressive2.602.602.612.612.612.612.632.632.602.60 L2-70BANPD4.724.804.254.104.854.763.073.073.473.30 L2-70BLookahead6.907.164.875.126.716.733.923.935.055.02 L2-70BSPECTRA (Ours)8.078.356.586.758.418.414.884.886.326.22', 'L2-7BAutoregressive40.3340.3241.0141.0341.1441.1341.0041.0440.4840.50 L2-7BANPD65.5468.1062.4059.3863.2759.9848.9447.6752.4750.06 L2-7BLookahead88.4191.0568.0068.2084.6983.8759.7960.7670.0469.07 L2-7BREST54.7453.9341.4341.3857.9956.4141.2840.7450.5851.79 L2-7BSPECTRA (Ours)96.8898.7586.5185.5098.7798.3872.3973.2281.9379.20', 'L3-70BAutoregressive2.582.572.582.582.592.592.592.592.552.55 L3-70BANPD3.974.193.863.724.724.753.773.593.143.03 L3-70BLookahead6.176.473.993.966.636.753.703.664.494.53 L3-70BSPECTRA (Ours)6.877.185.435.347.337.505.014.885.255.16', 'L3-8BAutoregressive36.5936.5836.7436.7436.2036.2135.2435.2036.5536.69 L3-8BANPD77.2178.76141.89141.3666.3165.57118.47112.9541.7740.20 L3-8BLookahead94.9297.09136.32135.9289.9990.47133.67133.1256.0955.49 L3-8BSPECTRA (Ours)103.61105.88142.89142.7292.8693.16143.80142.7261.6960.22', 'Sampling (temperature=1.0)', 'CL-13BAutoregressive30.9030.6431.3831.3731.2431.3931.4631.4530.7130.67 CL-13BANPD35.4834.8633.5432.3432.6434.3631.5730.9570.9265.68 CL-13BLookahead42.5440.7433.7932.4940.2542.1732.0231.1971.5068.46 CL-13BREST35.1533.2225.6725.2439.5838.4926.4325.8928.4126.69 CL-13BSPECTRA (Ours)51.8650.0437.5735.6751.6052.6436.2935.2772.9069.98', 'CL-7BAutoregressive39.6039.5840.8540.8740.0540.1040.8140.8140.4940.50 CL-7BANPD50.8951.7647.4446.6844.1446.3445.8645.81112.29103.57 CL-7BLookahead60.8760.2948.5447.6457.1261.1448.6448.27110.07105.00 CL-7BREST48.6446.4135.9835.4653.3552.2637.0436.5739.3636.51 CL-7BSPECTRA (Ours)71.7071.7855.2452.8167.2769.2054.4852.91112.43108.49', 'L2-13BAutoregressive31.2331.1731.4431.4731.4131.4232.0232.0631.6731.59 L2-13BANPD37.5337.9439.1137.9936.7936.7532.9732.7136.9134.34 L2-13BLookahead47.5947.3541.6041.7646.3346.5137.8237.8247.3545.48 L2-13BREST36.7836.1729.3329.2537.4636.7129.3829.2835.5036.21 L2-13BSPECTRA (Ours)53.1352.2848.6048.1152.9353.1142.9543.0354.9852.42', 'L2-7BAutoregressive39.8939.8840.5840.5940.0940.1040.5940.6640.6540.70 L2-7BANPD52.1452.7854.2352.9051.4050.9744.7343.7750.9248.24 L2-7BLookahead70.8271.1761.1561.3468.7869.0150.8451.8368.2766.77 L2-7BREST50.3549.9940.1940.0950.8650.0638.9438.1849.1250.54 L2-7BSPECTRA (Ours)78.4678.7472.1371.6881.7181.7659.7760.0980.2177.00', 'L3-8BAutoregressive35.7535.7635.1635.1736.0136.0236.0536.0735.3935.48 L3-8BANPD44.7143.7269.1266.7351.4851.5768.0364.5440.8439.23 L3-8BLookahead53.0550.5772.6869.1164.5963.7971.8868.9055.4653.74 L3-8BSPECTRA (Ours)69.5068.9279.8876.5369.0968.6278.9976.6960.3357.69', 'Table 3: Micro throughput (Mic-TP) and Macro throughput (Mac-TP) across multiple tasks and models.', '15', 'GPUMethodGSM8KMTBenchMac-TPMic-TPMac-TPMic-TP', 'A40Autoregressive32.6632.6632.1431.66 Lookahead48.5948.7349.1347.96 SPECTRA62.5661.5259.0056.80', 'A6000Autoregressive39.1539.1738.7838.24 Lookahead58.1358.3058.8457.40 SPECTRA75.2074.1671.369.28', 'RTX8000Autoregressive34.0334.2734.2134.02 Lookahead45.2545.4245.7344.16 SPECTRA57.9557.0954.1652.32', 'RTX3090Autoregressive40.6740.7641.1741.22 Lookahead53.6953.7553.5152.09 SPECTRA74.8773.8871.5869.79', 'Table 4: Throughput results for different GPU types on GSM8K and MTBench.', 'GPUMethodGSM8KMTBenchspeedupτspeedupτ', 'A40Lookahead1.491.931.532.07 SPECTRA1.922.461.842.36', 'A6000Lookahead1.481.921.522.06 SPECTRA1.922.461.842.36', 'RTX8000Lookahead1.331.931.342.08 SPECTRA1.702.461.582.35', 'RTX3090Lookahead1.321.921.302.06 SPECTRA1.842.461.742.36', 'Table 5: Hardware scalability of SPECTRA decoding onGSM8K and MTBench for various GPU architectures.', 'even in the quantized single-GPU setting, SPEC-1145', 'TRA provides a 2.43× speedup, outperforming1146', 'standard autoregressive decoding. These results1147', 'validate SPECTRA’s practicality for large-scale de-1148', 'ployments where memory constraints necessitate1149', 'distributed inference.1150', 'FVerifying Generation Quality with1151', 'SPECTRA Decoding1152', 'Greedy Decoding Performance.To assess the1153', 'quality of greedy decoding, we compare the in-1154', 'ference results of the LLaMA-2-7B Chat model1155', 'using SPECTRA Decoding against Hugging Face’s1156', 'standard greedy search. Our baseline consists of1157', 'single-precision (FP32) inference on 160 conver-1158', 'sational turns from the MT-Bench dataset. Under1159', 'FP32, SPECTRA Decoding produces identical out-1160', 'puts to the baseline.1161', 'However, when transitioning to half-precision1162', '(FP16), even Hugging Face’s native greedy search1163', 'generates 25 discrepancies (out of 160) compared1164', 'to the FP32 baseline. SPECTRA Decoding exhibits1165', 'a similar discrepancy rate (26), confirming that it1166', 'maintains the output distribution within the numer-1167', 'ical error margins typically observed in standard1168', 'half-precision inference libraries.1169', 'Sampling Decoding Performance.We also as-1170', 'sess generation quality under a stochastic sam-1171', 'pling setting (temperature = 1.0). As detailed in1172', 'Table 7, SPECTRA Decoding produces ROUGE-1173', '1, ROUGE-2, and ROUGE-L scores on both the1174', 'CNN/DailyMail (Nallapati et al., 2016) and XSum1175', '(Narayan et al., 2018) summarization datasets1176', 'that are nearly identical to those of standard1177', 'autoregressive sampling.At the same time,1178', 'SPECTRA achieves notable speedups (1.60× on1179', 'CNN/DailyMail and 1.69× on XSum) with com-1180', 'pression ratios of 2.05 and 2.08, respectively.1181', 'These results confirm that SPECTRA Decoding1182', 'accelerates inference while preserving generation1183', 'quality across diverse tasks.1184', 'These findings reaffirm that SPECTRA Decoding,1185', 'does not degrade generation quality compared to1186', 'conventional greedy or sampling-based methods.1187', 'GToken Acceptance Rate Analysis1188', 'Figure 6 plots the cumulative number of accepted1189', 'tokens versus decoding steps for each dataset (MT-1190', 'Bench, HumanEval, MBPP, and GSM8K). The1191', 'steeper ascent of the SPECTRA curve indicates that1192', 'our method requires substantially fewer decoding1193', 'steps compared to alternatives, for example, almost1194', 'two times shorter than ANPD. This improvement is1195', '16', 'GPU & Model SettingMethodMTBenchMac-TPMic-TPSpeedupτ', '1xH100 - Quantized Int8Autoregressive2.602.601.001.00 SPECTRA6.326.222.432.51', '2xH100 - FP16Autoregressive14.8114.701.001.00 SPECTRA29.6228.912.002.52', '4xH100 - FP16Autoregressive14.6014.481.001.00 SPECTRA29.6728.892.032.52', '8xH100 - FP16Autoregressive14.3914.281.001.00 SPECTRA29.2728.552.032.52', 'Table 6: Results in multi-GPU Enviroments on GSM8K and MTBench using LLama-2-chat-70B.', 'DatasetMethodROUGE-1ROUGE-2ROUGE-LSpeedupτ', 'CNNAutoregressive9.770.397.201.001.00 SPECTRA9.740.417.181.602.05', 'XSUMAutoregressive18.124.3612.431.001.00 SPECTRA18.134.4012.491.692.08', 'Table 7: Evaluation of SPECTRA Decoding on CNN/DailyMail and XSum using a temperature of 1.0. ROUGEscores, speedups over autoregressive decoding, and compression ratio (τ) are reported for LLaMA-2-7B-Chat.', 'attributed to a higher token acceptance rate, which1196', 'in turn reduces the overall number of decoding iter-1197', 'ations and enhances the efficiency of the generation1198', 'process.1199', 'HAlgorithms1200', '17', '0250500750', '0', '10', '20', '30', '40', 'MT-Bench', '0200400', '0', '10', '20', '30', '40', 'HumanEval', '050100150', '0', '5', '10', '15', 'MBPP', '0100200', '0', '10', '20', '30', '40', '50', '60', 'GSM8K', '#Accepted tokens (in thousands)', 'LookaheadRESTANPDSpectra (Ours)', 'Figure 6: Total number of accepted tokens across all samples at each decoding step.', 'Algorithm 2 Speculative Decoding (Multiple guesses and Greedy Sampling)', 'Given guess size K, number of guesses G, and target length T. Given initial prompt sequence x. while n < T do', 'Obtain multiple drafts ˜Y = {˜y(0), ˜y(1), . . . , ˜y(G)}.', 'In parallel, compute K + 1 verification tokens y′: for i = 1 : K do', 'y′(g) i= arg max PM(yi | ˜y(g) i−1, x),∀g ∈{0, . . . , G}', 'end forIdentify the sequence ˜y(g∗) with the highest token matches and the corresponding y′(g).', 'for t = 1 : K do', 'if y′(g) t= ˜y(g∗) tthen', 'Set yn+t ←˜y(g∗) tand n ←n + 1.', 'else', 'yn+t ←y′(g) tand exit for loop.', 'end if', 'end for', 'end while', '18', 'Algorithm 3 Greedy Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function GREEDYVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x and all gi2’s outputs – totally N distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:P ←D[l][i]', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:if sj = arg max P then▷accepted, update all potential speculations and probabilities', '14:o.append(sj)', '15:is_accept ←1', '16:Vnew, Dnew ←∅, ∅', '17:for k = j to size(V ) do', '18:if sj = V [k] then', '19:Vnew.append(V [k])', '20:Dnew.append(D[k])', '21:end if', '22:end for', '23:V, D ←Vnew, Dnew', '24:break', '25:else▷rejected, go to next speculation', '26:j ←j + 1', '27:end if', '28:end while', '29:if is_accept then', '30:continue', '31:else▷guarantee one step movement', '32:o.append(arg max P)', '33:break', '34:end if', '35:end for', '36:if is_accept then', '37:o.append(arg max D[1]N)', '38:end if', '39:return o', '40: end function', '19', 'Algorithm 4 Sample Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function SAMPLEVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x0 and all gi2’s outputs – totally N probability distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:Pj ←D[1]i', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:sample r ∼U(0, 1)', '14:if r ≤Pj(sj) then▷accepted, update all potential speculations and probabilities', '15:o.append(sj)', '16:is_accept ←1', '17:Vnew, Dnew ←∅, ∅', '18:for k = j to size(V ) do', '19:if sj = V [k] then', '20:Vnew.append(V [k])', '21:Dnew.append(D[k])', '22:end if', '23:end for', '24:V, D ←Vnew, Dnew', '25:break', '26:else▷rejected, go to next speculation', '27:Pj(sj) ←0', '28:Pj+1 = norm(Pj)', '29:j ←j + 1', '30:end if', '31:end while', '32:if is_accept then', '33:continue', '34:else▷guarantee one step movement', '35:sample xnext ∼Pj', '36:o.append(xnext)', '37:break', '38:end if', '39:end for', '40:if is_accept then', '41:o.append(sample xnext ∼D[1]N)', '42:end if', '43:return o', '44: end function', '20']
2025-02-19 13:05:04 - WARNING: done parsing pdf
2025-02-19 13:05:04 - WARNING: start ner pdf
2025-02-19 13:05:07 - INFO: Loading Data
2025-02-19 13:05:07 - WARNING: 2025-02-19 13:05:07 - INFO: Loading Data
2025-02-19 13:05:11 - WARNING: Predicting NER ...
2025-02-19 13:05:13 - WARNING: Finished predicting.
2025-02-19 13:05:13 - WARNING: Converting to Brat format...
2025-02-19 13:05:13 - WARNING: # of discontinuous mentions:
2025-02-19 13:05:13 - WARNING:  
2025-02-19 13:05:13 - WARNING: 0
2025-02-19 13:05:13 - WARNING: Finished.
2025-02-19 13:05:13 - WARNING: start predict re
2025-02-19 13:05:17 - WARNING: Example:   0%|          | 0/40 [00:00<?, ?it/s]
2025-02-19 13:05:17 - WARNING: Example: 100%|##########| 40/40 [00:00<00:00, 2166.79it/s]
2025-02-19 13:05:17 - WARNING: # of documents 40.
2025-02-19 13:05:17 - WARNING: # of positive examples 0.
2025-02-19 13:05:17 - WARNING: # of negative examples 106.
2025-02-19 13:05:17 - WARNING: dict_keys(['483', '513', '516', '1330', '1426'])
2025-02-19 13:05:17 - WARNING: done predict re
2025-02-19 13:05:17 - WARNING: model output text 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 13:05:17 - WARNING: len of model_output_text 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: 51
2025-02-19 13:05:17 - WARNING: original_text 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: and time-consuming. Speculative decoding has003
2025-02-19 13:05:17 - WARNING: mapping dict 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 7, 9: 8, 10: 8, 11: 9, 12: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 17, 21: 18, 22: 19, 23: 20, 24: 21, 25: 22, 26: 23, 27: 24, 28: 25, 29: 26, 30: 27, 31: 28, 32: 29, 33: 30, 34: 31, 35: 32, 36: 33, 37: 34, 38: 35, 39: 36, 40: 37, 41: 38, 42: 39, 43: 40, 44: 41, 45: 42, 46: 43, 47: 44, 48: 45, 49: 46, 50: 46}
2025-02-19 13:05:17 - WARNING: text: 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 13:05:17 - WARNING: origin bbox 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 13:05:17 - WARNING: normalized bbox 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 13:05:17 - WARNING: finalized bbox 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 13:05:17 - WARNING:  final bouding box 
2025-02-19 13:05:17 - WARNING:  
2025-02-19 13:05:17 - WARNING: {'x1': 12.217000007629395, 'y1': 269.133544921875, 'x2': 272.1279296875, 'y2': 281.13848876953125, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 13:05:17 - WARNING: done save re and ner, 
2025-02-19 13:05:17 - WARNING: start count num_rel 
2025-02-19 13:05:17 - WARNING: done count num_rel 
2025-02-19 13:05:17 - WARNING: start update document 
2025-02-19 13:05:19 - WARNING: start matching infor
2025-02-19 13:05:19 - WARNING: done matching infor
2025-02-19 13:05:19 - WARNING: start commit
2025-02-19 13:05:19 - WARNING: done update document 
2025-02-19 13:05:22 - WARNING: Email sent successfully!
2025-02-19 13:05:23 - INFO: Task dev_tasks.process_pdf_task[d3291a90-996a-414d-8445-ccada568f049] succeeded in 20.075739949941635s: {'id': 47723600, 'filename': '_ACL_2025__LLM_Efficiency (2).pdf', 'upload_time': '2025/02/19, 04:05:02', 'entities': 232, 'relations': 6, 'pages': 20, 'status': 'completed'}
2025-02-19 13:05:23 - WARNING: 2025-02-19 13:05:23 - INFO: Task dev_tasks.process_pdf_task[d3291a90-996a-414d-8445-ccada568f049] succeeded in 20.075739949941635s: {'id': 47723600, 'filename': '_ACL_2025__LLM_Efficiency (2).pdf', 'upload_time': '2025/02/19, 04:05:02', 'entities': 232, 'relations': 6, 'pages': 20, 'status': 'completed'}
2025-02-19 13:10:10 - INFO: Task dev_tasks.process_pdf_task[2e3ecd39-f3f8-448c-aefb-d89630c62119] received
2025-02-19 13:10:10 - WARNING: 2025-02-19 13:10:10 - INFO: Task dev_tasks.process_pdf_task[2e3ecd39-f3f8-448c-aefb-d89630c62119] received
2025-02-19 13:10:10 - WARNING: uploads/_ACL_2025__LLM_Efficiency (2) (1).pdf
2025-02-19 13:10:10 - WARNING: start parsing pdf
2025-02-19 13:10:11 - WARNING: parsed 1576 paragraphs
2025-02-19 13:10:11 - WARNING: ['SPECTRA: Faster Large Language Model Inference withOptimized Internal and External Speculation', 'Anonymous ACL submission', 'Abstract', 'Inference with modern Large Language Mod-001', 'els (LLMs) is both computationally expensive002', 'and time-consuming. Speculative decoding has003', 'emerged as a promising solution, but existing004', 'approaches face key limitations: training-based005', 'methods require a draft model that is challeng-006', 'ing to obtain and lacks generalizability, while007', 'non-training methods offer limited speedup008', 'gains. In this work, we present SPECTRA, a009', 'novel framework for accelerating LLM infer-010', 'ence without the need for additional training.011', 'SPECTRA introduces two innovative techniques012', 'for efficiently managing internal and external013', 'knowledge, each outperforming corresponding014', 'state-of-the-art (SOTA) methods independently.015', 'When combined, these techniques achieve up016', 'to a 4.08x speedup across various benchmarks017', 'and LLM architectures, significantly surpassing018', 'existing non-training approaches. The imple-019', 'mentation of SPECTRA is publicly available.020', '1Introduction021', 'Generating long sequences with low latency using022', 'Large Language Models (LLMs) is a critical re-023', 'quirement. Current LLMs rely on autoregressive024', 'decoding (Touvron et al., 2023; Bai et al., 2023;025', 'Jiang et al., 2023; OpenAI et al., 2024), which026', 'suffers from inefficiency because it generates text027', 'one token at a time. This results in generation028', 'time scaling linearly with the sequence length and029', 'underutilizes the parallel processing capabilities030', 'of modern GPUs. A widely studied approach to031', 'mitigate this issue is speculative decoding (Chen032', 'et al., 2023; Leviathan et al., 2023), which fol-033', 'lows a guess-and-verify paradigm. In this approach,034', 'a smaller LLM (draft model) (Chen et al., 2023;035', 'Leviathan et al., 2023; Miao et al., 2024; Sun et al.,036', '2023b; Zhou et al., 2024; Cai et al., 2024) or the037', 'original LLM trained in a specialized manner (self-038', 'speculative decoding) (Elhoushi et al., 2024; Liu039', 'et al., 2024a; Yang et al., 2024; Zhang et al., 2024a;040', 'Li et al., 2024b) predicts multiple tokens in ad-041', 'vance. The original LLM then verifies these pre-042', 'dictions in parallel, improving efficiency. However,043', 'these approaches require additional training, which044', 'demands substantial computational resources and045', 'may degrade the original model’s capabilities.046', 'Another line of research focuses on speculat-047', 'ing subsequent tokens without requiring additional048', 'training. This approach eliminates the need for049', 'training new models or modifying the original large050', 'language model (LLM), making it practical for051', 'off-the-shelf deployment. Some methods leverage052', 'specialized mechanisms to generate speculative to-053', 'kens directly from the LLM’s predictions (Fu et al.,054', '2024; Ou et al., 2024), while others rely on ex-055', 'ternal information sources to derive these tokens056', '(Yang et al., 2023; He et al., 2024; Li et al., 2024a).057', 'However, the speedup gain in these approaches re-058', 'mains limited due to the quality of the speculative059', 'guesses.060', 'We introduce SPECTRA (Figure 1a), a specu-061', 'lative decoding method that improves generation062', 'speed without requiring any training or modifica-063', 'tions to the original LLM. SPECTRA consists of064', 'two main components: a core module (SPECTRA-065', 'CORE, Figure 1c), which integrates seamlessly into066', 'LLMs in a plug-and-play manner, and an optional067', 'retrieval module (SPECTRA-RETRIEVAL, Figure068', '1e) that further enhances performance. The core069', 'module SPECTRA-CORE improves speculative de-070', 'coding by leveraging the token distribution pre-071', 'dicted by the LLM to generate high-quality guesses.072', 'Specifically, it employs two multi-level N-gram073', 'dictionaries that enable bi-directional search for074', 'dynamic-length guesses, balancing both quality075', 'and quantity. Additionally, SPECTRA optimizes a076', 'candidate pool to continuously update the N-gram077', 'dictionaries, ensuring broad token coverage. All078', 'updates to these resources, along with guess verifi-079', 'cation, are performed efficiently in a single forward080', 'pass. The retrieval module, SPECTRA-RETRIEVAL,081', '1', 'SPECTRA-RETRIEVAL', 'Module', 'N-gram Store', 'Input', 'Lookahead', 'Candidate', 'LLM', '(b) Lookahead decoding(c) SPECTRA-CORE', 'Guesses', 'Input', 'SPECTRA', 'Candidate', 'LLM', 'Bi-directional', 'Guesses', 'Multi-level', 'N-gram Store', 'InputCorpus', 'Guesses', 'LLM', 'Trie', 'Input', 'Guesses', 'LLM', 'Trie', '(d) REST', 'Corpus Refined', 'by Perplexity', '(e) SPECTRA-RETRIEVAL', '(a) Ours: SPECTRA', 'SPECTRA-CORE', 'Module', 'Input', 'GuessesLLM', 'Figure 1: Overview of Spectra and comparison with other non-training SOTA approaches. (a) Overview of SPECTRA.(b) Overview of Lookahead Decoding (Fu et al., 2024). (c) Overview of the SPECTRA-CORE module, which utilizesthe knowledge inside LLM for obtaining guesses. (d) Overview of REST (He et al., 2024). (e) Overview of theSPECTRA-RETRIEVAL module, which is designed to be integrated efficiently with SPECTRA-CORE to boost thespeedup. The results in the bar chart are measured on HumanEval.', 'can be integrated to further enhance speedup. Ex-082', 'isting approaches that rely on external sources for083', 'generating guesses (He et al., 2024) struggle to in-084', 'tegrate with other speculative decoding methods,085', 'as the search time outweighs the speedup gains.086', 'SPECTRA-RETRIEVAL addresses this issue by re-087', 'ducing the search space, selecting only high-quality088', 'content from the corpus based on perplexity scores089', 'computed by the target LLM. This optimization en-090', 'ables seamless integration with SPECTRA-CORE,091', 'maximizing efficiency.092', 'Empirical results on six tasks—including multi-093', 'turn conversation, code generation, and mathemati-094', 'cal reasoning—across three LLM families (Llama095', '2 (Touvron et al., 2023), Llama 3 (Dubey et al.,096', '2024), and CodeLlama (Rozière et al., 2024)) with097', 'model sizes ranging from 7B to 70B demonstrate098', 'that SPECTRA outperforms other non-training spec-099', 'ulative decoding methods, achieving speedups of100', 'up to 4x. We publicly release the code and data.101', 'The key contributions of this paper are as follows:102', '• We introduce SPECTRA, which improves spec-103', 'ulative decoding by effectively leveraging the104', 'LLM’s predicted token distribution. SPEC-105', 'TRA is a plug-and-play solution that requires106', 'no modifications to the LLM (Section 3.1).107', '• SPECTRA’s retrieval module refines external108', 'corpora using perplexity scores computed by109', 'the target LLM, providing a general frame-110', 'work that enables speculative decoding ap-111', 'proaches relying on external information to112', 'be seamlessly integrated with other specula-113', 'tive decoding techniques (Section 3.2).114', '• Extensive experiments across diverse tasks,115', 'LLM architectures, GPU types, and settings116', 'demonstrate the efficiency of SPECTRA, out-117', 'performing other non-training speculative de-118', 'coding approaches (Section 5).SPECTRA119', 'also integrates with acceleration tools such as120', 'FlashAttention and pipeline parallelism (Sec-121', 'tion 5.2). The code and data are available.122', '2Preliminaries123', '2.1Autoregressive Decoding in LLMs124', 'Given an input sequence x = (x1, x2, . . . , xs)125', 'of length s, and a slice of length m as x1:m =126', '(x1, x2, . . . , xm), the output of an LLM repre-127', 'sents a probability distribution over the next to-128', 'ken. The probability of generating the s-th token,129', 'conditioned on all preceding tokens, is given by130', 'PM(xs | x1:s−1). The next token xs is sampled131', 'from this distribution using methods such as greedy,132', 'top-k, or top-p sampling (see (Kool et al., 2020;133', 'Holtzman et al., 2020)). For greedy sampling, the134', 'next token is selected as xs = arg max PM(xs |135', 'x1:s−1). Consequently, the LLM generates an out-136', 'put sequence (y1, y2, . . . , ym) of length m autore-137', 'gressively, where each token yi is computed as138', '2', 'yi = argmax PM(yi | y1:i−1, x).139', '2.2Speculative Decoding140', 'Speculative decoding follows a guess-and-verify141', 'approach, where multiple candidate future to-142', 'kens are speculated and subsequently verified143', 'in a single decoding step.With tree attention144', '(Miao et al., 2024), multiple drafts can be ver-145', 'ified simultaneously. Let G denote the number146', 'of guesses, and define the set of guesses as ˜Y =147', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence148', 'has length K. The j-th token of the i-th guess is149', 'denoted as ˜y(i) j .150', 'In the case of speculative decoding with greedy151', 'sampling, given the prompt x, a drafting method152', 'generates the draft sequences ˜Y . Using these drafts,153', 'the LLM computes the true tokens (y′1, y′2, . . . , y′K)154', 'in parallel. These tokens are then verified, and155', 'h is defined as the highest number of correctly156', 'guessed tokens across all guesses. Consequently,157', 'h + 1 tokens are generated in a single forward158', 'step. Algorithm 2 outlines speculative decoding159', 'with greedy sampling, and additional details are160', 'provided in Appendix A.161', '3SPECTRA DECODING162', 'SPECTRA consists of two modules (SPECTRA-163', 'CORE and SPECTRA-RETRIEVAL) that can func-164', 'tion independently or together. The core module165', '(SPECTRA-CORE) improves speedup by leveraging166', 'the LLM’s predicted token distribution to gener-167', 'ate high-quality guesses and integrates into LLMs168', 'in a plug-and-play manner. The retrieval module169', '(SPECTRA-RETRIEVAL) derives guesses from a re-170', 'fined external information source and is designed to171', 'integrate with SPECTRA-CORE to further enhance172', 'performance.173', '3.1SPECTRA-CORE174', 'SPECTRA-CORE maintains an N-gram storage and175', 'a candidate pool. The candidate pool C contains176', 'W sequences, {c(0), c(1), . . . , c(W−1)}, with each177', 'sequence consisting of N tokens. Let c(i) jrepresent178', 'the j-th token in the i-th sequence. The N-gram179', 'storage includes two dictionaries: the forward dic-180', 'tionary Sfwd and the backward dictionary Sbwd. At181', 'each time step, guesses G are obtained through a182', 'bidirectional search using Sfwd and Sbwd. A sin-183', 'gle forward pass to the LLM retrieves all neces-184', 'sary distributions, which are used to generate new185', 'candidate tokens for C and verify the guesses G.186', 'Algorithm 1 SPECTRA Internal Knowledge', 'Require: Sequence x = (x1, x2, . . . , xn), model PM, maxN-gram size N, candidate pool size W, max guesses G,max number of new tokens m. Refine threshold τ', '1: Initialize N-gram Forward-dictionary Sfwd ←∅2: Initialize N-gram Backward-dictionary Sbwd ←∅3: Random c(i) j , ∀j ∈[0, N −1], ∀i ∈[0..W −1]', '4: t ←n + 15: while t ≤m do6:{Obtain the guesses}', '7:G ←Sfwd[xt−1]', '8:u = ∅', '9:for j = 0 to N −1 do', '10:for k = N −1 to 1 do', '11:uj ←Sbwd[xt+j−k:t−1 ⊕u0:j−1]', '12:break if found value for uj', '13:end for', '14:end for', '15:G.append(u)', '16:{Foward in LLM}', '17:Obtain necessary distributions of PM in parallel.', '18:{Verification}', '19:{Greedy verify (Alg. 3) or Sampling verify (Alg. 4)}', '20:hits ←VerificationFunction(x, PM, g)', '21:x = x ⊕hits', '22:t ←t + size(hits)', '23:{Predict Candidates}', '24:for i = 0 to W −1 do', '25:r ∼Uniform[0, 1]', '26:Pc(c(i) N−1) ←PM(c(i) N−1 | c(i) :N−2, x)', '27:if r > τ then', '28:c(i) N−1 ←argmax c/∈SfwdPc(c(i) N−1)', '29:else', '30:c(i) N−1 ←argmax Pc(c(i) N−1)', '31:end if', '32:end for', '33:{Update N-gram dictionaries}', '34:for i = 0 to W −1 do', '35:for j = 0 to N −2 do', '36:Sfwd[c(i) j ].append(c(i) j+1:)', '37:Sbwd[c(i) 0:j] ←c(i) j+1', '38:end for', '39:end for', '40:{Update Candidates}', '41:c(i) j←c(i) j+1, ∀j ∈[0, N −2], ∀i', '42: end while43: Output: xn+1:n+m = (y1, y2, . . . , ym)', 'The dictionaries Sfwd and Sbwd are updated with N-187', 'grams from the candidate pool. The details of the188', 'SPECTRA-CORE decoding process are described189', 'in Algorithm 1.190', 'Bi-directional Search for GuessesAt each step,191', 'SPECTRA generates G guess sequences G=192', '{˜y(0), ˜y(1), . . . , ˜y(G)}. Unlike previous work (Fu193', 'et al., 2024), which enforces uniform guess lengths,194', 'SPECTRA supports variable-length guesses, im-195', 'proving both flexibility and efficiency. The for-196', 'ward dictionary Sfwd maps a token to a list of197', 'sequences, while the backward dictionary Sbwd198', 'maps a sequence to a single token. At time step199', '3', 'Input', 'Input', 'Large Language Model', 'Token distribution', 'Figure 2: Details of SPECTRA forward step in LLM.The dashed arrow indicates interactions between thetokens, which are realized by the LLM’s attention mask.', 't, the set of guesses is obtained through a bidi-200', 'rectional search (Alg. 1, lines 7–15). This search201', 'operates in two directions: (1) the forward direc-202', 'tion, which prioritizes the quantity of guesses, and203', '(2) the backward direction, which prioritizes the204', 'quality of guesses. In the forward direction, the205', 'last generated token xt−1 is used to search Sfwd206', 'for guess sequences (Alg. 1, line 7). In the back-207', 'ward direction, a high-quality guess is constructed208', 'by iteratively predicting one token at a time using209', 'Sbwd, repeating the process until a desired sequence210', 'length N is reached (Alg. 1, lines 8–14).211', 'Predict & Verify in One Forward PassAll dis-212', 'tributions required for predicting candidates and213', 'verifying guesses are obtained in a single forward214', 'pass to the LLM, leveraging parallel processing215', '(Figure 2). This is achieved using a specially de-216', 'signed attention mask that specifies the allowed217', 'interactions between tokens. For instance, the to-218', 'ken c(1) 2attends only to c(1) 1 , c(1) 0 , and the input.219', 'Predict Tokens for Candidate PoolWe predict220', 'the next candidate tokens c(i) N−1 for the candidate221', 'pool using the distribution obtained from the for-222', 'ward pass (Alg. 1, lines 24–32). A straightfor-223', 'ward approach is to select tokens with the highest224', 'probability in the token distribution. However, we225', 'observe that when searching for guesses in the for-226', 'ward dictionary Sfwd, it is crucial for the search to-227', 'ken to exist in the dictionary; otherwise, no guesses228', 'can be retrieved. To address this, we introduce a229', 'randomness-based mechanism to increase the cov-230', 'erage of Sfwd. Specifically, we probabilistically231', 'encourage the selection of unseen tokens in Sfwd232', 'using a hyperparameter τ ∈[0, 1]. Let r be a233', 'random draw from [0, 1]. If r > τ, we select to-234', 'kens with the highest probability that are not in235', 'Sfwd; otherwise, we choose tokens with the high-236', 'est probability regardless of their presence in Sfwd.237', 'Although c(i) N−1 does not immediately affect the238', 'coverage of Sfwd, it contributes to coverage expan-239', 'sion in subsequent time steps through our candidate240', 'updating mechanism. At the end of each time step,241', 'all candidate sequences are shifted left by one to-242', 'ken: c(i) j←c(i) j+1, leaving c(i) N−1 empty and ready243', 'for prediction in the next time step (Alg. 1, line 41).244', 'Update N-gram DictionariesAt the end of each245', 'time step, candidate tokens from the pool C are246', 'used to update the N-gram dictionaries Sfwd and247', 'Sbwd. While previous work (Fu et al., 2024) only248', 'adds the full N-gram (c(i) 0 , c(i) 1 , . . . , c(i) N ), we ob-249', 'serve that subsequences within N-grams often ap-250', 'pear later in the generation process. By including251', 'these subsequences in the N-gram storage, we im-252', 'prove both the quality of guesses and the coverage253', 'of the dictionaries. Specifically, we add subse-254', 'quences to Sfwd using the first token as the key,255', 'and update Sbwd by mapping the preceding part of256', 'the sequence to the last token (Alg. 1, lines 33–39).257', '3.2SPECTRA-RETRIEVAL258', 'SPECTRA-RETRIEVALleveragesanexternal259', 'knowledge source to generate guesses. This in-260', 'volves processing a text corpus and indexing it into261', 'a structure that supports fast prefix search, such as a262', 'trie. At each time step, the last generated tokens are263', 'used as input to this structure to retrieve guesses for264', 'speculative decoding. However, we observe that us-265', 'ing random texts from the corpus without selection266', 'can limit the speedup gain. To address this, we pro-267', 'pose a method to identify and select high-quality,268', 'relevant texts from the corpus tailored to the spe-269', 'cific LLM. This improves the speedup gain and270', 'enables seamless integration with other speculative271', 'decoding approaches, including SPECTRA-CORE.272', 'Corpus Refinement by PerplexityGiven a text sequence u = (u0, u1, u2, . . . ), perplexity quan-tifies the average uncertainty of the model when predicting the next token, conditioned on the pre-ceding tokens. It is calculated as:', 'PPL(u) = exp', '�', '−1', 't', 't�', 'i=1log pθ(ui | u<i)', '�', 'A lower perplexity indicates that the model assigns273', 'higher probabilities to the sequence, suggesting274', 'that the sequence is well-aligned with the model’s275', 'predictions and can produce high-quality guesses276', 'for speculative decoding. To optimize the retrieval277', 'process, we select texts with the lowest perplexity278', '4', 'from the corpus to form a smaller, high-quality sub-279', 'set, which is then used to construct the trie structure280', 'for generating guesses.281', 'Integration with SPECTRA-COREOur exper-282', 'iments (Section 5.2, Table 2) demonstrate that283', 'naively integrating guesses from external sources284', '(e.g., REST (He et al., 2024)) into other specula-285', 'tive methods (e.g., Lookahead (Fu et al., 2024))286', 'can lead to a noticeable drop in speedup. This oc-287', 'curs because the forward pass in the LLM can only288', 'handle a limited number of guesses, and exceeding289', 'this limit increases memory usage and slows down290', 'generation. With a limited guess budget, guesses291', 'from external sources can only account for a frac-292', 'tion of the total guesses, causing the search time293', 'in the indexing structure (e.g., a trie) to outweigh294', 'the speedup gain. To address this, it is crucial295', 'to limit the size of the external knowledge while296', 'maintaining the quality of the guesses. By refining297', 'the corpus using perplexity, SPECTRA-RETRIEVAL298', 'seamlessly integrates with SPECTRA-CORE, further299', 'boosting the speedup gain. Specifically, we inte-300', 'grate SPECTRA-RETRIEVAL into SPECTRA-CORE301', 'by including its guesses in the set of guesses during302', 'the guess generation step (Alg. 1, lines 7–15).303', '4Experiments304', 'Models.We evaluate LLaMA-2-Chat 7B, 13B,305', '70B (Touvron et al., 2023), CodeLlama 7B, 13B306', '(Rozière et al., 2024), and LLaMA-3-Instruct 8B,307', '70B (Dubey et al., 2024).308', 'Tasks.Weconductcomprehensiveevalua-309', 'tions on various generation tasks.MT-Bench310', '(Zheng et al., 2023) for multi-turn conversation;311', 'GSM8K(Cobbe et al., 2021) for mathemati-312', 'cal reasoning; HumanEval(Chen et al., 2021),313', 'MBPP(Austin et al., 2021) and ClassEval (Du314', 'et al., 2023) for code generation.315', 'Metrics.SPECTRA does not modify the original316', 'LLM and the acceptance conditions, making it a317', 'lossless acceleration method. Therefore, the gener-318', 'ation quality remains the same as the original LLM.319', 'We only evaluate the acceleration performance us-320', 'ing the following metrics.321', '• Speedup Ratio: The speedup ratio relative to322', 'autoregressive decoding.323', '• Compression ratio: The ratio of the total324', 'number of autoregressive steps to the number325', 'of Spectra decoding steps needed to produce326', 'the same sequence length.327', 'Baselines.We use standard autoregressive decod-328', 'ing as the baseline (speed-up ratio = 1.00x). We fur-329', 'ther compare SPECTRA with leading non-training330', 'speculative decoding approaches, namely Adaptive331', 'N-gram (Ou et al., 2024), REST (He et al., 2024),332', 'and Lookahead (Fu et al., 2024). For details regard-333', 'ing implementation settings of both SPECTRA and334', 'these baselines, please refer to Appendix B.335', '5Results336', '5.1Main Results337', 'Overall Performance.The top portion of Table 1338', 'presents the speedup ratios of all evaluated meth-339', 'ods under a greedy decoding setup. Our approach,340', 'SPECTRA, consistently yields the highest accelera-341', 'tion across the entire range of datasets and LLMs.342', 'In particular, SPECTRA achieves speedups up to343', '4.08× with LLama-3-8B-Instruct on the MBPP344', 'dataset.345', 'For smaller models (7B), SPECTRA often sur-346', 'passes 3× acceleration, underscoring the effective-347', 'ness of multi-token compression. By contrast, for348', '13B models, while the boost remains strong, it is349', 'relatively more moderate, typically falling in the350', '1.6×–3× band. We attribute this trend to the in-351', 'creased overhead of each forward pass in larger net-352', 'works, which can dampen the proportional gains of353', 'fewer decoding iterations per token. Despite this,354', 'SPECTRA continues to outperform baselines across355', 'all parameter settings.356', 'Significant advantages are evident in tasks such357', 'as GSM8K and ClassEval, where outputs often358', 'follow recurring patterns (e.g., repeated variable359', 'names or class definitions). In these scenarios,360', 'SPECTRA combines internal knowledge of par-361', 'tial sequences with external retrieval suggestions,362', 'thereby proposing accurate multi-token guesses.363', 'On the other hand, in domains featuring more var-364', 'ied or unpredictable responses—such as complex365', 'multi-turn conversations in MT-Bench—the accep-366', 'tance rate is somewhat lower, although still com-367', 'petitive.368', 'Compression Ratio.Table 1 also reports each369', 'method’s compression rate, a measure agnostic370', 'to specific hardware configurations. Across ev-371', 'ery dataset and LLM tested, SPECTRA delivers the372', 'highest average compression ratio. Each of SPEC-373', 'TRA’s draft-and-verify iterations typically yields374', '5', 'ClassevalGSM8KHumanevalMBPPMTBenchAVGModelMethodspeedupτspeedupτspeedupτspeedupτspeedupτspeedup', 'Greedy (temperature=0)', 'CL-13BANPD1.942.522.813.722.082.502.713.582.613.412.43 CL-13BLookahead2.253.612.804.242.303.162.914.442.594.042.57 CL-13BREST1.282.140.931.541.582.310.851.400.941.531.12 CL-13BSPECTRA (Ours)2.384.062.914.652.633.953.294.462.654.402.77', 'CL-7BANPD2.302.683.213.752.162.473.163.783.353.832.84 CL-7BLookahead2.593.662.993.832.503.052.903.673.234.272.84 CL-7BREST1.452.220.911.391.702.340.961.451.021.441.21 CL-7BSPECTRA (Ours)2.704.103.334.592.963.903.564.453.704.523.25', 'L2-13BANPD1.361.781.471.721.341.611.121.321.171.371.29 L2-13BLookahead1.812.761.461.871.732.321.381.691.512.041.58 L2-13BREST1.222.010.941.461.251.940.951.441.141.901.10 L2-13BSPECTRA (Ours)2.003.241.832.621.962.911.632.241.752.601.83', 'L2-70BANPD1.821.901.631.611.861.871.171.201.341.301.56 L2-70BLookahead2.652.871.862.022.572.671.491.541.942.002.10 L2-70BSPECTRA (Ours)3.103.402.522.693.223.371.861.932.432.512.62', 'L2-7BANPD1.621.951.521.681.541.671.191.331.301.371.43 L2-7BLookahead2.192.941.661.932.062.421.461.691.732.051.82 L2-7BREST1.362.121.011.471.412.041.011.461.251.901.21 L2-7BSPECTRA (Ours)2.403.432.112.642.403.051.772.162.022.592.14', 'L3-70BANPD1.541.671.501.471.831.881.461.411.231.231.51 L3-70BLookahead2.402.621.541.582.562.701.431.451.761.861.94 L3-70BSPECTRA (Ours)2.672.912.102.142.843.021.941.942.062.132.32', 'L3-8BANPD2.112.493.864.571.832.093.363.581.141.232.46 L3-8BLookahead2.593.443.714.612.492.893.794.651.531.852.82 L3-8BSPECTRA (Ours)2.833.493.894.772.573.024.084.761.692.103.01', 'Sampling (temperature=1.0)', 'CL-13BANPD1.151.461.071.311.051.301.001.242.312.891.31 CL-13BLookahead1.382.001.081.431.291.751.021.342.333.481.42 CL-13BREST1.141.870.821.351.271.960.841.390.931.501.00 CL-13BSPECTRA (Ours)1.682.221.201.751.652.121.151.702.373.801.61', 'CL-7BANPD1.291.501.161.301.101.321.121.272.773.051.49 CL-7BLookahead1.542.031.191.411.431.811.191.432.723.501.61 CL-7BREST1.231.860.881.331.331.980.911.400.971.441.06 CL-7BSPECTRA (Ours)1.812.251.351.731.682.121.331.722.783.941.79', 'L2-13BANPD1.201.521.241.461.171.401.031.221.171.351.16 L2-13BLookahead1.522.221.321.691.482.001.181.481.492.011.40 L2-13BREST1.181.960.931.451.191.880.921.441.121.881.07 L2-13BSPECTRA (Ours)1.702.751.552.231.692.591.341.891.742.571.60', 'L2-7BANPD1.311.511.341.481.281.461.101.221.251.361.26 L2-7BLookahead1.782.301.511.761.722.091.251.491.682.021.59 L2-7BREST1.262.030.991.461.271.930.961.411.211.881.14 L2-7BSPECTRA (Ours)1.972.831.782.282.042.751.471.841.972.541.85', 'L3-8BANPD1.251.371.972.181.431.651.892.071.151.211.54 L3-8BLookahead1.481.782.072.411.792.211.992.401.571.811.78 L3-8BSPECTRA (Ours)1.942.842.272.781.922.512.192.781.702.052.01', 'Table 1: Overall performance of speculative decoding methods across multiple tasks. “CL-xB” denotes CodeLlamawith xB parameters, “L2-xB” denotes LLaMA-2-Chat of size xB, and “L3-xB” denotes LLaMA-3-Instruct of size xB. We report the speedup ratio (vs. autoregressive) and the compression ratio τ.', '2.1–4.8 tokens, substantially outpacing alternative375', 'approaches and nearly doubling the acceptance376', 'length achieved by REST.377', 'Acceleration in Sampling Decoding.The lower378', 'section of Table 1 investigates the performance379', 'of SPECTRA under sampling-based decoding with380', 'a temperature of 1.0. The results highlight how381', '6', 'SPECTRA continues to accelerate generation rel-382', 'ative to baselines, offering roughly 1.15–2.77×383', 'speedups over standard autoregressive decoding.384', 'These gains are more modest than in greedy decod-385', 'ing, reflecting the lower acceptance rate under the386', 'sampling-based verification phase, which is consis-387', 'tent with earlier findings (Fu et al., 2024; Leviathan388', 'et al., 2023).389', '5.2Analysis390', 'Ablationstudy.Weconductedadetailed391', 'component-wise analysis to determine the contri-392', 'bution of each module to the framework’s over-393', 'all performance (Table 2). Specifically, the re-394', 'sults on LLaMA2-7B-chat reveal that removing395', 'different components yields varying impacts on396', 'GSM8K speedups.Under the “CORE Module”397', 'configuration, excluding multi-level n-grams low-398', 'ers the speedup from 2.04× to 1.95× (a 4% de-399', 'crease), whereas turning off forward information400', 'reduces it from 2.04× to 1.50× (a 26% drop). Sim-401', 'ilarly, omitting backward information results in402', 'a speedup of 1.94×, down from 2.04×. In con-403', 'trast, the “RETRIEVAL Module” setting shows that404', 'leaving out perplexity-based filtering decreases the405', 'speedup from 1.18× to 1.16×. Our fully integrated406', 'approach, SPECTRA, achieves a 2.14× speedup407', 'on GSM8K—outperforming both the “CORE Mod-408', 'ule” (2.04×) and “RETRIEVAL Module” (1.18×)409', 'variants. This improvement demonstrates the im-410', 'portance of combining multi-level n-grams, for-411', 'ward/backward drafting, and perplexity-based re-412', 'finement in boosting acceptance rates and enhanc-413', 'ing overall speedups. A similar trend was also414', 'observed in the results of the MTBench dataset.415', 'Additionally,wecomparedourmethod416', 'against a naive combination of Lookahead and417', 'REST—where guess sequences from REST are418', 'added to Lookahead. This combined approach419', 'falls significantly short of our SPECTRA method,420', 'highlighting that a simple merger of two techniques421', 'is insufficient without our carefully optimized422', 'integration strategy and components.423', 'Priority for source of guessesSince verifying424', 'too many candidate tokens at once can strain GPU425', 'resources and reduce speedups (Fu et al., 2024; Li426', 'et al., 2024b), SPECTRA limits how many guesses427', 'proceed to verification in each step (Appendix B).428', 'To understand whether internal or external guesses429', 'are more valuable, we temporarily remove this430', 'cap and measure acceptance rates (Figure 3). We431', 'GSM8KMTBenchMethodspeedupτspeedupτ', 'REST1.011.471.251.90', 'Lookahead1.661.931.732.05', 'Lookahead + REST1.081.471.271.90', 'SPECTRA’s ablation', 'CORE Module2.042.501.922.35 - w/o forward info1.501.681.201.37 - w/o backward info1.942.211.742.12 - w/o Sub-Ngram1.952.341.752.18', 'RETRIEVAL Module1.181.311.241.50 - w/o PPL refine1.161.291.201.45', 'SPECTRA (ours)2.142.642.022.59', 'Table 2: Ablation of SPECTRA’s components (greedydecoding, LLaMA2-7B-Chat).“Sub-Ngram” aug-ments each n-gram with its sub-sequences; “for-ward/backward info” uses internal expansions; and“PPL refine” applies perplexity-based filtering for ex-ternal retrieval. “Lookahead + REST” denotes a naivecombination where guess sequences from REST are di-rectly added to Lookahead', 'observe that sequences generated via internal ex-432', 'pansions—particularly forward and backward pre-433', 'dictions—have a higher acceptance probability434', 'than those retrieved from external sources. Con-435', 'sequently, SPECTRA prioritizes internal guesses436', 'for verification. Interestingly, in code-generation437', 'tasks like HumanEval, external suggestions be-438', 'come more influential, likely due to code’s repeti-439', 'tive structure and the retrieval of similar snippets.440', 'This observation indicates that a strategic blend441', 'of backward internal knowledge and external re-442', 'trieval can be particularly fruitful in these domains,443', 'especially when computational resources limit ex-444', 'tensive forward expansions.445', 'FlashAttention.Figure 3 shows that enabling446', 'FlashAttention consistently boosts the speedup of447', 'all methods, albeit to varying degrees. Notably,448', 'we observe an additional 0.24× speedup gain for449', 'SPECTRA on both GSM8K and MTBench. This450', 'is because FlashAttention better exploits the paral-451', 'lel structure of speculative decoding by reducing452', 'attention overheads, especially when verifying mul-453', 'tiple guessed tokens in parallel. Although smaller454', 'gains are also seen for other methods, SPECTRA455', 'benefits the most, as it presents the longest verifica-456', 'tion branches and thus stands to profit significantly457', 'from more efficient attention implementations.458', '7', '020406080100', 'Total accept tokens (%)', 'ClassEval', 'GSM8K', 'HumanEval', 'MBPP', 'MT-Bench', 'Internal-FwdInternal-BwdExternal', 'Figure 3: Acceptance rates for different guess sources(e.g., SPECTRA-CORE forward dictionary, backwarddictionary, SPECTRA-RETRIEVAL’s guesses). The ac-ceptance rate is the fraction of guessed tokens that passverification and are appended to the final output.', '0', '1', '2', 'Speedup', '1.00x', '1.68x1.55x', '1.03x', '2.08x', '1.07x', '1.86x1.70x', '1.09x', '2.32x', 'GSM8K', 'W/o flashWith flash', 'Autoreg.LookaheadANPDRESTSpectra', '0', '1', '2', 'Speedup', '1.00x', '1.75x', '1.31x1.24x', '2.01x', '1.10x', '1.96x', '1.42x1.38x', '2.25x', 'MTBench', 'W/o flashWith flash', 'Figure 4: Effect of FlashAttention on speculative de-coding speed: Measured speedups on GSM8K andMTBench (LLama2-7B-Chat, greedy decoding). “NoFlash” uses standard attention; “With Flash” usesFlashAttention for faster parallel verification.', '6Related Works459', 'Large language models (LLMs) are increasingly460', 'deployed in a range of applications, motivating on-461', 'going research into more efficient inference (Liu462', 'et al., 2025). Common strategies include quan-463', 'tizing model weights into lower-precision formats464', '(Liu et al., 2024b; Lin et al., 2024; Zhao et al., 2024;465', 'Park et al., 2024), pruning redundant parameters466', '(Ma et al., 2023; Xia et al., 2023; Sun et al., 2023a;467', 'Le et al., 2025), and employing knowledge distilla-468', 'tion (Gu et al., 2024; Friha et al., 2024; Zhang et al.,469', '2024b). These techniques help reduce the compu-470', 'tational load per forward pass, thereby lowering471', 'generation latency. However, they often introduce472', 'some degradation in model performance, forcing473', 'practitioners to balance quality with efficiency.474', 'A growing line of work explores speculative de-475', 'coding as a strategy for accelerating generation476', 'while maintaining the exact output distribution477', '(Chen et al., 2023; Leviathan et al., 2023). Some478', 'speculative decoding approaches train a smaller479', 'LLM (referred to as a draft model) (Chen et al.,480', '2023; Leviathan et al., 2023; Miao et al., 2024; Sun481', 'et al., 2023b; Zhou et al., 2024; Cai et al., 2024),482', 'or train the original LLM itself in a special man-483', 'ner (referred to as self-speculative) (Elhoushi et al.,484', '2024; Liu et al., 2024a; Yang et al., 2024; Zhang485', 'et al., 2024a; Li et al., 2024b) to guess several sub-486', 'sequent tokens and then verify them parallelly us-487', 'ing the original LLM. As these approaches require488', 'training, they pose limitations, such as requiring489', 'heavy computational resources and losing the orig-490', 'inal model capabilities.491', 'To avoid additional training, alternative specula-492', 'tive decoding methods leverage external resources493', 'or structural properties of language generation.494', 'Retrieval-based methods sidestep draft model train-495', 'ing by using a datastore indexed with observed496', 'prefixes to retrieve guess sequences (Yang et al.,497', '2023; He et al., 2024; Li et al., 2024a). Other498', 'approaches, such as Jacobi-like parallel decoding499', '(Santilli et al., 2023) and lookahead decoding (Fu500', 'et al., 2024), mitigate left-to-right dependencies by501', 'generating and validating multiple candidate tokens502', 'in parallel. These training-free techniques achieve503', 'comparable speedups to learned methods without504', 'requiring model optimization, making them ideal505', 'for scenarios with computational or deployment506', 'constraints.507', '7Conclusions508', 'In this work, we introduced SPECTRA, a hybrid509', 'speculative decoding framework that combines510', 'multi-level n-grams (internal knowledge) with511', 'perplexity-based retrieval (external knowledge) to512', 'achieve speedups of up to 4.08× across various513', 'LLMs and benchmarks, without additional train-514', 'ing or compromising exact output fidelity. Our515', 'ablation studies show that each module (multi-516', 'level n-grams, forward/backward expansions, and517', 'perplexity-based datastore curation) substantially518', 'boosts acceptance rates, and their synergy outper-519', 'forms existing non-training methods. By offering a520', 'lossless speedup that efficiently exploits both inter-521', 'nal patterns and external texts, SPECTRA provides522', 'a practical, high-impact solution for accelerating523', 'inference in large language models.524', '8', '8Limitations525', '(1) Cost of Building External Datastores.Al-526', 'though our internal-knowledge strategy only relies527', 'on sequences observed during generation (and thus528', 'requires no extra data), our external-knowledge529', 'approach depends on constructing and indexing a530', 'sizeable datastore from potentially large corpora.531', 'This process can be time-consuming and memory-532', 'intensive, particularly in domains where data up-533', 'dates frequently or storage is constrained. While534', 'this additional investment can yield substantial535', 'speedups by increasing token acceptance rates, it536', 'may not be universally feasible or cost-effective.537', '(2) Limited Evaluation Scope.Our experiments538', 'center primarily on English-language benchmarks539', 'in conversational and coding tasks using LLaMA-540', 'based models. Although SPECTRA can, in princi-541', 'ple, be applied to other models or languages, addi-542', 'tional factors such as domain-specific tokenization543', 'or specialized textual structures may affect the ac-544', 'ceptance rate and overall speedup. Future work is545', 'needed to assess the generality of SPECTRA across546', 'diverse linguistic settings (e.g., low-resource lan-547', 'guages or specialized technical documents) and for548', 'a wider range of model families (beyond LLaMA-549', 'based architectures) to confirm and refine its appli-550', 'cability.551', 'References552', 'Jacob Austin, Augustus Odena, Maxwell Nye, Maarten553', 'Bosma, Henryk Michalewski, David Dohan, Ellen554', 'Jiang, Carrie Cai, Michael Terry, Quoc Le, and others.555', '2021. Program synthesis with large language models.556', 'arXiv preprint arXiv:2108.07732.557', 'Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,558', 'Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei559', 'Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,560', 'Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,561', 'Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,562', 'Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong563', 'Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-564', 'guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,565', 'Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,566', 'Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-567', 'uan Zhang, Yichang Zhang, Zhenru Zhang, Chang568', 'Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang569', 'Zhu. 2023. Qwen Technical Report.570', 'Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu571', 'Peng, Jason D. Lee, Deming Chen, and Tri Dao.572', '2024. MEDUSA: Simple LLM inference acceler-573', 'ation framework with multiple decoding heads. In574', 'Proceedings of the 41st International Conference on575', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-576', 'enna, Austria.577', 'Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,578', 'Jean-Baptiste Lespiau, Laurent Sifre, and John579', 'Jumper. 2023. Accelerating Large Language Model580', 'Decoding with Speculative Sampling.581', 'Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,582', 'Henrique Ponde De Oliveira Pinto, Jared Kaplan,583', 'Harri Edwards, Yuri Burda, Nicholas Joseph, Greg584', 'Brockman, and others. 2021.Evaluating large585', 'language models trained on code. arXiv preprint586', 'arXiv:2107.03374.587', 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,588', 'Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias589', 'Plappert, Jerry Tworek, Jacob Hilton, Reiichiro590', 'Nakano, and others. 2021.Training verifiers591', 'to solve math word problems.arXiv preprint592', 'arXiv:2110.14168.593', 'Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin,594', 'Shengding Hu, Zhiyuan Liu, Maosong Sun, and595', 'Bowen Zhou. 2023. Enhancing Chat Language Mod-596', 'els by Scaling High-quality Instructional Conversa-597', 'tions. In Proceedings of the 2023 Conference on598', 'Empirical Methods in Natural Language Processing,599', 'pages 3029–3051, Singapore. Association for Com-600', 'putational Linguistics.601', 'Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang,602', 'Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng603', 'Sha, Xin Peng, and Yiling Lou. 2023.Classe-604', 'val: A manually-crafted benchmark for evaluating605', 'llms on class-level code generation. arXiv preprint606', 'arXiv:2308.01861.607', 'Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,608', 'Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,609', 'Akhil Mathur, Alan Schelten, Amy Yang, Angela610', 'Fan, et al. 2024. The llama 3 herd of models. arXiv611', 'preprint arXiv:2407.21783.612', 'Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich,613', 'Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas614', 'Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed615', 'Roman, Ahmed Aly, Beidi Chen, and Carole-Jean616', 'Wu. 2024. LayerSkip: Enabling Early Exit Infer-617', 'ence and Self-Speculative Decoding. In Proceedings618', 'of the 62nd Annual Meeting of the Association for619', 'Computational Linguistics (Volume 1: Long Papers),620', 'pages 12622–12642, Bangkok, Thailand. Association621', 'for Computational Linguistics.622', 'Othmane Friha, Mohamed Amine Ferrag, Burak623', 'Kantarci, Burak Cakmak, Arda Ozgun, and Nassira624', 'Ghoualmi-Zine. 2024. Llm-based edge intelligence:625', 'A comprehensive survey on architectures, applica-626', 'tions, security and trustworthiness. IEEE Open Jour-627', 'nal of the Communications Society.628', 'Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang.629', '2024.Break the sequential dependency of LLM630', 'inference using LOOKAHEAD DECODING. In631', 'Proceedings of the 41st International Conference on632', '9', 'Machine Learning, ICML’24. JMLR.org. Place: Vi-633', 'enna, Austria.634', 'Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024.635', 'Minillm: Knowledge distillation of large language636', 'models. In The Twelfth International Conference on637', 'Learning Representations.638', 'Zhenyu He, Zexuan Zhong, Tianle Cai, Jason Lee, and639', 'Di He. 2024. REST: Retrieval-Based Speculative640', 'Decoding. In Proceedings of the 2024 Conference of641', 'the North American Chapter of the Association for642', 'Computational Linguistics: Human Language Tech-643', 'nologies (Volume 1: Long Papers), pages 1582–1595,644', 'Mexico City, Mexico. Association for Computational645', 'Linguistics.646', 'Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and647', 'Yejin Choi. 2020. The Curious Case of Neural Text648', 'Degeneration. In International Conference on Learn-649', 'ing Representations.650', 'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-651', 'sch, Chris Bamford, Devendra Singh Chaplot, Diego652', 'de las Casas, Florian Bressand, Gianna Lengyel, Guil-653', 'laume Lample, Lucile Saulnier, Lélio Renard Lavaud,654', 'Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,655', 'Thibaut Lavril, Thomas Wang, Timothée Lacroix,656', 'and William El Sayed. 2023. Mistral 7B.657', 'Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI,658', 'Chenghao Mou, Yacine Jernite, Margaret Mitchell,659', 'Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf,660', 'Dzmitry Bahdanau, Leandro Von Werra, and Harm de661', 'Vries. 2023. The Stack: 3 TB of permissively li-662', 'censed source code. Transactions on Machine Learn-663', 'ing Research.664', 'Wouter Kool, Herke van Hoof, and Max Welling. 2020.665', 'Ancestral Gumbel-Top-k Sampling for Sampling666', 'Without Replacement. Journal of Machine Learning667', 'Research, 21(47):1–36.668', 'Khang Nguyen Le, Ryo Sato, Dai Nakashima, Takeshi669', 'Suzuki, and Minh Le Nguyen. 2025. Optiprune: Ef-670', 'fective pruning approach for every target sparsity. In671', 'Proceedings of the 31st International Conference on672', 'Computational Linguistics, pages 3600–3612.673', 'Yaniv Leviathan, Matan Kalman, and Yossi Matias.674', '2023. Fast inference from transformers via spec-675', 'ulative decoding. In Proceedings of the 40th Interna-676', 'tional Conference on Machine Learning, ICML’23.677', 'JMLR.org. Place: Honolulu, Hawaii, USA.678', 'Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen,679', 'Jimmy Lin, Wen-tau Yih, and Xi Victoria Lin. 2024a.680', 'Nearest Neighbor Speculative Decoding for LLM681', 'Generation and Attribution. In The Thirty-eighth An-682', 'nual Conference on Neural Information Processing683', 'Systems.684', 'Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang685', 'Zhang. 2024b. EAGLE-2: Faster Inference of Lan-686', 'guage Models with Dynamic Draft Trees. In Proceed-687', 'ings of the 2024 Conference on Empirical Methods688', 'in Natural Language Processing, pages 7421–7432,689', 'Miami, Florida, USA. Association for Computational690', 'Linguistics.691', 'Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-692', 'Ming Chen, Wei-Chen Wang, Guangxuan Xiao,693', 'Xingyu Dang, Chuang Gan, and Song Han. 2024.694', 'Awq: Activation-aware weight quantization for on-695', 'device llm compression and acceleration. Proceed-696', 'ings of Machine Learning and Systems, 6:87–100.697', 'Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng698', 'Ni, Duyu Tang, Kai Han, and Yunhe Wang. 2024a.699', 'Kangaroo: Lossless Self-Speculative Decoding for700', 'Accelerating LLMs via Double Early Exiting. In The701', 'Thirty-eighth Annual Conference on Neural Informa-702', 'tion Processing Systems.703', 'Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan704', 'Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xi-705', 'aohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu,706', 'Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang,707', 'Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu,708', 'and Bao Ge. 2025. Understanding llms: A compre-709', 'hensive overview from training to inference. Neuro-710', 'computing, 620:129190.711', 'Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie712', 'Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,713', 'Raghuraman Krishnamoorthi, and Vikas Chandra.714', '2024b. LLM-QAT: Data-free quantization aware715', 'training for large language models. In Findings of716', 'the Association for Computational Linguistics: ACL717', '2024, pages 467–484, Bangkok, Thailand. Associa-718', 'tion for Computational Linguistics.719', 'Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023.720', 'Llm-pruner: On the structural pruning of large lan-721', 'guage models. Advances in neural information pro-722', 'cessing systems, 36:21702–21720.723', 'Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao724', 'Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee725', 'Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan726', 'Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Ab-727', 'hyankar, and Zhihao Jia. 2024. SpecInfer: Accelerat-728', 'ing Large Language Model Serving with Tree-based729', 'Speculative Inference and Verification. In Proceed-730', 'ings of the 29th ACM International Conference on Ar-731', 'chitectural Support for Programming Languages and732', 'Operating Systems, Volume 3, ASPLOS ’24, pages733', '932–949, New York, NY, USA. Association for Com-734', 'puting Machinery. Event-place: La Jolla, CA, USA.735', 'Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,736', 'Ça˘glar Gu˙lçehre, and Bing Xiang. 2016. Abstrac-737', 'tive text summarization using sequence-to-sequence738', 'RNNs and beyond.In Proceedings of the 20th739', 'SIGNLL Conference on Computational Natural Lan-740', 'guage Learning, pages 280–290, Berlin, Germany.741', 'Association for Computational Linguistics.742', 'Shashi Narayan, Shay B Cohen, and Mirella Lap-743', 'ata. 2018.Don’t give me the details, just the744', '10', 'summary!topic-aware convolutional neural net-745', 'works for extreme summarization. arXiv preprint746', 'arXiv:1808.08745.747', 'OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,748', 'Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-749', 'man, Diogo Almeida, Janko Altenschmidt, Sam Alt-750', 'man, Shyamal Anadkat, Red Avila, Igor Babuschkin,751', 'Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-752', 'ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-753', 'wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,754', 'Christopher Berner, Lenny Bogdonoff, Oleg Boiko,755', 'Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-756', 'man, Tim Brooks, Miles Brundage, Kevin Button,757', 'Trevor Cai, Rosie Campbell, Andrew Cann, Brittany758', 'Carey, Chelsea Carlson, Rory Carmichael, Brooke759', 'Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully760', 'Chen, Ruby Chen, Jason Chen, Mark Chen, Ben761', 'Chess, Chester Cho, Casey Chu, Hyung Won Chung,762', 'Dave Cummings, Jeremiah Currier, Yunxing Dai,763', 'Cory Decareaux, Thomas Degry, Noah Deutsch,764', 'Damien Deville, Arka Dhar, David Dohan, Steve765', 'Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,766', 'Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,767', 'Simón Posada Fishman, Juston Forte, Isabella Ful-768', 'ford, Leo Gao, Elie Georges, Christian Gibson, Vik769', 'Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-770', 'Lopes, Jonathan Gordon, Morgan Grafstein, Scott771', 'Gray, Ryan Greene, Joshua Gross, Shixiang Shane772', 'Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,773', 'Yuchen He, Mike Heaton, Johannes Heidecke, Chris774', 'Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,775', 'Brandon Houghton, Kenny Hsu, Shengli Hu, Xin776', 'Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,777', 'Joanne Jang, Angela Jiang, Roger Jiang, Haozhun778', 'Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-779', 'woo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka-780', 'mali, Ingmar Kanitscheider, Nitish Shirish Keskar,781', 'Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,782', 'Christina Kim, Yongjik Kim, Jan Hendrik Kirch-783', 'ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,784', 'Łukasz Kondraciuk, Andrew Kondrich, Aris Kon-785', 'stantinidis, Kyle Kosic, Gretchen Krueger, Vishal786', 'Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan787', 'Leike, Jade Leung, Daniel Levy, Chak Ming Li,788', 'Rachel Lim, Molly Lin, Stephanie Lin, Mateusz789', 'Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,790', 'Anna Makanju, Kim Malfacini, Sam Manning, Todor791', 'Markov, Yaniv Markovski, Bianca Martin, Katie792', 'Mayer, Andrew Mayne, Bob McGrew, Scott Mayer793', 'McKinney, Christine McLeavey, Paul McMillan,794', 'Jake McNeil, David Medina, Aalok Mehta, Jacob795', 'Menick, Luke Metz, Andrey Mishchenko, Pamela796', 'Mishkin, Vinnie Monaco, Evan Morikawa, Daniel797', 'Mossing, Tong Mu, Mira Murati, Oleg Murk, David798', 'Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,799', 'Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,800', 'Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex801', 'Paino, Joe Palermo, Ashley Pantuliano, Giambat-802', 'tista Parascandolo, Joel Parish, Emy Parparita, Alex803', 'Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-804', 'man, Filipe de Avila Belbute Peres, Michael Petrov,805', 'Henrique Ponde de Oliveira Pinto, Michael, Poko-806', 'rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-807', 'ell, Alethea Power, Boris Power, Elizabeth Proehl,808', 'Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,809', 'Cameron Raymond, Francis Real, Kendra Rimbach,810', 'Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-811', 'der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,812', 'Girish Sastry, Heather Schmidt, David Schnurr, John813', 'Schulman, Daniel Selsam, Kyla Sheppard, Toki814', 'Sherbakov, Jessica Shieh, Sarah Shoker, Pranav815', 'Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,816', 'Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin817', 'Sokolowsky, Yang Song, Natalie Staudacher, Fe-818', 'lipe Petroski Such, Natalie Summers, Ilya Sutskever,819', 'Jie Tang, Nikolas Tezak, Madeleine B. Thompson,820', 'Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,821', 'Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-822', 'lipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,823', 'Chelsea Voss, Carroll Wainwright, Justin Jay Wang,824', 'Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,825', 'C. J. Weinmann, Akila Welihinda, Peter Welin-826', 'der, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave827', 'Willner, Clemens Winter, Samuel Wolrich, Hannah828', 'Wong, Lauren Workman, Sherwin Wu, Jeff Wu,829', 'Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin830', 'Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers,831', 'Chong Zhang, Marvin Zhang, Shengjia Zhao, Tian-832', 'hao Zheng, Juntang Zhuang, William Zhuk, and Bar-833', 'ret Zoph. 2024. GPT-4 Technical Report.834', 'Jie Ou, Yueming Chen, and Prof. Tian. 2024. Lossless835', 'Acceleration of Large Language Model via Adap-836', 'tive N-gram Parallel Decoding. In Proceedings of837', 'the 2024 Conference of the North American Chap-838', 'ter of the Association for Computational Linguistics:839', 'Human Language Technologies (Volume 6: Industry840', 'Track), pages 10–22, Mexico City, Mexico. Associa-841', 'tion for Computational Linguistics.842', 'Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun843', 'Sim, and Jae W. Lee. 2024. Any-precision llm: Low-844', 'cost deployment of multiple, different-sized llms. In845', 'Proceedings of the 41st International Conference on846', 'Machine Learning.847', 'Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten848', 'Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,849', 'Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy850', 'Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna851', 'Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron852', 'Grattafiori, Wenhan Xiong, Alexandre Défossez,853', 'Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-854', 'tin, Nicolas Usunier, Thomas Scialom, and Gabriel855', 'Synnaeve. 2024. Code Llama: Open Foundation856', 'Models for Code. _eprint: 2308.12950.857', 'Andrea Santilli, Silvio Severino, Emilian Postolache,858', 'Valentino Maiorca, Michele Mancusi, Riccardo859', 'Marin, and Emanuele Rodola. 2023. Accelerating860', 'transformer inference for translation via parallel de-861', 'coding. In Proceedings of the 61st Annual Meeting of862', 'the Association for Computational Linguistics (Vol-863', 'ume 1: Long Papers), pages 12336–12355, Toronto,864', 'Canada. Association for Computational Linguistics.865', 'Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter.866', '11', '2023a. A simple and effective pruning approach for867', 'large language models. ArXiv, abs/2306.11695.868', 'Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ah-869', 'mad Beirami, Himanshu Jain, and Felix Yu. 2023b.870', 'SpecTr: fast speculative decoding via optimal trans-871', 'port. In Proceedings of the 37th International Con-872', 'ference on Neural Information Processing Systems,873', 'NIPS ’23, Red Hook, NY, USA. Curran Associates874', 'Inc. Event-place: New Orleans, LA, USA.875', 'Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-876', 'bert, Amjad Almahairi, Yasmine Babaei, Nikolay877', 'Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti878', 'Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton879', 'Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,880', 'Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,881', 'Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-882', 'thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan883', 'Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,884', 'Isabel Kloumann, Artem Korenev, Punit Singh Koura,885', 'Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-886', 'ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-887', 'tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-888', 'bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-889', 'stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,890', 'Ruan Silva, Eric Michael Smith, Ranjan Subrama-891', 'nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-892', 'lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,893', 'Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,894', 'Melanie Kambadur, Sharan Narang, Aurelien Ro-895', 'driguez, Robert Stojnic, Sergey Edunov, and Thomas896', 'Scialom. 2023. Llama 2: Open Foundation and Fine-897', 'Tuned Chat Models.898', 'Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi899', 'Chen. 2023. Sheared llama: Accelerating language900', 'model pre-training via structured pruning. ArXiv,901', 'abs/2310.06694.902', 'Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin903', 'Jiang, Linjun Yang, Rangan Majumder, and Furu904', 'Wei. 2023.Inference with Reference: Lossless905', 'Acceleration of Large Language Models. _eprint:906', '2304.04487.907', 'Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris908', 'Papailiopoulos, and Kangwook Lee. 2024. Predictive909', 'Pipelined Decoding: A Compute-Latency Trade-off910', 'for Exact LLM Decoding. Transactions on Machine911', 'Learning Research.912', 'Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen,913', 'Gang Chen, and Sharad Mehrotra. 2024a. Draft&914', 'Verify: Lossless Large Language Model Acceleration915', 'via Self-Speculative Decoding. In Proceedings of the916', '62nd Annual Meeting of the Association for Compu-917', 'tational Linguistics (Volume 1: Long Papers), pages918', '11263–11282, Bangkok, Thailand. Association for919', 'Computational Linguistics.920', 'Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng921', 'Chen, and Jinan Xu. 2024b. Dual-space knowledge922', 'distillation for large language models. In Proceed-923', 'ings of the 2024 Conference on Empirical Methods in924', 'Natural Language Processing, pages 18164–18181,925', 'Miami, Florida, USA. Association for Computational926', 'Linguistics.927', 'Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn928', 'Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy,929', 'Tianqi Chen, and Baris Kasikci. 2024. Atom: Low-930', 'bit quantization for efficient and accurate llm serv-931', 'ing. Proceedings of Machine Learning and Systems,932', '6:196–209.933', 'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan934', 'Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,935', 'Zhuohan Li, Dacheng Li, Eric Xing, and others. 2023.936', 'Judging llm-as-a-judge with mt-bench and chatbot937', 'arena. Advances in Neural Information Processing938', 'Systems, 36:46595–46623.939', 'Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat,940', 'Aditya Krishna Menon, Afshin Rostamizadeh, Sanjiv941', 'Kumar, Jean-François Kagy, and Rishabh Agarwal.942', '2024. DistillSpec: Improving Speculative Decoding943', 'via Knowledge Distillation. In The Twelfth Interna-944', 'tional Conference on Learning Representations.945', 'AMore on Speculative Decoding946', 'Autoregressive decoding (Touvron et al., 2023; Bai947', 'et al., 2023; Jiang et al., 2023; OpenAI et al., 2024),948', 'suffers from inefficiency because it generates text949', 'one token at a time (Figure 5, Left).Specula-950', 'tive decoding (Chen et al., 2023; Leviathan et al.,951', '2023) follows a guess-and-verify paradigm (Figure952', '5, Right). In speculative decoding, a smaller LLM953', '(draft model) (Chen et al., 2023; Leviathan et al.,954', '2023; Miao et al., 2024; Sun et al., 2023b; Zhou955', 'et al., 2024; Cai et al., 2024) or the original LLM956', 'trained in a specialized manner (self-speculative957', 'decoding) (Elhoushi et al., 2024; Liu et al., 2024a;958', 'Yang et al., 2024; Zhang et al., 2024a; Li et al.,959', '2024b) predicts multiple tokens in advance. The960', 'original LLM then verifies these predictions in par-961', 'allel, improving efficiency.962', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'Autoregressive Decoding', '...', 'Speculate (make guesses)', 'Speculative Decoding', 'A', 'B', 'B', 'C', 'C', 'D', 'T', 'Target LLM', 'AAcceptReject', 'Figure 5: Examples of Autoregressive decoding (Left)and Speculative Decoding (Right). While autoregres-sive decoding generates one token per forward step,speculative decoding generates three tokens with oneforward step.', '12', 'LLMs process discrete integer sequences as in-963', 'puts, where each integer represents a token. We de-964', 'fine the input sequence as x = (x1, x2, . . . , xs) ∈965', 'Ns of length s, and denote a slice of length m at966', 'step t as x1:m = (x1, x2, . . . , xm). The output of967', 'an LLM represents the probability distribution over968', 'the next token. The probability of generating the969', 's-th token, conditioned on all preceding tokens, is970', 'given by PM(xs | x1:s−1). The next token xs is971', 'then sampled from this distribution using various972', 'methods (e.g., greedy, top-k, and top-p sampling;973', 'see (Kool et al., 2020; Holtzman et al., 2020)). In974', 'the case of greedy sampling, the next token is se-975', 'lected as xs = arg max PM(xs | x1:s−1)976', 'Let x0 be the prompt tokens provided by the977', 'user. The LLM generates an output sequence of978', 'length m, with each generated token yi computed979', 'autoregressively. Assuming greedy sampling, the980', 'decoding process follows:981', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y1 = arg max PM(y1 | x) y2 = arg max PM(y2 | y1, x)... ym = arg max PM(ym | y1:m−1, x).', '(1)982', 'A.1Speculative Decoding983', 'Speculative decoding follows a Guess-And-Verify984', 'approach, where multiple candidate future to-985', 'kens are speculated and subsequently verified986', 'in a single decoding step.With tree attention987', '(Miao et al., 2024), multiple drafts can be ver-988', 'ified simultaneously. Let G denote the number989', 'of guesses, and define the set of guesses as ˜Y =990', '{˜y(0), ˜y(1), . . . , ˜y(G)}, where each guess sequence991', 'has length K. The j-th token of the i-th guess is992', 'denoted as ˜y(i) j .993', 'In the case of speculative decoding with greedy994', 'sampling, given the prompt x, a drafting method995', 'is used to generate the draft sequences ˜Y . Using996', 'these drafts, the LLM then computes the true tokens997', '(y′1, y′2, . . . , y′K) in parallel. For instance, for the998', 'guess sequence ˜y(0), the true tokens are determined999', 'as:1000', '\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2', '\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3', 'y′1 = arg max PM(y1 | x)', 'y′2 = arg max PM(y2 | ˜y(0) 1 , x)...', 'y′K = arg max PM(yK | ˜y(0) 1:K−1, x).', '(2)1001', 'These generated tokens are then verified. Let h1002', 'be the highest number of correct guessed tokens1003', 'across all guesses. Consequently, h + 1 tokens are1004', 'generated in one forward step. Algorithm 2 out-1005', 'lines speculative decoding with greedy sampling.1006', 'BImplementation Details1007', 'B.1Frameworks and Libraries1008', 'We implement SPECTRA in Python using PyTorch1009', '2.1.0 and the Hugging Face transformers library.1010', 'For large-scale model loading (e.g., LLaMA-2-1011', '70B, LLaMA-3-70B), we employ 16-bit (FP16)1012', 'precision with a pre-allocated key-value cache.1013', 'B.2Models and Checkpoints1014', 'We run our experiments primarily with:1015', '• LLaMA-2-Chat (Touvron et al., 2023) in1016', 'sizes 7B, 13B, 70B.1017', '• CodeLlama (Rozière et al., 2024) in sizes 7B1018', 'and 13B.1019', '• LLaMA-3-Instruct (Dubey et al., 2024) in1020', 'sizes 8B and 70B.1021', 'All checkpoints are obtained from the official or1022', 'Hugging Face repositories without fine-tuning or1023', 'modification. For each model, we enable half-1024', 'precision inference. We also verify numerically (by1025', 'comparing 32-bit and 16-bit outputs) that specula-1026', 'tive decoding preserves exact or near-exact token1027', 'sequences within typical floating-point tolerances.1028', 'B.3Hardware1029', 'Most experiments are conducted on a single1030', 'NVIDIA A100 GPU with 80GB of memory. We1031', 'also evaluate on other NVIDIA GPUs (RTX 3090,1032', 'RTX 8000, A40, A6000) to study hardware-1033', 'specific scaling. For the largest checkpoints (70B)1034', 'that do not fit on a single GPU under certain con-1035', 'figurations, we optionally distribute them across1036', 'multiple GPUs (2x, 4x, or 8x H100) using standard1037', 'pipeline-parallelism from Hugging Face’s library.1038', 'B.4Hyperparameters1039', 'Lookahead, REST, and ANPD.We replicate1040', 'each baseline using their publicly available GitHub1041', 'code, keeping to the default settings and hyperpa-1042', 'rameters outlined in the original papers.1043', 'Spectra.By default, we use a 5-gram setup for1044', 'our forward/backward dictionaries, storing all sub-1045', 'sequences (i.e., sub-ngrams). We also maintain a1046', 'candidate pool of size W = 15 per key to generate1047', '13', 'new n-gram records; after each forward pass, can-1048', 'didate sequences are shifted by one token and then1049', 're-populated. We introduce a threshold τ ∈[0, 1],1050', 'default set to 0.1, to decide when to force the se-1051', 'lection of a token not yet in the forward dictionary.1052', 'This mechanism balances coverage of unseen pre-1053', 'fixes with reinforcing common contexts. At every1054', 'speculative decoding step, we allow up to G = 601055', 'guess tokens. Internal guesses receive priority, and1056', 'if there is still capacity under the guess limit, we1057', 'add external guesses.1058', 'For external lookups, we implement a trie struc-1059', 'ture for rapid prefix queries, following a design1060', 'similar to REST (He et al., 2024). For conver-1061', 'sation tasks (e.g., MT-Bench), we gather approxi-1062', 'mately 100k examples from the UltraChat dataset1063', '(Ding et al., 2023), focusing on those with minimal1064', 'perplexity under the same LLM we aim to accel-1065', 'erate. For code tasks (e.g., HumanEval, MBPP),1066', 'we draw from TheStack (Kocetkov et al., 2023)1067', 'and again refine it to the 100k snippets with the1068', 'lowest perplexity for memory efficiency. We mea-1069', 'sure perplexity by running a single forward pass (in1070', 'streaming mode) over candidate samples and rank-1071', 'ing them. Despite being relatively small (100k),1072', 'this curated corpus achieves robust guess quality.1073', 'All speedup and throughput metrics are com-1074', 'puted at a batch size of 1. In code generation tasks,1075', 'the maximum generation length is typically 5121076', 'tokens, whereas for conversation tasks (MT-Bench,1077', 'GSM8K), we allow up to 1024 tokens or stop early1078', 'if the model outputs an end-of-sequence token. All1079', 'random seeds are set to 0.1080', 'CDetails Results with Throughputs1081', 'We provide a detailed throughput analysis to com-1082', 'plement the speedup ratios reported in the main1083', 'text. Our goal is to demonstrate how SPECTRA1084', 'scales across various model sizes, datasets, and1085', 'GPU architectures. We measure throughput using1086', 'two key metrics:1087', '• Macro Throughput (Mac-TP). Calculated1088', 'as the average of per-generation token-1089', 'processing rates—i.e., for each generation1090', 'step i, we compute tokeni/timei and then1091', 'average over all steps.1092', '• Micro Throughput (Mic-TP). Calculated as1093', 'the total number of generated tokens divided1094', 'by the total elapsed time1095', 'Table 4 focuses on GSM8K and MTBench per-1096', 'formance across four different GPU models, while1097', 'Table 3 provides more granular results on ad-1098', 'ditional datasets and model configurations.In1099', 'all cases, SPECTRA consistently achieves higher1100', 'throughput than both non-speculative baselines and1101', 'other training-free accelerators, as evidenced by im-1102', 'provements in both Mic-TP and Mac-TP. Notably,1103', 'this performance advantage remains stable even on1104', 'older GPUs (e.g., the RTX 3090 and RTX 8000),1105', 'demonstrating SPECTRA’s robustness to varying1106', 'hardware capabilities.1107', 'DEvaluating SPECTRA in Different GPU1108', 'Types1109', 'Different GPU types.Table 5 reports speedups1110', 'on GSM8K and MTBench across four GPUs with1111', 'varying memory throughput and compute capabili-1112', 'ties. While absolute wall-clock times differ across1113', 'GPUs, the relative accelerations remain consistent.1114', 'SPECTRA consistently outperforms other baselines,1115', 'including Lookahead, achieving higher speedups in1116', 'all cases. On older GPUs (e.g., RTX 3090 or RTX1117', '8000), the gap between Lookahead and SPECTRA1118', 'narrows slightly due to less efficient parallelism,1119', 'but SPECTRA maintains its lead. These results1120', 'demonstrate that SPECTRA is robust to hardware1121', 'variations and effective across both data-center and1122', 'consumer-grade GPUs.1123', 'EEvaluating SPECTRA in Multi-GPU1124', 'Environments1125', 'A critical consideration for practical deployment is1126', 'how SPECTRA scales when models are distributed1127', 'across multiple GPUs—a common requirement for1128', 'large LLMs exceeding single-device memory ca-1129', 'pacity. To evaluate this, we measure SPECTRA’s1130', 'performance under three distributed configurations1131', 'of LLaMA-2-70B: (1) 2xH100 with full precision,1132', '(2) 4xH100 with full precision, and (3) 8xH1001133', 'with full precision. We also include a baseline1134', 'of 1xH100 with 8-bit quantization for memory-1135', 'constrained single-GPU inference. Table 6 reports1136', 'throughput and speedup metrics.1137', 'SPECTRA achieves consistent speedups of 2.00—1138', '2.03× across all multi-GPU configurations while1139', 'maintaining a stable compression ratio (τ) of 2.52.1140', 'This demonstrates robust scalability—partitioning1141', 'model weights introduces minimal overhead, and1142', 'the speculative verification process remains effi-1143', 'cient despite inter-GPU communication. Notably,1144', '14', 'ClassevalGSM8KHumanevalMBPPMTBenchModelMethodMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TPMac-TPMic-TP', 'Greedy (temperature=0)', 'CL-13BAutoregressive30.8530.8532.0332.0332.3532.3532.0732.0730.6930.63 CL-13BANPD59.7758.0389.9989.1867.4364.6586.7686.4180.1076.68 CL-13BLookahead69.2868.6289.7389.0074.3373.2393.3892.8079.3878.67 CL-13BREST39.5337.7329.9329.4751.1547.4927.4127.3928.9227.18 CL-13BSPECTRA (Ours)73.4772.9893.3693.2384.9184.41105.44105.3981.3280.68', 'CL-7BAutoregressive41.1741.1741.1741.1741.4141.4141.6041.6038.9138.93 CL-7BANPD94.7693.02132.26131.3089.2687.13131.35130.99130.41126.64 CL-7BLookahead106.51105.95123.04121.90103.45103.51120.75120.23125.58124.77 CL-7BREST59.4956.6137.6137.2170.3865.2240.1140.0939.6436.70 CL-7BSPECTRA (Ours)111.09110.68137.24136.86122.54122.41148.32148.07143.98144.32', 'L2-13BAutoregressive31.8531.5632.4032.4332.2732.2732.1932.1931.9331.78 L2-13BANPD43.3044.4447.5445.2243.2442.2836.2035.8437.4434.84 L2-13BLookahead57.4958.9447.4447.6255.7655.5844.4144.1548.1146.62 L2-13BREST38.8137.7430.3630.2240.4739.7030.7030.6736.3937.02 L2-13BSPECTRA (Ours)63.6464.3159.2158.6363.3963.1852.4352.1956.0453.75', 'L2-70BAutoregressive2.602.602.612.612.612.612.632.632.602.60 L2-70BANPD4.724.804.254.104.854.763.073.073.473.30 L2-70BLookahead6.907.164.875.126.716.733.923.935.055.02 L2-70BSPECTRA (Ours)8.078.356.586.758.418.414.884.886.326.22', 'L2-7BAutoregressive40.3340.3241.0141.0341.1441.1341.0041.0440.4840.50 L2-7BANPD65.5468.1062.4059.3863.2759.9848.9447.6752.4750.06 L2-7BLookahead88.4191.0568.0068.2084.6983.8759.7960.7670.0469.07 L2-7BREST54.7453.9341.4341.3857.9956.4141.2840.7450.5851.79 L2-7BSPECTRA (Ours)96.8898.7586.5185.5098.7798.3872.3973.2281.9379.20', 'L3-70BAutoregressive2.582.572.582.582.592.592.592.592.552.55 L3-70BANPD3.974.193.863.724.724.753.773.593.143.03 L3-70BLookahead6.176.473.993.966.636.753.703.664.494.53 L3-70BSPECTRA (Ours)6.877.185.435.347.337.505.014.885.255.16', 'L3-8BAutoregressive36.5936.5836.7436.7436.2036.2135.2435.2036.5536.69 L3-8BANPD77.2178.76141.89141.3666.3165.57118.47112.9541.7740.20 L3-8BLookahead94.9297.09136.32135.9289.9990.47133.67133.1256.0955.49 L3-8BSPECTRA (Ours)103.61105.88142.89142.7292.8693.16143.80142.7261.6960.22', 'Sampling (temperature=1.0)', 'CL-13BAutoregressive30.9030.6431.3831.3731.2431.3931.4631.4530.7130.67 CL-13BANPD35.4834.8633.5432.3432.6434.3631.5730.9570.9265.68 CL-13BLookahead42.5440.7433.7932.4940.2542.1732.0231.1971.5068.46 CL-13BREST35.1533.2225.6725.2439.5838.4926.4325.8928.4126.69 CL-13BSPECTRA (Ours)51.8650.0437.5735.6751.6052.6436.2935.2772.9069.98', 'CL-7BAutoregressive39.6039.5840.8540.8740.0540.1040.8140.8140.4940.50 CL-7BANPD50.8951.7647.4446.6844.1446.3445.8645.81112.29103.57 CL-7BLookahead60.8760.2948.5447.6457.1261.1448.6448.27110.07105.00 CL-7BREST48.6446.4135.9835.4653.3552.2637.0436.5739.3636.51 CL-7BSPECTRA (Ours)71.7071.7855.2452.8167.2769.2054.4852.91112.43108.49', 'L2-13BAutoregressive31.2331.1731.4431.4731.4131.4232.0232.0631.6731.59 L2-13BANPD37.5337.9439.1137.9936.7936.7532.9732.7136.9134.34 L2-13BLookahead47.5947.3541.6041.7646.3346.5137.8237.8247.3545.48 L2-13BREST36.7836.1729.3329.2537.4636.7129.3829.2835.5036.21 L2-13BSPECTRA (Ours)53.1352.2848.6048.1152.9353.1142.9543.0354.9852.42', 'L2-7BAutoregressive39.8939.8840.5840.5940.0940.1040.5940.6640.6540.70 L2-7BANPD52.1452.7854.2352.9051.4050.9744.7343.7750.9248.24 L2-7BLookahead70.8271.1761.1561.3468.7869.0150.8451.8368.2766.77 L2-7BREST50.3549.9940.1940.0950.8650.0638.9438.1849.1250.54 L2-7BSPECTRA (Ours)78.4678.7472.1371.6881.7181.7659.7760.0980.2177.00', 'L3-8BAutoregressive35.7535.7635.1635.1736.0136.0236.0536.0735.3935.48 L3-8BANPD44.7143.7269.1266.7351.4851.5768.0364.5440.8439.23 L3-8BLookahead53.0550.5772.6869.1164.5963.7971.8868.9055.4653.74 L3-8BSPECTRA (Ours)69.5068.9279.8876.5369.0968.6278.9976.6960.3357.69', 'Table 3: Micro throughput (Mic-TP) and Macro throughput (Mac-TP) across multiple tasks and models.', '15', 'GPUMethodGSM8KMTBenchMac-TPMic-TPMac-TPMic-TP', 'A40Autoregressive32.6632.6632.1431.66 Lookahead48.5948.7349.1347.96 SPECTRA62.5661.5259.0056.80', 'A6000Autoregressive39.1539.1738.7838.24 Lookahead58.1358.3058.8457.40 SPECTRA75.2074.1671.369.28', 'RTX8000Autoregressive34.0334.2734.2134.02 Lookahead45.2545.4245.7344.16 SPECTRA57.9557.0954.1652.32', 'RTX3090Autoregressive40.6740.7641.1741.22 Lookahead53.6953.7553.5152.09 SPECTRA74.8773.8871.5869.79', 'Table 4: Throughput results for different GPU types on GSM8K and MTBench.', 'GPUMethodGSM8KMTBenchspeedupτspeedupτ', 'A40Lookahead1.491.931.532.07 SPECTRA1.922.461.842.36', 'A6000Lookahead1.481.921.522.06 SPECTRA1.922.461.842.36', 'RTX8000Lookahead1.331.931.342.08 SPECTRA1.702.461.582.35', 'RTX3090Lookahead1.321.921.302.06 SPECTRA1.842.461.742.36', 'Table 5: Hardware scalability of SPECTRA decoding onGSM8K and MTBench for various GPU architectures.', 'even in the quantized single-GPU setting, SPEC-1145', 'TRA provides a 2.43× speedup, outperforming1146', 'standard autoregressive decoding. These results1147', 'validate SPECTRA’s practicality for large-scale de-1148', 'ployments where memory constraints necessitate1149', 'distributed inference.1150', 'FVerifying Generation Quality with1151', 'SPECTRA Decoding1152', 'Greedy Decoding Performance.To assess the1153', 'quality of greedy decoding, we compare the in-1154', 'ference results of the LLaMA-2-7B Chat model1155', 'using SPECTRA Decoding against Hugging Face’s1156', 'standard greedy search. Our baseline consists of1157', 'single-precision (FP32) inference on 160 conver-1158', 'sational turns from the MT-Bench dataset. Under1159', 'FP32, SPECTRA Decoding produces identical out-1160', 'puts to the baseline.1161', 'However, when transitioning to half-precision1162', '(FP16), even Hugging Face’s native greedy search1163', 'generates 25 discrepancies (out of 160) compared1164', 'to the FP32 baseline. SPECTRA Decoding exhibits1165', 'a similar discrepancy rate (26), confirming that it1166', 'maintains the output distribution within the numer-1167', 'ical error margins typically observed in standard1168', 'half-precision inference libraries.1169', 'Sampling Decoding Performance.We also as-1170', 'sess generation quality under a stochastic sam-1171', 'pling setting (temperature = 1.0). As detailed in1172', 'Table 7, SPECTRA Decoding produces ROUGE-1173', '1, ROUGE-2, and ROUGE-L scores on both the1174', 'CNN/DailyMail (Nallapati et al., 2016) and XSum1175', '(Narayan et al., 2018) summarization datasets1176', 'that are nearly identical to those of standard1177', 'autoregressive sampling.At the same time,1178', 'SPECTRA achieves notable speedups (1.60× on1179', 'CNN/DailyMail and 1.69× on XSum) with com-1180', 'pression ratios of 2.05 and 2.08, respectively.1181', 'These results confirm that SPECTRA Decoding1182', 'accelerates inference while preserving generation1183', 'quality across diverse tasks.1184', 'These findings reaffirm that SPECTRA Decoding,1185', 'does not degrade generation quality compared to1186', 'conventional greedy or sampling-based methods.1187', 'GToken Acceptance Rate Analysis1188', 'Figure 6 plots the cumulative number of accepted1189', 'tokens versus decoding steps for each dataset (MT-1190', 'Bench, HumanEval, MBPP, and GSM8K). The1191', 'steeper ascent of the SPECTRA curve indicates that1192', 'our method requires substantially fewer decoding1193', 'steps compared to alternatives, for example, almost1194', 'two times shorter than ANPD. This improvement is1195', '16', 'GPU & Model SettingMethodMTBenchMac-TPMic-TPSpeedupτ', '1xH100 - Quantized Int8Autoregressive2.602.601.001.00 SPECTRA6.326.222.432.51', '2xH100 - FP16Autoregressive14.8114.701.001.00 SPECTRA29.6228.912.002.52', '4xH100 - FP16Autoregressive14.6014.481.001.00 SPECTRA29.6728.892.032.52', '8xH100 - FP16Autoregressive14.3914.281.001.00 SPECTRA29.2728.552.032.52', 'Table 6: Results in multi-GPU Enviroments on GSM8K and MTBench using LLama-2-chat-70B.', 'DatasetMethodROUGE-1ROUGE-2ROUGE-LSpeedupτ', 'CNNAutoregressive9.770.397.201.001.00 SPECTRA9.740.417.181.602.05', 'XSUMAutoregressive18.124.3612.431.001.00 SPECTRA18.134.4012.491.692.08', 'Table 7: Evaluation of SPECTRA Decoding on CNN/DailyMail and XSum using a temperature of 1.0. ROUGEscores, speedups over autoregressive decoding, and compression ratio (τ) are reported for LLaMA-2-7B-Chat.', 'attributed to a higher token acceptance rate, which1196', 'in turn reduces the overall number of decoding iter-1197', 'ations and enhances the efficiency of the generation1198', 'process.1199', 'HAlgorithms1200', '17', '0250500750', '0', '10', '20', '30', '40', 'MT-Bench', '0200400', '0', '10', '20', '30', '40', 'HumanEval', '050100150', '0', '5', '10', '15', 'MBPP', '0100200', '0', '10', '20', '30', '40', '50', '60', 'GSM8K', '#Accepted tokens (in thousands)', 'LookaheadRESTANPDSpectra (Ours)', 'Figure 6: Total number of accepted tokens across all samples at each decoding step.', 'Algorithm 2 Speculative Decoding (Multiple guesses and Greedy Sampling)', 'Given guess size K, number of guesses G, and target length T. Given initial prompt sequence x. while n < T do', 'Obtain multiple drafts ˜Y = {˜y(0), ˜y(1), . . . , ˜y(G)}.', 'In parallel, compute K + 1 verification tokens y′: for i = 1 : K do', 'y′(g) i= arg max PM(yi | ˜y(g) i−1, x),∀g ∈{0, . . . , G}', 'end forIdentify the sequence ˜y(g∗) with the highest token matches and the corresponding y′(g).', 'for t = 1 : K do', 'if y′(g) t= ˜y(g∗) tthen', 'Set yn+t ←˜y(g∗) tand n ←n + 1.', 'else', 'yn+t ←y′(g) tand exit for loop.', 'end if', 'end for', 'end while', '18', 'Algorithm 3 Greedy Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function GREEDYVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x and all gi2’s outputs – totally N distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:P ←D[l][i]', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:if sj = arg max P then▷accepted, update all potential speculations and probabilities', '14:o.append(sj)', '15:is_accept ←1', '16:Vnew, Dnew ←∅, ∅', '17:for k = j to size(V ) do', '18:if sj = V [k] then', '19:Vnew.append(V [k])', '20:Dnew.append(D[k])', '21:end if', '22:end for', '23:V, D ←Vnew, Dnew', '24:break', '25:else▷rejected, go to next speculation', '26:j ←j + 1', '27:end if', '28:end while', '29:if is_accept then', '30:continue', '31:else▷guarantee one step movement', '32:o.append(arg max P)', '33:break', '34:end if', '35:end for', '36:if is_accept then', '37:o.append(arg max D[1]N)', '38:end if', '39:return o', '40: end function', '19', 'Algorithm 4 Sample Verification with SPECTRA DECODING', 'Require: sequence x, model PM, guesses gi with i ∈[1, G] Ensure: o {accepted tokens of length 1 to N}', '1: function SAMPLEVERIFICATION(x, PM, g)', '2:V, D, o ←∅, ∅, ∅', '3:for i = 1 to G do', '4:V.append(gi2)▷each gi2 is a n-1 gram', '5:D.append(PM(gi2, xnext|gi2, x))▷obtain last token of x0 and all gi2’s outputs – totally N probability distributions', '6:end for', '7:for i = 1 to N −1 do', '8:j ←1', '9:is_accept ←0', '10:Pj ←D[1]i', '11:while j ≤size(V ) do', '12:sj ←V [j]', '13:sample r ∼U(0, 1)', '14:if r ≤Pj(sj) then▷accepted, update all potential speculations and probabilities', '15:o.append(sj)', '16:is_accept ←1', '17:Vnew, Dnew ←∅, ∅', '18:for k = j to size(V ) do', '19:if sj = V [k] then', '20:Vnew.append(V [k])', '21:Dnew.append(D[k])', '22:end if', '23:end for', '24:V, D ←Vnew, Dnew', '25:break', '26:else▷rejected, go to next speculation', '27:Pj(sj) ←0', '28:Pj+1 = norm(Pj)', '29:j ←j + 1', '30:end if', '31:end while', '32:if is_accept then', '33:continue', '34:else▷guarantee one step movement', '35:sample xnext ∼Pj', '36:o.append(xnext)', '37:break', '38:end if', '39:end for', '40:if is_accept then', '41:o.append(sample xnext ∼D[1]N)', '42:end if', '43:return o', '44: end function', '20']
2025-02-19 13:10:11 - WARNING: done parsing pdf
2025-02-19 13:10:11 - WARNING: start ner pdf
2025-02-19 13:10:15 - INFO: Loading Data
2025-02-19 13:10:15 - WARNING: 2025-02-19 13:10:15 - INFO: Loading Data
2025-02-19 13:10:18 - WARNING: Predicting NER ...
2025-02-19 13:10:24 - WARNING: Finished predicting.
2025-02-19 13:10:24 - WARNING: Converting to Brat format...
2025-02-19 13:10:24 - WARNING: # of discontinuous mentions:
2025-02-19 13:10:24 - WARNING:  
2025-02-19 13:10:24 - WARNING: 0
2025-02-19 13:10:24 - WARNING: Finished.
2025-02-19 13:10:24 - WARNING: start predict re
2025-02-19 13:10:27 - WARNING: Example:   0%|          | 0/40 [00:00<?, ?it/s]
2025-02-19 13:10:27 - WARNING: Example: 100%|##########| 40/40 [00:00<00:00, 1799.13it/s]
2025-02-19 13:10:27 - WARNING: # of documents 40.
2025-02-19 13:10:27 - WARNING: # of positive examples 0.
2025-02-19 13:10:27 - WARNING: # of negative examples 106.
2025-02-19 13:10:28 - WARNING: dict_keys(['483', '513', '516', '1330', '1426'])
2025-02-19 13:10:28 - WARNING: done predict re
2025-02-19 13:10:28 - WARNING: model output text 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 13:10:28 - WARNING: len of model_output_text 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: 51
2025-02-19 13:10:28 - WARNING: original_text 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: and time-consuming. Speculative decoding has003
2025-02-19 13:10:28 - WARNING: mapping dict 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 7, 9: 8, 10: 8, 11: 9, 12: 10, 13: 11, 14: 12, 15: 13, 16: 14, 17: 15, 18: 16, 19: 17, 20: 17, 21: 18, 22: 19, 23: 20, 24: 21, 25: 22, 26: 23, 27: 24, 28: 25, 29: 26, 30: 27, 31: 28, 32: 29, 33: 30, 34: 31, 35: 32, 36: 33, 37: 34, 38: 35, 39: 36, 40: 37, 41: 38, 42: 39, 43: 40, 44: 41, 45: 42, 46: 43, 47: 44, 48: 45, 49: 46, 50: 46}
2025-02-19 13:10:28 - WARNING: text: 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: and time - consuming . Speculative decoding has003 
2025-02-19 13:10:28 - WARNING: origin bbox 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 13:10:28 - WARNING: normalized bbox 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 13:10:28 - WARNING: finalized bbox 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: {'x1': 21.07975196838379, 'y1': 270.7753601074219, 'x2': 25.511127471923828, 'y2': 280.7857971191406, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 13:10:28 - WARNING:  final bouding box 
2025-02-19 13:10:28 - WARNING:  
2025-02-19 13:10:28 - WARNING: {'x1': 12.217000007629395, 'y1': 269.133544921875, 'x2': 272.1279296875, 'y2': 281.13848876953125, 'width': 595.2760009765625, 'height': 841.8900146484375, 'pageNumber': 1}
2025-02-19 13:10:28 - WARNING: done save re and ner, 
2025-02-19 13:10:28 - WARNING: start count num_rel 
2025-02-19 13:10:28 - WARNING: done count num_rel 
2025-02-19 13:10:28 - WARNING: start update document 
2025-02-19 13:10:29 - WARNING: start matching infor
2025-02-19 13:10:29 - WARNING: done matching infor
2025-02-19 13:10:29 - WARNING: start commit
2025-02-19 13:10:30 - WARNING: done update document 
2025-02-19 13:10:32 - WARNING: Failed to send email. Error: {'an': (553, b'5.1.3 The recipient address <an> is not a valid RFC 5321 address. For more\n5.1.3 information, go to\n5.1.3  https://support.google.com/a/answer/3221692 and review RFC 5321\n5.1.3 specifications. d9443c01a7336-220d5366834sm96101305ad.56 - gsmtp')}
2025-02-19 13:10:33 - INFO: Task dev_tasks.process_pdf_task[2e3ecd39-f3f8-448c-aefb-d89630c62119] succeeded in 22.419604685157537s: {'id': 85825206, 'filename': '_ACL_2025__LLM_Efficiency (2) (1).pdf', 'upload_time': '2025/02/19, 04:10:10', 'entities': 232, 'relations': 6, 'pages': 20, 'status': 'completed'}
2025-02-19 13:10:33 - WARNING: 2025-02-19 13:10:33 - INFO: Task dev_tasks.process_pdf_task[2e3ecd39-f3f8-448c-aefb-d89630c62119] succeeded in 22.419604685157537s: {'id': 85825206, 'filename': '_ACL_2025__LLM_Efficiency (2) (1).pdf', 'upload_time': '2025/02/19, 04:10:10', 'entities': 232, 'relations': 6, 'pages': 20, 'status': 'completed'}
2025-02-19 15:26:27 - INFO: Task dev_tasks.process_pdf_task[bc7b0d06-70c6-4fd5-b43c-2296500c3f5a] received
2025-02-19 15:26:27 - WARNING: 2025-02-19 15:26:27 - INFO: Task dev_tasks.process_pdf_task[bc7b0d06-70c6-4fd5-b43c-2296500c3f5a] received
2025-02-19 15:26:27 - WARNING: uploads/yoonessi2011.pdf
2025-02-19 15:26:27 - WARNING: start parsing pdf
2025-02-19 15:26:27 - WARNING: parsed 73 paragraphs
2025-02-19 15:26:27 - WARNING: ['Morphology of sulfonated polyarylenethioethersulfone random copolymer series as proton exchange fuel cells membranes by small angle neutron scattering', 'Mitra Yoonessi a,b,*, Hendrik Heinz c, Thuy D. Dang d, Zongwu Bai e', 'a Ohio Aerospace Institute, Cleveland, OH 44142, USAb NASA Glenn Research Center, Cleveland, OH 44135, USAc Department of Polymer Engineering, University of Akron, Akron, OH 44325, USAd Air Force Research Laboratory, AFRL/RXBN, Wright-Patterson AFB, OH 45433, USAe University of Dayton Research Institute, 300 College Park Drive, Dayton, OH 45469, USA', 'a r t i c l e i n f o', 'Article history: Received 27 June 2011 Received in revised form 23 September 2011 Accepted 28 September 2011 Available online 4 October 2011', 'Keywords: Fuel cells membrane Morphology Neutron scattering', 'a b s t r a c t', 'Sulfonated polyarylenethioethersulfone (SPTES) copolymers with high proton conductivity (100 e215 mS/cm at 65 �C, 85% relative humidity) are promising potential proton exchange membrane (PEM) for fuel cells. Small angle neutron scattering (SANS) of the hydrated SPTES copolymer membranes at 25�C exhibit a nanostructure which can be approximated by correlated polydisperse spherical aggregates containing water molecules with liquid-like ordering (Percus Yevick approximation) and large scale water pockets. The ionic domain radius and the volume packing density of the aggregates present in the hydrated SPTES copolymer membranes at 25 �C increased with increasing degree of sulfonation. SPTES-80 with highest degree of sulfonation (71.6%) showed a Guinier plateau at the very low q range (q < 1 �10�4 1/Å) indicating presence of isolated large scale morphology (Rg ¼ 1.3 �0.18 micron). The radius of spherical ionic aggregates present in the hydrated SPTES-50 and SPTES-60 copolymer membranes increased with increasing temperature to 55 �C, but the large scale morphology changed to a fractal network. Further increase of the sulfonation degree to 63.3% and 71.6% (SPTES-70 and SPTES-80) resulted in a substantial morphology change of the spherical aggregates to an irregular bicontinuous hydrophobic/hydrophilic morphology for the hydrated SPTES-70 and SPTES-80 copolymer membranes at 55 �C. Presence of ionic maxima followed by a power law decay of �4 for SPTES-70 and SPTES-80 copolymer membranes was attributed to the bicontinuous phase morphology at high degree of sulfonation and elevated temperature (55 �C). The disruption of the larger scale fractal morphology was characterized by significant decrease in the intermediate scattering intensity. Hydrophobic and hydrophilic domains were separated distinctly by sulfonic groups at the interface showing as power law decay of �4 for all hydrated SPTES copolymers. �2011 Elsevier Ltd. All rights reserved.', '1. Introduction', 'Ionomers are important class of polymeric materials with ionizable groups on the polymer backbone or in the pendant which can phase separate to hydrophobic and hydrophilic domains [1,2]. Ionomers with ionizable acidic groups have potential application as polyelectrolyte membranes in fuel cells. Hydrogen fuel cell is an electrochemical reactor in which the proton transport from anode to cathode leads to a reaction at the cathode catalyst interface [3e5]. Therefore, transport of protons and hydronium ions through proton', 'exchange membrane (PEM) is the key factor on the performance of a hydrogen fuel cell. High proton conductivity, impermeability to reactant gases, high thermal and mechanical stability both in the dry and hydrated states, water uptake, dimensional stability, and low cost are fundamental characteristics of PEM for hydrogen fuel cells. The structure, dynamics, and transport characteristics of Nafion�as commercially utilized PEM have been studied by small angle neutron scattering (SANS) [6e11], small angle x-ray scattering (SAXS) [12e16], quasi-elastic neutron scattering (QENS) [17], and nuclear magnetic resonance spectroscopy (NMR) [18]. Transport properties and nanostructure of sulfonated polyimide (SPI) membranes have been studied using pulsed field gradient NMR and NMR quadrupolar relaxation rates determinations [19,20], and small angle scattering methods (SAXS and SANS), respectively [21,22].', '* Corresponding author. Ohio Aerospace Institute, Cleveland, OH 44135, USA. Tel./ fax: þ1 9376265333. E-mail address: mitra.yoonessi@nasa.gov (M. Yoonessi).', 'Contents lists available at SciVerse ScienceDirect', 'Polymer', 'journal homepage: www.elsevier.com/locate/polymer', '0032-3861/$ e see front matter �2011 Elsevier Ltd. All rights reserved. doi:10.1016/j.polymer.2011.09.047', 'Polymer 52 (2011) 5615e5621', 'The microstructure of sulfonated polyetherether ketone (sPEEK) has been investigated by SAXS [12]. We recently reported a class of ionomers based on aromatic hydrocarbon copolymers with high proton conductivity and excellent thermal mechanical stability both in the dry and hydrated states [22e29]. Sulfonated poly-arylenethioethersulfone(SPTES)copolymershavefollowing chemical structures (Fig. 1). SPTES copolymers including SPTES-50, SPTES-60, SPTES-70 and SPTES-80, have equivalent weight (EW) and IEC (mequiv./g) values of 610, 515, 459, 417, and 1.64, 1.94, 2.18, and 2.4, respectively [22]. They exhibited proton conductivity of 100, 145, 175, and 215 mS/cm respectively, at 65 �C and 85% relative humidity [22]. Their excellent proton conductivity at high temperatures combined with their high glass transition temperature (w200 �C) and mechanical stability (both in the dry and hydrated states) make them excellent potentials as high temperature PEM materials for fuel cells. SPTES-50 copolymer membrane has successfully been fabricated to membrane electrode assemblies and exhibited polarization curves and durability up to 400 h [29]. Their successful operations at 90e100 �C were limited by boiling point of water (100 �C at 1 atm). Replacing water molecules with heterocycles such as imidazolium where the charge carrier has a very low vapor pressure can result in proton conductivity at higher temperatures. Despite excellent electrochemical properties, excessive water uptake of SPTES-70 and SPTES-80 copolymer membranes provide difficulties to be made as membrane electrode assembly [22,27] The proton transport and performance of SPTES copolymers highly depend on the presence of water molecules. In addition to the number of sulfonic groups, their acidity (pKa) ability to dissociate water molecules to proton, water activity coefficient, and number of water molecules associated with each sulfonic group, the supermolecular structure of the hydro-phobic and hydrophilic phases is also defined by polymer chain characteristics such as chain persistent length, and presence of sulfonic group on the backbone or in the side chains. We reported the presence of ionic nanodomains containing water molecules in the SPTES-70 using in-situ x-ray scattering [27]. The morphology and the nanostructure of SPTES-50 were approximated by corre-lated polydisperse spherical aggregates and a larger scale water domain network and were quantified by modeling of the SANS spectra with polydisperse hard sphere model with Percus Yevick liquid-like ordering [29]. This study reports the nanostructure and morphology of sulfonated polyarylenethioethersulfone (SPTES) copolymer membranes which is directly related to the proton transport through the membrane in terms of their degree of sulfonation and temperature dependency.', '2. Materials and methods', '2.1. Materials', 'Visually observed defect free films of SPTES copolymers were prepared by dissolving the purified copolymer in dimethyl acet-amide (DMAc, Sigma Aldrich) (5e10 wt%) filtering, placing in a flat dish in a vacuum oven with a gradual temperature rise to 100 �C for 24 h and 120 �C for 2 h. The resulting uniform flat films were immersed for 2 h in deionized water and dried under vacuum (24 h, 80 �C) after they were acidified in sulfuric acid (4 M, 24 h) to ensure complete conversion of sulfonic groups to their protonated forms.', '2.2. Characterization', 'SANS experiments were performed at the National Institute of Standards and Technology (NIST), Neutron Center for Research using 30 m NG-7 SANS instrument with a neutron wavelength, l, of 6 Å (Dl/l ¼ 10%) and three sample-to-detector distance of 1.5 m, 10 m (l ¼ 6 Å), and 15 m (l ¼ 8 Å), 0.001 < q < 0.3168 Å�1 at 25, and 55 �C (accuracy of 0.5 �C). Hydrated membranes were placed in demountable 1 mm thick titanium liquid cells filled with D2O after equilibrium in D2O (24 h). Scattered intensities were reduced, corrected for the transmission and background and placed on absolute scale. Then, circularly averaged to produce absolute scale scattering intensity, I(q), as a function of the wave vector, q, where q ¼ (4p/l)sin(q/2) and q is the scattering angle. Calculations were performed using Igor Pro�software [30,31]. USANS experiments covered a q-range of 0.00005 < q (Å�1) < 0.01, corresponding to a real-space length scale of 0.1 microne10 micron.', '3. Results and discussions', '3.1. Hydrated SPTES copolymers at 25 �C', 'SPTES copolymers have high proton conductivity at high relative humidities and elevated temperatures; e.g. 65 �C and 85% RH of 100, 145, 175, and 215 mS/cm for SPTES-50, -60, -70, and -80 copolymers, respectively [27]. The degree of sulfonation increases for SPTES-50, -60, -70 and -80 copolymers in the order of 45.04, 54.9, 63.3, and 71.6%, respectively. SPTES-50 copolymer has the lowest the degree of sulfonation and SPTES-80 copolymer has the highest degree of sulfonation in this class of copolymers [22,27]. The scattering spectra of the fully hydrated (D2O) SPTES-50, -60, -70, and -80 copolymers at 25 �C show a scattering behavior of a two phase structure (Fig. 2a). The contribution of sulfonic groups to the scattering is considered to be negligible due to their small volume compared to the polymer and the water phase volume. It was visually observed that membranes do not dissociate when fully hydrated in the examined temperature. It was also determined by weight measurement that the membranes maintain the weight after deswelling. Therefore, the hydrophobic polymeric structure is the supporting network even at high water content and increased temperatures. Fig. 2a presents enhanced scattering intensity data (vertically shifted) for the hydrated SPTES-60, -70, and -80 copol-ymer membranes in the order of 20X, 1000X, and 2 �105X for the clarity of data presentation. The absolute scale scattering intensities in the medium q range of 0.02 < q (1/Å) < 0.1 are almost in the same intensity range for all SPTES copolymers. The high q scattering spectra for all copolymers exhibit a feature which is attributed to the presence of ionic domains containing water molecules. According to the scattering results, the morphology of all hydrated SPTES-50, -60, -70, and -80 copolymer membranes at 25 �C can be approximated as correlated polydisperse spherical aggregates with liquid-like ordering (P.Y. ordering) and a power law decay [29]. The scattering data for the hydrated SPTES-50, -60, -70, and -80 copolymer membranes at 25 �C were compared and quantified using polydisperse (Schulz polydispersity) hard sphere model with Percus Yevick liquid-like ordering [32e34] and a low q power law decay [29]. The modeling of the SPETS 50 nanostructure and its properties have been reported elsewhere but presented here for', 'Fig. 1. Chemical structure of the highly sulfonated endcapped polyarylenethioethersulfone, SPTES-50, -60, -70, -80 (k ¼ 0.5, 0.6, 0.7, 0.8).', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215616', 'clarity and completeness of the SPTES series analysis [29]. Fig. 2b shows the experimental scattering spectra of the hydrated SPTES-70 copolymer membrane at 25 �C compared with the theoretical calculations of the model (solid line). The results of the comparison of the hydrated SPTES-50, -60, -70 and -80 copolymer membranes scattering data with the model are summarized in Table 1. According to this modeling the domain radius increased with increasing degree of sulfonation in the order of 13.45,14.9,15.4, and 16.9 Å for SPTES-50, -60, -70 and -80 copolymer membranes. The hard sphere packing density increased with increasing degree of sulfonation in the order of 19.8%, 23%, 25%, 29% for SPTES-50, -60, -70, and -80 copolymer membranes. This could indicate that ionic domains are capable of holding more water molecules with the presence of more sulfonic groups on the polymer backbone (increasing the degree of sulfonation). The high-q scattering spectra exhibits onset of a peak formation when the degree of sulfonation was increased to 71.6% for SPTES-80 copolymer. Pres-ence of scattering maxima can be due to the scattering from an ordered structure (periodicity) or the oscillations resulted from the structural factor effects or the excluded volume effect arising from short range liquid-like ordering. The onset of peak formation is attributed to the excluded volume effects related to the liquid-like ordering [33e35]. The low-q power law decay of nearly �3 was observed for all SPTES copolymers in the range of 1 �10�4 < q (1/ Å) < 3 �10�3. The power law decay of �3 is attributed to the presence of interacting three-dimensional fractal morphology. In addition, a Guinier plateau [35] was present for the hydrated SPTES-80 copolymer at the very low q (q (1/Å) < 9 �10�5). This is in addition to the nearly identical scattering intensity in the inter-mediate q-range (0.02 < q (1/Å) < 0.1). It can be concluded that the change in the degree of sulfonation had little or no effect on the intermediate and low angle scattering wave vector. The low-q power law decay was nearly �3 �0.1 for all SPTES copolymers. The presence of sharp interface between hydrophobic and hydro-philic domains was deduced from the power law decay of �4 at large angle wave vectors for all hydrated SPTES copolymer membranes. All scattering data presented in Fig. 2a shows a power slope of �3.85 to �4.15 which is approximated as w �4. The presence of Porod behavior (decay of �4) has been attributed to the two immiscible phase with a sharp boundary [35e37]. This shows that the hydrophobic and hydrophilic domains are separated with a distinct interface containing sulfonic groups. Presence of a plateau in the scattering spectra is characteristics of isolated scatterers [35]. The radius of gyration (Rg) of the isolated scatterers can be approximated by IðqÞ ¼ I0expðð�q2R2gÞ=3Þ, where I0 is the extrapolated zero scattering intensity (Guinier approxi-mation) [35]. Presence of the Guinier plateau in the q range of 4.3 �10�5 < q (1/Å) < 8.2 �10�5 for SPTES-80 copolymer suggests segregation of the isolated large scale hydrophilic water pockets when the degree of sulfonation increased to 71.6%. Radius of gyration of the isolated larger scale water pockets in fully hydrated', 'Fig. 2. Scattering spectra of fully hydrated (D2O) SPTES copolymer series at 25 �C 2a) Scattering spectra of SPTES-50 (C), SPTES-60 (B), SPTES-70 (6), and SPTES-80 (,); Scattering intensity for hydrated SPTES-60, SPTES-70 and SPTES-80 were shifted vertically for clarity (SPTES-60: 20X, SPTES-70: 1000X, SPTES-80: 2 �105X). 2b) Experimental scattering spectra of SPTES 70 compared with polydisperse hard sphere model with liquid-like ordering and a power law decay of �2.9. 2c) The plot of Ln (I) vs. q2 in the 4.36 �10�5 < q (1/Å) < 8.25 �10�5.', 'Table 1 Structural characteristics of SPTES copolymer membrane predicted by polydisperse hard sphere with Percus Yevick liquid-like ordering and a low-q decay.', 'MaterialR (Å)Vol. fractionPolydispersityLow q decay', 'Hydrated Membranes at 25 �C SPTES-5013.45 �0.20.20.43�3 SPTES-6014.9 �0.50.230.36�2.9 SPTES-7015.4 �0.50.250.32�2.9 SPTES-8016.9 �0.50.290.31�3.1', 'Hydrated Membranes at 55 �C SPTES-5026.4 �0.50.280.35e SPTES-6032 �0.20.330.31e', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215617', 'SPTES-80 copolymer membrane was 1.3 �0.18 micron which was calculated from this approximation (Fig. 2c).', '3.2. Hydrated SPTES copolymers at 55 �C', 'Based on the physical observations, water saturated membranes at 55 �C were swollen films without visual dissociation. Fig. 3 exhibits the scattering spectra of the fully hydrated (D2O) SPTES-50, -60, -70 and -80 copolymer membranes at 55 �C. The inter-mediate scattering intensity decreased with increasing degree of sulfonation in the order of SPTES-50, -60, -70, and -80 copolymers. The scattering spectra of the fully hydrated SPTES-60, -70, and -80 copolymer membranes start to form scattering maxima at high q range when the degree of sulfonation increased. This indicated the increase in the excluded volume due to the increase in the volume of the scatterers at higher degree of sulfonation and increased temperature (55 �C). The presence of scattering maxima is more significant for SPTES-70 copolymer and SPTES-80 copolymer with 63.3 and 71.6% degree of sulfonation. The large angle scattering feature shows scattering maxima at qmax of 0.103 and 0.1108 1/Å for SPTES-70 and SPTES-80 copolymers which correspond to spatial characteristics lengths (l ¼ 2p/qmax) of 60.97 and 56.7 Å due to concentration fluctuations of hydrated domains (hard spheres). The scattering spectra of the hydrated SPTES-50, -60, -70, and -80 copolymer membranes changed significantlywith increasing degree of sulfonation. This indicates a substantial change in the hydrophobic/hydrophilic morphology with increasing degree of sulfonation at 55 �C which is more pronounced for SPTES-70 and SPTES-80 copolymers (degree of sulfonation 63.3 and 71.6%, respectively). Hydrophobic and hydrophilic regions are segregated distinctly by the sulfonic groups at the interface which is repre-sented by the Porod behavior, asymptotic behavior of w �4 for all SPTES copolymers [35e37]. The morphology of these membranes is complex and controlled by interfacial phenomena. The number of sulfonic groups per repeat unit volume and their acidity com-plemented with the chain persistent length and chain mobility are governing factors in the formed morphology. This study approxi-mates the morphology of the fully hydrated SPTES-50 and SPTES-60 copolymer membranes at 55 �C are approximated as spherical nanodomains containing water molecules with liquid-like ordering similar to their morphology at 25 �C. It is also proposed that this morphology changedtofractalmorphologywithincreasing temperature (increasing the intermediate scattering decay). The domain radius and the sphere packing density were increased from 26.4 Å and 28% for SPTES-50 copolymer to 32 Å and 33% for SPTES-60copolymerwhiletheintermediatescatteringintensity decreased. The domain radius and packing density of the hydrated SPTES-50 copolymer membrane increased from 13.45 Å and 19.8% to 26.4 Å and 28% when the temperature increased from 25 �C to 55 �C with this approximation. The same increasing trend of 14.9 Åe32 Å for the average domain radius and 23%e33% of the sphere packing density was observed. The decrease in the inter-mediate scattering of the hydrated SPTES-60 copolymer membrane compared to the hydrated SPTES-50 copolymer membrane is attributed to the disruption of the large scale morphology. The low q power law decay of the hydrated SPTES-60 copolymer membrane was close to the decay of the hydrated SPTES-50 copolymer membrane. The increase in the degree of sulfonation of SPTES-60, -70, and -80 copolymers from 54.9, to 63.3 and 71.6% resulted in a shift in the position of the high q scattering maxima of the hydrated membranes toward the large angle scattering regime. This peak formation is more prominent for SPTES-70 and SPTES-80 copoly-mers with higher sulfonation degree. The decrease in the inter-mediate scattering intensity in the order of SPTES-50, -60, -70 and', '-80 copolymers is attributed to the loss of the large scale water network, intermediate fractal morphology within the polymer and onset of a morphology change to a two large scale phase morphology. The closed domain morphology of polydisperse spherical aggregates containing water molecules with fractal network were present for fully hydrated SPTES-50 and SPTES-60 copolymer membranes at 55�C (Figs. 3 and 4a). However, substantialchangesinthemorphologyoffullyhydrated membranes occurred when the degree of sulfonation is increased to 63.3 and 71.6% for SPTES-70 and SPTES-80 copolymers at higher temperature of 55 �C. Combination of high temperature, high density of sulfonic groups, and significant amount of water mole-cules within the polymer backbone could have resulted in the coalescence of the small spherical ionic domains and fractal water network into a larger scale bicontinuous network of intermeshed hydrophobic and hydrophilic morphology for fully hydrated SPTES-80 copolymer membrane at 55 �C. The bicontinuous model origi-nally proposed for micro-emulsion of two immiscible phases of water and oil with comparable amount based on Landau theory [38,39]. This model describes two irregular shapes with distinct boundary and has been used for intermesh of hydrophobic and hydrophilic structure when the particle shape is not well-defined [40,41]. This model proposes I(q) ¼ (a2þc1q þ c2q2)�1 for a2 > 0, c1 < 0, and c2 > 0, a single broad scattering maxima, and power law decay of �4 at large scattering angles. Two characteristics lengths of d and x are deduced from this analysis having a2, c1, and c2[39,40], d is the domain periodicity or interdomain distance and x is the correlation length which has been attributed to the dispersion of d. According to this theory, c1 is negative due to the surfactant. The absolute values of c1 and the ratio of x/d should increase with increasing surfactant. The specific internal surface area can be derived from the ratio of S/V ¼ 44(1�4)/x [38,39]. The scattering spectra of the hydrated SPTES-70 copolymer membrane has a low-q upturn, a lower intermediate scattering intensity compared to the hydrated SPTES-60 and SPTES-50 copolymers, and a large angle maxima followed by a power law decay of �4. This indicates the onset of the coalescence of the ionic aggregates, disruption of the fractal water network, and the formation of the two irregular large scale bicontinuous phase morphology. The high q range (q > 0.037 1/Å) of the scattering', 'Fig. 3. Scattering spectra of fully hydrated (D2O) SPTES copolymers at 55 �C. The high q scattering exhibit a power law decay of �4. High q range maxima position of hydrated SPTES-60, SPTES-70 and SPTES-80 shifts toward higher q with increasing degree of sulfonation.', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215618', 'spectra of the hydrated SPTES-70 copolymer membrane at 55 �C was compared with the T.S. bicontinuous model (Fig. 4b). This simulation resulted in a periodicity, d, of 59.9 Å which is in excellent agreement with the one obtained from the scattering maxima position, 60.97 Å. According to this simulation, the correlation length (x) is 31.14 Å. The medium-q range scattering still exhibits the larger scale morphology which is not completely converted to the bi-continuous phase morphology. However, they are partially collapsed characterized by a lower scattering intensity in the intermediate q range. The high q maxima followed by a power law decay of �4 is approximated as bicontinuous morphology. The low and medium q (q < 0.037 1/Å) scattering spectra of the SPTES 70 is due to the presence of fractal network of waters which have not coalesce yet. The scattering spectra of the SPTES-80 copolymer are consistent with a bicontinuous two phase structure of irregular shapes with sulfonic groups as interfacial ionic region. The scattering experi-mental data of the hydrated SPTES-80 copolymer membrane was compared with this model (q > 0.015 1/Å), (Fig. 4c and Table 2). The periodicity of the water domains (or hydrophobic domains), d, predicted by this model is w54 Å which is in excellent agreement with the one obtained from the scattering maxima (qmax), 56.7 Å. This value is the distance between water domains. The correlation length (x) obtained from this model is 37.86 Å which is a measure of dispersion. The distance between the water phases, periodicity, was decreased from 59.9 Å to 54 Å with increasing degree of sulfona-tion. The correlation length, x, was increased from 31.14 Å to 37.86 Å when the degree of sulfonation increased. Increasing the interfacial area results in an increase in the x/d ratio. The presence of the upturn in the low range of the hydrated SPTES 70 and 80 at 55 �C (Fig. 4b and c) is attributed to the fractal morphology of the large scale features.', '3.3. Discussions', 'Scattering data of series of SPTES membranes were obtained in the full hydration state with increasing the sulfonation degree in the order of 45.04, 54.9, 63.3, and 71.6% for SPTES-50, -60, -70, and -80 copolymer membranes. Complete study was performed to provide understanding membranes morphology with increasing the temperature of the hydrated membranes from 25 �C to 55 �C. Proposed model assumes spherical nanodomains containing water molecules forming from clustering of the sulfonation groups. This assumption was performed based on previously reported study which showed spherical nanodomains in the dry SPTES 50 membrane under HR-TEM [29]. The scattering spectra of the membranes with high degree of sulfonation (SPTES 70, and 80) changed significantly when the temperature increased to 55 �C. The morphology is approximated as bi-continuous system where the hydrocarbon is the main support network containing water. This model has been recently proposed by Wnek et al. [40], Gebel [8], and Kreuer [12], Wnek also provided visualization of the hydro-phobic cluster using SYBYL version 6.7 (Tripos) and HINT (Hydro-pathicINTeractions)computationalmodelingsoftware[40]. Despite the support of the transport studies of this model [42],', 'Fig. 4. Experimental SANS data of fully hydrated (D2O) SPTES-60 (lower degree of sulfonation), SPTES-70 and SPTES-80 (highest degree of sulfonation) membranes at 55 �C. High q scattering spectra exhibits a power law decay of �4 indicating a sharp ionic interface for all hydrated polymer membranes. 4a) SANS spectra of hydrated SPTES-60 at 55 �C (B) is comparedwiththe polydispersehardsphere model withliquid-like ordering and a power law decay. 4b) SANS spectra of the SPTES-70 indicates that the domain aggregates and fractal domains start to collapse and form bicontinuous irregular two immiscible phase morphology. Bicontinuous model was compared with SANS spectra in the q range of (q > 0.0371/Å). 4c) Experimental scattering data of fullyhydrated SPTES-80 (B) at 55 �C compared with Tuebner Strey bicontinuous phase model.', 'Table 2 Structural characteristics of fully hydrated SPTES-70 and SPTES-80 at 55�C described by Tubnet Strey model.', 'Materiald (Å)x (Å)x/dl (Å)', 'Hydrated Membranes at 55 �C SPTES-7059.9 �0.231.1 �0.40.5261 �0.3 SPTES-80w54 �0.337.8 �0.60.756.7 �0.2', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215619', 'there is no direct visualization method for this proposed approxi-mation. The T. S. model is also supported by high proton conduc-tivity of the hydrated SPTES 70 and 80 membranes at 55 �C where a larger number of hydronium ions in a larger domain can facilitate proton transport. A summary of the structural evolution of the SPTES membranes as a function of temperature and degree of sulfonation is illustrated in Table 3. This study attempted to study a full range of SPTES membrane nanostructure for the first time and provide an understanding of large membrane water uptakes, and their morphology and their relation with high conductivity of the membranes using both liquid like ordering of polydisperse nano-spheres and bi-continuous T.S. model approximation.', '4. Conclusions', 'A series of SPTES copolymers with high proton conductivity of 100e215 mS/cm at 65 �C and 85% relative humidity as potential fuel cells membranes were studied. SANS studies of fully hydrated membranes showed that the nanostructure of the fully hydrated SPTES-50, -60, -70, and -80 copolymer membranes at 25 �C in agreement with ionic aggregates containing water molecules with a large scale morphology network of water pockets morphology. This model predicted that the increase in the degree of sulfonation resulted in an increase in the radius of ionic domains and an increase in the volume packing density of water in the aggregates. It was assumed that the same morphology of polydisperse correlated spherical ionic domains were present in the SPTES-50 and SPTES-60 copolymers when the temperature increased to 55 �C. Increase in the degree of sulfonation for fully hydrated SPTES-70 and SPTES-80 copolymer at 55 �C led to a substantial reorganization of the membrane morphology which was described by a bi-continuous hydrophobic/hydrophilic network.', 'Acknowledgments', 'The authors would like to thank the Air Force Office of Scientific Research and Materials and Manufacturing Directorate, Nano-structured andBiological Materials Branch forfunding this research. Richard A. Vaia, Michael F. Durstock (WPAFB), and Derek Ho (formerly at NIST) are thanked for the technical discussions support. The National Institute of Standards and Technology is thanked for funding (Proposal S18-38) to conduct neutron scat-tering experiments which were supported by National Science Foundation under agreement DMR-9986442. The mention of commercial products does not imply endorsement by NIST, nor does it imply that the materials or equipment identified are necessarily the best available for the purpose.', 'References', '[1] Schlick S. Ionomers: characterization, theory and applications. FL: CRC Press; 1996. [2] Tant MR, Mauritz KA, Wilkes GL, editors. Ionomers: synthesis, structure, properties and applications. London: Chapman and Hall; 1997. [3] Larminie J, Dicks A, editors. Fuel cells systems explained. London: John Wiley & Sons; 2003. [4] Cleghorn SJC, Ren X, Springer TE, Wilson MS, Zawodinski C, Zawodinski TA, et al. Int J Hydrogen Energy 1997;22:1137e44. [5] Hoogers G. Fuel cell technology handbook. CRC Press LLC; 2003. MA. [6] Eisenberg A, Yeager HL. ACS symposium series 180. Washington, DC: Amer-ican Chemical Society; 1982. [7] Kim MH, Glinka CJ, Grot SA, Grot WG. Macromolecules 2006;39:4775e87. [8] Gebel G, Lambard J. Macromolecules 1997;30:7914e20. [9] Rollet AL, Diat OR, Gebel G. J Phy Chem B 2002;106:3033e6. [10] Young SK, Trevino SF, Beck Tan NC. J Polym Sci. Part B Polym Phy 2002;40: 387e400. [11] Rollet AL, Gebel G, Simonin JP, Turq P. J Polym Sci. Part B Polym Phy 2001;39: 548e58. [12] Kreuer D. J Mem Sci 2001;185:29e39.', 'Table 3 Summary of membrane structural changes (SPTES 50, 60, 70, and 80 at 25 �C, and SPTES 50 and 60 at 55 �C) as a function of degree of sulfonation and temperature by polydisperse hard sphere model with liquid-like ordering (P.Y. ordering). T.S. bi-continuous model was approximated for higher sulfonation degree copolymers, SPTES 70 and 80, at 55 �C.', 'SPTES 50SPTES 60SPTES 70SPTES 80', 'Sulfonation Level, %45.0454.963.371.6', 'Temperature, oC2555255525552555', 'MorphologyP.D. hard sphere, P.Y. ordering', 'P.D. hard sphere, P.Y. ordering', 'P.D. hard sphere, P.Y. ordering', 'P.D. hard sphere, P.Y. ordering', 'P.D. hard sphere, P.Y. ordering', 'T.S. bi-continuous modelP.D. hard sphere, P.Y. ordering', 'T.S. bi-continuous model', 'Radius, Å13.45 �0.226.4 �0.514.9 �0.532 �0.215.4 �0.516.9 �0.5 Correlation length (x), Å', '31.1 �0.437.8 �0.6', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215620', '[13] Elliott JA, Hanna SA, Elliott MS, Cooley GE. Macromolecule 2000;33:4161e71. [14] Schmidt-Rohr K, Chen Q. Nat Mater 2007;7:75e83. [15] Haubold HG, Vad Th, Jungbluth H, Hiller P. Electro Acta 2001;46:1559e63. [16] Elliott JA, Hanna SJ. Appl Crystallogr 1999;32:1069e83. [17] Pivovar AM, Pivovar BS. J Phys Chem B 2005;109:785e93. [18] Nosaka AY, Nosaka YJ. J Power Sources 2008;180:733e7. [19] Rollet AL, Blachot JF, Delville A, Diat O, Guillermo A, Porion P, et al. Eur Phys J 2003;12:130e4. [20] Rollet AL, Porion PT, Delville A, Diat O, Gebel G. Mag Res Imag 2005;23:367e8. [21] Blachot JF, Diat O, Putaux JL, Rollet AL, Rubatat L, Vallois C, et al. J Mem Sci 2003;214:31e42. [22] Bai Z, Durstock MF, Dang TD. J Mem Sci 2006;281:508e16. [23] Bai Z, Price GE, Yoonessi M, Juhl SB, Durstock MF, Dang TD. J Mem Sci 2007; 305:69e76. [24] Yoonessi M, Bai Z, Dang TD, Durstock MF, Vaia RA. Proceeding of American Institute of chemical Engineers (AIChE), 2005. [25] Dang T, Bai Z, Dalton MJ. Fossum E 27th ACS National Meeting, 2004 Ana-heim, CA. [26] Bai Z, Williams LD, Durstock MF, Dang TD. Polym Preprints (American Chem Soc Division Polym Chemistry) 2004;45:60e1. [27] Yoonessi M, Bai Z, Dang TD. J Polym Sci. Part B Polym Phys 2007;45:2813e22. [28] Bai Z, Dang TD. Macro Rapid Comm 2006;27:1271e7.', '[29] Yoonessi M, Heinz H, Dang TD, Wheeler R, Bai Z. Polymer 2010;51: 1585e92. [30] Kline S. SANS data reduction tutorial. Gaithersburg, MD: NIST Center for Neutron Research; 2001. [31] Hammouda B, http://www.ncnr.nist.gov/staff/hammouda/the_SANS_toolbox. pdf, April, 2008. [32] Kinning DJ, Thomas EL. Macromolecules 1984;17:1712e8. [33] Percus JK, Yevick GJ. Phys Rev 1958;110:1e13. [34] Percus JK. Phys Rev Lett 1962;8:462e3. [35] Guinier A, Fournet G. Small-angle scattering of x-rays. NewYork: John Wiley and Sons; 1955. [36] Porod G. In: Glatter O, Kratky O, editors. Small-angle x-ray scattering. London: Academic Press; 1982. [37] Higgins JS, Benoit HC. Polymers and neutron scattering. Oxford: Clarendon Press; 1994. [38] Teubner M, Strey R. J Chem Phys 1987;87:3195e7. [39] Schubert KV, Strey R, Kline SR, Kaler EW. J Chem Phys 1994;101:5343e56. [40] Serpico JM, Ehrenberg SG, Fontanella JJ, Jiao X, Perahia D, McGrady KA, et al. Macromolecules 2002;35:5916e21. [41] Nieh MP, Guiver MD, Kim DS, Ding J, Norsten T. Macromolecules 2008;41: 6176e82. [42] Edmondson CA, Fontanella JJ, Chung SH, Greenbaum SG, Wnek GE. Electro-chim Acta 2001;46:1623e8.', 'M. Yoonessi et al. / Polymer 52 (2011) 5615e56215621']
2025-02-19 15:26:27 - WARNING: done parsing pdf
2025-02-19 15:26:27 - WARNING: start ner pdf
2025-02-19 15:26:28 - INFO: Loading Data
2025-02-19 15:26:28 - WARNING: 2025-02-19 15:26:28 - INFO: Loading Data
2025-02-19 15:26:31 - WARNING: Predicting NER ...
2025-02-19 15:26:33 - WARNING: Finished predicting.
2025-02-19 15:26:33 - WARNING: Converting to Brat format...
2025-02-19 15:26:33 - WARNING: # of discontinuous mentions:
2025-02-19 15:26:33 - WARNING:  
2025-02-19 15:26:33 - WARNING: 21
2025-02-19 15:26:33 - WARNING: Finished.
2025-02-19 15:26:33 - WARNING: start predict re
2025-02-19 15:26:33 - WARNING: Example:   0%|          | 0/26 [00:00<?, ?it/s]
2025-02-19 15:26:33 - WARNING: Example:  19%|#9        | 5/26 [00:00<00:00, 41.76it/s]
2025-02-19 15:26:34 - WARNING: Example:  58%|#####7    | 15/26 [00:00<00:00, 72.15it/s]
2025-02-19 15:26:34 - WARNING: Example: 100%|##########| 26/26 [00:00<00:00, 87.90it/s]
2025-02-19 15:26:34 - WARNING: # of documents 26.
2025-02-19 15:26:34 - WARNING: # of positive examples 0.
2025-02-19 15:26:34 - WARNING: # of negative examples 8104.
2025-02-19 15:26:34 - WARNING: dict_keys(['7', '10', '17', '20', '25', '28', '37', '38', '39', '41', '43', '44', '49', '51', '56'])
2025-02-19 15:26:34 - WARNING: done predict re
2025-02-19 15:26:34 - WARNING: model output text 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: Keywords : Fuel cells membrane Morphology Neutron scattering 
2025-02-19 15:26:34 - WARNING: len of model_output_text 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: 61
2025-02-19 15:26:34 - WARNING: original_text 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: Keywords: Fuel cells membrane Morphology Neutron scattering
2025-02-19 15:26:34 - WARNING: mapping dict 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 58}
2025-02-19 15:26:34 - WARNING: text: 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: Keywords : Fuel cells membrane Morphology Neutron scattering 
2025-02-19 15:26:34 - WARNING: origin bbox 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: {'x1': 95.86449432373047, 'y1': 411.0362548828125, 'x2': 99.4517593383789, 'y2': 418.8723449707031, 'width': 595.2760009765625, 'height': 793.7009887695312, 'pageNumber': 1}
2025-02-19 15:26:34 - WARNING: normalized bbox 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: {'x1': 95.86449432373047, 'y1': 411.0362548828125, 'x2': 99.4517593383789, 'y2': 418.8723449707031, 'width': 595.2760009765625, 'height': 793.7009887695312, 'pageNumber': 1}
2025-02-19 15:26:34 - WARNING: finalized bbox 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: {'x1': 95.86449432373047, 'y1': 411.0362548828125, 'x2': 99.4517593383789, 'y2': 418.8723449707031, 'width': 595.2760009765625, 'height': 793.7009887695312, 'pageNumber': 1}
2025-02-19 15:26:34 - WARNING:  final bouding box 
2025-02-19 15:26:34 - WARNING:  
2025-02-19 15:26:34 - WARNING: {'x1': 42.51969909667969, 'y1': 385.4175109863281, 'x2': 105.66481018066406, 'y2': 418.8723449707031, 'width': 595.2760009765625, 'height': 793.7009887695312, 'pageNumber': 1}
2025-02-19 15:26:34 - WARNING: done save re and ner, 
2025-02-19 15:26:34 - WARNING: start count num_rel 
2025-02-19 15:26:34 - WARNING: done count num_rel 
2025-02-19 15:26:34 - WARNING: start update document 
2025-02-19 15:26:35 - WARNING: start matching infor
2025-02-19 15:26:35 - WARNING: done matching infor
2025-02-19 15:26:35 - WARNING: start commit
2025-02-19 15:26:35 - WARNING: done update document 
2025-02-19 15:26:38 - WARNING: Failed to send email. Error: {'an': (553, b'5.1.3 The recipient address <an> is not a valid RFC 5321 address. For more\n5.1.3 information, go to\n5.1.3  https://support.google.com/a/answer/3221692 and review RFC 5321\n5.1.3 specifications. d2e1a72fcca58-732814ebb76sm4821784b3a.141 - gsmtp')}
2025-02-19 15:26:38 - INFO: Task dev_tasks.process_pdf_task[bc7b0d06-70c6-4fd5-b43c-2296500c3f5a] succeeded in 11.383752193301916s: {'id': 97873793, 'filename': 'yoonessi2011.pdf', 'upload_time': '2025/02/19, 06:26:26', 'entities': 489, 'relations': 265, 'pages': 7, 'status': 'completed'}
2025-02-19 15:26:38 - WARNING: 2025-02-19 15:26:38 - INFO: Task dev_tasks.process_pdf_task[bc7b0d06-70c6-4fd5-b43c-2296500c3f5a] succeeded in 11.383752193301916s: {'id': 97873793, 'filename': 'yoonessi2011.pdf', 'upload_time': '2025/02/19, 06:26:26', 'entities': 489, 'relations': 265, 'pages': 7, 'status': 'completed'}
